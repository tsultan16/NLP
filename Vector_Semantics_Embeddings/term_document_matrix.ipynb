{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Term Document Matrix`\n",
    "\n",
    "Given $D$ diferent documents containing text and a vocabulary $V$, the term-document matrix is a $|V| \\times D$ matrix whice $(i,j)th$ cell contains the frequency with which word $i$ from the vocabulary occurs in the $jth$ document. Then each row of this matrix can be interpreted as a $D$ dimensional embedding vector representation for a word from the vocab. We can then measure how similar two words are by computing the cosine similarity between their embedding vectors. \n",
    "\n",
    "We make two additional improvements: \n",
    "1) Because raw frequencies can vary over a large range, we keep track of log(counts+1) instead of raw counts (add 1 smoothing to avoid $log(0)$).\n",
    "2) We weight each count by the inverse document frequency (idf)\n",
    "\n",
    "So each cell is the matrix is computed as: $tf_{i,j} \\times idf_i$\n",
    "\n",
    "where $tf_{i,j} = \\log_{10}(count(i,j) + 1)$ and $idf_i = \\log_{10}(D/df_i)$, $df_i$ is the number of documents word $i$ appears in.\n",
    "\n",
    "\n",
    "We will use the Brown corpus from NLTK to create a TD matrix and look at some properties of the resulting word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punc(w):\n",
    "    return any(c.isalpha() for c in w)\n",
    "\n",
    "# remove punctuations from list of words and lowercase folding \n",
    "def remove_punc(s):\n",
    "    return [w.lower() for w in s if check_punc(w)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: adventure\n",
      "Num words in category: 58372\n",
      "Category: belles_lettres\n",
      "Num words in category: 151548\n",
      "Category: editorial\n",
      "Num words in category: 54087\n",
      "Category: fiction\n",
      "Num words in category: 58296\n",
      "Category: government\n",
      "Num words in category: 61143\n",
      "Category: hobbies\n",
      "Num words in category: 71552\n",
      "Category: humor\n",
      "Num words in category: 18265\n",
      "Category: learned\n",
      "Num words in category: 159940\n",
      "Category: lore\n",
      "Num words in category: 96695\n",
      "Category: mystery\n",
      "Num words in category: 48174\n",
      "Category: news\n",
      "Num words in category: 87004\n",
      "Category: religion\n",
      "Num words in category: 34308\n",
      "Category: reviews\n",
      "Num words in category: 35088\n",
      "Category: romance\n",
      "Num words in category: 58612\n",
      "Category: science_fiction\n",
      "Num words in category: 12035\n"
     ]
    }
   ],
   "source": [
    "# we will treat all text from a particular category/genre as a single document, then each genre will be a dimension of our word vectors\n",
    "categories = brown.categories()\n",
    "category_words = {}\n",
    "for category in categories:\n",
    "    print(f\"Category: {category}\")\n",
    "    # now we will store the words from all documents across each category in separate lists\n",
    "    category_words[category] = []\n",
    "    for fileid in brown.fileids(categories=category):\n",
    "        category_words[category].extend(remove_punc(brown.words(fileids=fileid)))\n",
    "    print(f\"Num words in category: {len(category_words[category])}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 48052\n"
     ]
    }
   ],
   "source": [
    "# now lets create the vocabulary\n",
    "vocab = sorted(list(set([w for d in list(category_words.values()) for w in d])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "categories = sorted(categories)\n",
    "cats2idx = {c:i for i,c in enumerate(categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create and populate the term-document matrix\n",
    "V = len(vocab)\n",
    "D = len(categories)\n",
    "T = np.zeros(shape=(V,D))\n",
    "\n",
    "# scan through documents/categories and accumulate counts\n",
    "for j, category in enumerate(categories):\n",
    "    for word in category_words[category]:\n",
    "        T[word2idx[word], j] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute inverse document freqeuncy (if we had large number of documents, we would also take log of idf)\n",
    "idf = D / (T > 0).sum(axis=1, keepdims=True)\n",
    "# convert to log counts\n",
    "T = np.log10(T+1)   \n",
    "# multiply log counts by log idf\n",
    "T = idf * T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've computed our term-document matrix, let's look at some word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes normalized cosine similarity between two word embedding vectors\n",
    "def cosine_similarity(w1,w2):\n",
    "    similarity_score = np.dot(w1, w2) / (np.linalg.norm(w1) * np.linalg.norm(w2))\n",
    "    return similarity_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'mysterious' and 'judicial' = 0.6879424352638849\n",
      "Simialrity socre between 'mysterious' and 'court' = 0.6549556896017964\n"
     ]
    }
   ],
   "source": [
    "word1 = \"mysterious\"\n",
    "word2 = \"judicial\"\n",
    "word3 = \"court\"\n",
    "\n",
    "w1 = T[word2idx[word1]]\n",
    "w2 = T[word2idx[word2]]\n",
    "w3 = T[word2idx[word3]]\n",
    "similarity_12 = cosine_similarity(w1, w2)\n",
    "similarity_13 = cosine_similarity(w1, w3)\n",
    "\n",
    "print(f\"Simialrity socre between '{word1}' and '{word2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{word1}' and '{word3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that these word embedding don't seem to make much sense. This is not surprising because the document categories have significant overlap and so the these features of the word embedding don't do a good job at capturing the word semantics.\n",
    "\n",
    "#### The columns of the matrix can be interpreted as a vector representation for each document category, i.e. a \"document embedding\". Let's check simirity between these document embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'mystery' and 'romance' = 0.013699794367111637\n",
      "Simialrity socre between 'mystery' and 'religion' = 0.004418250824928202\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"mystery\"\n",
    "doc2 = \"romance\"\n",
    "doc3 = \"religion\"\n",
    "\n",
    "d1 = T[:,cats2idx[doc1]]\n",
    "d2 = T[:,cats2idx[doc2]]\n",
    "d3 = T[:,cats2idx[doc3]]\n",
    "similarity_12 = cosine_similarity(d1, d2)\n",
    "similarity_13 = cosine_similarity(d1, d3)\n",
    "\n",
    "print(f\"Simialrity socre between '{doc1}' and '{doc2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{doc1}' and '{doc3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'editorial' and 'news' = 0.020000326299592922\n",
      "Simialrity socre between 'editorial' and 'science_fiction' = 0.004505660960214556\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"editorial\"\n",
    "doc2 = \"news\"\n",
    "doc3 = \"science_fiction\"\n",
    "\n",
    "d1 = T[:,cats2idx[doc1]]\n",
    "d2 = T[:,cats2idx[doc2]]\n",
    "d3 = T[:,cats2idx[doc3]]\n",
    "similarity_12 = cosine_similarity(d1, d2)\n",
    "similarity_13 = cosine_similarity(d1, d3)\n",
    "\n",
    "print(f\"Simialrity socre between '{doc1}' and '{doc2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{doc1}' and '{doc3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These document representations seem to do a good job at capturing the semantic of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
