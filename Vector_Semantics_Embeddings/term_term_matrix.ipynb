{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term-Term/Word Co-occurance Matrix\n",
    "\n",
    "Given a corpus and vocabulary $V$, the word co-occurance matrix is a $|V| \\times |V|$ matrix whose $(i,j)th$ cell contains the frequency with which word $j$ appears in the context of word $i$. Word $j$ is called a `context word` and word $i$ is called a `center word`. The context window around a center word is defined as a $\\pm k$ word window around the center word, i.e. $k$ words to the left and $k$ word to the right of that center word. Each row of this matrix can then be interpreted as a $|V|$ dimensional embedding vector for a word from the vocabulary. For smaller context windows, the embedding vectors tend to capture more syntactic information/local context, while for larger context windows the embedding vectors capture more global context.\n",
    "\n",
    "We will use the Brown corpus again to create a co-occurance matrix and look at some properties of the resulting word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tanzid/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num words in corpus: 530090\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def check_punc(w):\n",
    "    return any(c.isalpha() for c in w)\n",
    "\n",
    "# remove punctuations and stopwords from list of words and apply lowercase folding \n",
    "def preprocess(s):\n",
    "    words = [w.lower() for w in s if check_punc(w)]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return words\n",
    "\n",
    "# preprocess the corpus (remove punctutations and lowecase folding)\n",
    "corpus_words = preprocess(brown.words())\n",
    "print(f\"Num words in corpus: {len(corpus_words)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(corpus_words)\n",
    "\n",
    "# now lets create the vocabulary (keep 5000 most common words)\n",
    "vocab = (word_counts.most_common(8000))\n",
    "vocab = sorted([v[0] for v in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8002\n"
     ]
    }
   ],
   "source": [
    "# context window half-size\n",
    "k = 20 \n",
    "corpus_length = len(corpus_words)\n",
    "\n",
    "# we also insert a special padding token so that we can fit the context window at the beginning and end of the corpus text\n",
    "pad_token = \"<PAD>\"\n",
    "unk_token = \"<UNK>\"\n",
    "vocab = [pad_token, unk_token] + vocab\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "# adding padding to corpus\n",
    "corpus_words = [pad_token]*k + corpus_words + [pad_token]*k\n",
    "# replace with oov tokens in corpus\n",
    "corpus_words = [w if w in vocab else unk_token for w in corpus_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create and populate the term-term matrix\n",
    "V = len(vocab)\n",
    "T = np.zeros(shape=(V,V))\n",
    "\n",
    "# scan through documents/categories and accumulate counts\n",
    "for i in range(corpus_length):\n",
    "    window = corpus_words[i:i+1+2*k]\n",
    "    center_word = window[k]\n",
    "    context_words = window[:k] + window[k+1:]\n",
    "    # accumulate context word counts\n",
    "    for context_word in context_words:\n",
    "        T[word2idx[center_word], word2idx[context_word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert counts to log counts\n",
    "T = np.log10(T+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes normalized cosine similarity between two word embedding vectors\n",
    "def cosine_similarity(w1,w2):\n",
    "    similarity_score = np.dot(w1, w2) / (np.linalg.norm(w1) * np.linalg.norm(w2))\n",
    "    return similarity_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"pie\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'pie' and 'train' = 0.2430828364039373\n",
      "Simialrity socre between 'pie' and 'sugar' = 0.2839855167816917\n"
     ]
    }
   ],
   "source": [
    "word1 = \"pie\"\n",
    "word2 = \"train\"\n",
    "word3 = \"sugar\"\n",
    "\n",
    "w1 = T[word2idx[word1]]\n",
    "w2 = T[word2idx[word2]]\n",
    "w3 = T[word2idx[word3]]\n",
    "similarity_12 = cosine_similarity(w1, w2)\n",
    "similarity_13 = cosine_similarity(w1, w3)\n",
    "\n",
    "print(f\"Simialrity socre between '{word1}' and '{word2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{word1}' and '{word3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'population' and 'crowd' = 0.3537778241475392\n",
      "Simialrity socre between 'population' and 'unhappy' = 0.31288833385673875\n"
     ]
    }
   ],
   "source": [
    "word1 = \"population\"\n",
    "word2 = \"crowd\"\n",
    "word3 = \"unhappy\"\n",
    "\n",
    "w1 = T[word2idx[word1]]\n",
    "w2 = T[word2idx[word2]]\n",
    "w3 = T[word2idx[word3]]\n",
    "similarity_12 = cosine_similarity(w1, w2)\n",
    "similarity_13 = cosine_similarity(w1, w3)\n",
    "\n",
    "print(f\"Simialrity socre between '{word1}' and '{word2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{word1}' and '{word3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'occupation' and 'job' = 0.3860584884524287\n",
      "Simialrity socre between 'occupation' and 'horse' = 0.3155096152321899\n"
     ]
    }
   ],
   "source": [
    "word1 = \"occupation\"\n",
    "word2 = \"job\"\n",
    "word3 = \"horse\"\n",
    "\n",
    "w1 = T[word2idx[word1]]\n",
    "w2 = T[word2idx[word2]]\n",
    "w3 = T[word2idx[word3]]\n",
    "similarity_12 = cosine_similarity(w1, w2)\n",
    "similarity_13 = cosine_similarity(w1, w3)\n",
    "\n",
    "print(f\"Simialrity socre between '{word1}' and '{word2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{word1}' and '{word3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The similarity scores between embedding vectors for similar words seem to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An improvement over this vanially co-occurance matrix implementation is to use `Positive Point-wise Mutual Information` instead of raw co-occurance counts. In the vanilla cooccurance matrix, the $(i,j)th$ cell contains the value $f_{i,j} = count(w_i, w_j)$ which is the count of the co-occurance of context word $j$ in the window of center word $i$. Instead of these raw counts, consider the pointwise mutual information:\n",
    "\n",
    "$PMI(w_i, w_j) = \\log_2 \\frac{P(w_i,w_j)}{P(w_i)P(w_j)}$\n",
    "\n",
    "where $P(w_i,w_j) = \\frac{f_{i,j}}{\\sum_{i,j} f_{i,j}}$, $P(w_i) = \\frac{\\sum_j f_{i,j}}{\\sum_{i,j} f_{i,j}}$, $P(w_j) = \\frac{\\sum_i f_{i,j}}{\\sum_{i,j} f_{i,j}}$  \n",
    "\n",
    "To understand the meaning of the PMI, we first note that $P(w_i,w_j)$ denotes the joint probability of the two words $w_i$ and $w_j$ co-occuring. If these two words always occured independently, then their joint probability would be $P(w_i)P(w_j)$. So the PMI is a comparision between the two words occuring together compared to if the two words occured independently at random chance. A higher PMI would indicate high co-occurance, while a zero PMI would indicate the word occur independently. We ignore negative values which indicate the words co-occur with proabilty below random chance which is not useful, so we clip negative values of PMI. This is called the Positive-PMI:\n",
    "\n",
    "$PPMI = max(0, \\log_2 \\frac{P(w_i,w_j)}{P(w_i)P(w_j)})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.zeros(shape=(V,V))\n",
    "\n",
    "# scan through documents/categories and accumulate counts\n",
    "for i in range(corpus_length):\n",
    "    window = corpus_words[i:i+1+2*k]\n",
    "    center_word = window[k]\n",
    "    context_words = window[:k] + window[k+1:]\n",
    "    # accumulate context word counts\n",
    "    for context_word in context_words:\n",
    "        f[word2idx[center_word], word2idx[context_word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add-1 smoothing to avoid zero probabilities\n",
    "f += 1\n",
    "\n",
    "f_sum = f.sum()\n",
    "P_ij = f / f_sum\n",
    "P_i = P_ij.sum(axis=1, keepdims=True)\n",
    "P_j = P_ij.sum(axis=0, keepdims=True)\n",
    "PPMI = np.log2((P_ij / P_i) / P_j)\n",
    "PPMI = PPMI * (PPMI>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simialrity socre between 'pie' and 'sugar' = 0.28617411789651437\n",
      "Simialrity socre between 'pie' and 'honest' = 0.15642863473097238\n"
     ]
    }
   ],
   "source": [
    "word1 = \"pie\"\n",
    "word2 = \"sugar\"\n",
    "word3 = \"honest\"\n",
    "\n",
    "w1 = PPMI[word2idx[word1]]\n",
    "w2 = PPMI[word2idx[word2]]\n",
    "w3 = PPMI[word2idx[word3]]\n",
    "similarity_12 = cosine_similarity(w1, w2)\n",
    "similarity_13 = cosine_similarity(w1, w3)\n",
    "\n",
    "print(f\"Simialrity socre between '{word1}' and '{word2}' = {similarity_12}\")\n",
    "print(f\"Simialrity socre between '{word1}' and '{word3}' = {similarity_13}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
