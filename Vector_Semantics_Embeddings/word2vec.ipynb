{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Word2Vec (with Negative Sampling)\n",
    "\n",
    "The word2vec algorithm is a simple self-supervised model for learning dense word embedding vectors from a corpus of text. It is trained on the task of predicting the `probability distribution` over `context words` given a `center word`, for all possible center words from the vocabulary. The parameters of this model consist of two separate $|V| \\times D$ embedding matrices ($|V|$ is the vocab size and $D$ is the embedding dimensions): matrix $W$ whose rows are the embeddings of outside words and matrix $C$ whose rows are the embeddings of center words. (We could instead have one single embedding for both outside and context words, however this approach of having separate embeddings is more convenient). Then the model for computing the probability distribution is simply defined as follows:\n",
    "\n",
    "$P(w|c) = \\frac{exp(\\vec{w} \\cdot \\vec{c})}{\\sum_{w' \\in V} exp(\\vec{w}' \\cdot \\vec{c})}$\n",
    "\n",
    "where $\\vec{w}$ and $\\vec{c}$ are the embedding vectors of the context and center words resepectively. Note that this is just a softmax over all the dot products of every possible context word $w$ given a particluar center word $c$.\n",
    "\n",
    "Now due to large volcabulary size $|V|$, computing this probability distribution is very inefficient (because of the sum in the denominator). Instead of computing a probability distribution over all possible context words, we instead simplify our task into a `binary classification` problem. Given a pair of context word $w$ and center word $c$, our simplified task is to train a binary classifier to predict whether $w$ actually occurs in the context of $c$ or not (we use label $1$ for True and $0$ for False). We define this simple binary classification problem as follows:\n",
    "\n",
    "$P(y=1|w_{pos},c) = \\sigma (\\vec{w}_{pos} \\cdot \\vec{c})$\n",
    "\n",
    "$P(y=0|w_{neg},c) = 1-\\sigma (\\vec{w}_{neg} \\cdot \\vec{c}) = \\sigma (-\\vec{w}_{neg} \\cdot \\vec{c})$\n",
    "\n",
    "where $\\sigma()$ is the saigmoid function, $w_{pos}$ denotes a true context word and $w_{neg}$ denotes a `noise` word which is not a true context word. For training this classifier, we will use $k$ times as many noise words than context words (this reflects the fact that each center word will have far fewer words from the vocab that appear in it's context than words that don't). During training, we will simply slide a context window of half-size $L$ over the training corpus, so at each position this gives us $2L$ different positive pairs $\\{(w^{i}_{pos},c) | i =1,2..,2L\\}$. For each of these positive pairs, we generate k negative samples by sampling from the unigram probability distribution over the vocabulary (making sure that none of these noise words match the positive word). Then we compute the negative log-likelihood loss for the each positive pair along with the k negative pairs:\n",
    "\n",
    "$L = -\\log(P(y=1|w_{pos},c)) - \\sum_{j=1}^k \\log(P(y=0|w^{j}_{neg},c)) = -\\log(\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})) - \\sum_{j=1}^k \\log(\\sigma (-\\vec{w}^{j}_{neg} \\cdot \\vec{c}))$\n",
    "\n",
    "We can think of each window position providing us with a batch of $L$ positive instances and $kL$ negative instances. Then we can minimize this loss via gradient descent. The gradients with are:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec c} = (\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})-1) \\vec{w}_{pos} + \\sum_{j=1}^k \\sigma (\\vec{w}^{j}_{neg} \\cdot \\vec{c}) \\vec{w}^{j}_{neg}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec{w}_{pos}} = (\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})-1) \\vec{c}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec{w}^{j}_{neg}} = \\sigma (\\vec{w}^{j}_{neg} \\cdot \\vec{c}) \\vec{c}$\n",
    "\n",
    "\n",
    "Note: When sampling the noise words, instead of using the unigram probability distribution $P(w)$ over the vocabulary words, it's better to use a weighted version of this distirbution $P_{\\alpha}(w)$:\n",
    "\n",
    "$P(w) = \\frac{count(w)}{ \\sum_{w'\\in V} count(w')}$\n",
    "\n",
    "and the weighted unigram distribution is defined as:\n",
    "\n",
    "$P_{\\alpha}(w) = \\frac{count(w)^{\\alpha}}{ \\sum_{w'\\in V} count(w')^{\\alpha}}$\n",
    "\n",
    "where $\\alpha$ is an exponent between $0$ and $1$. This kind of weighting helps to slightly increase the probabilities of the rarer words and slightly suppressess the probability of the most common words. Empirically $\\alpha = 0.75$ tends to work well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement and train a skipgram word2vec model using the Stanford Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punc(w):\n",
    "    return any(c.isalpha() for c in w)\n",
    "\n",
    "# remove punctuations from list of words and apply lowercase folding \n",
    "def preprocess(s):\n",
    "    words = s.lower().strip().split()[1:]\n",
    "    words = [w for w in words if check_punc(w)]\n",
    "    return words\n",
    "\n",
    "# load dataset\n",
    "word_count = 0\n",
    "unigram_count = defaultdict(int)\n",
    "with open('datasetSentences.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    # lowercase folding and tokenize\n",
    "    sentences = []\n",
    "    for line in lines[1:]:\n",
    "        words = preprocess(line)\n",
    "        sentences.append(words)\n",
    "        for word in words:\n",
    "            unigram_count[word] += 1\n",
    "            word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 19371\n",
      "Num sentences: 11855\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "unk_token = \"<UNK>\"\n",
    "vocab = [unk_token] + sorted(list(set([word for sentence in sentences for word in sentence])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Num sentences: {len(sentences)}\")\n",
    "\n",
    "# tokenize the sentences\n",
    "sentences_tokenized = []\n",
    "for s in sentences:\n",
    "    sentences_tokenized.append([word2idx[w] for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram weighted probability distribution\n",
    "alpha = 0.75\n",
    "P_alpha = np.zeros(shape=(vocab_size))\n",
    "for i,w in enumerate(vocab):\n",
    "    P_alpha[i] = unigram_count[w]**alpha\n",
    "P_alpha = P_alpha / P_alpha.sum()    \n",
    "unigram_idx = np.arange(0,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of word2vec model\n",
    "D = 10 # embedding dim\n",
    "L = 5  # context window half-size\n",
    "\n",
    "# parameters: embedding matrices\n",
    "W = 0.001 * np.random.randn(vocab_size, D)\n",
    "C = 0.001 * np.random.randn(vocab_size, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_context(L):\n",
    "    # first randomly select a sentence\n",
    "    sent_idx = np.random.randint(0,len(sentences_tokenized)-1)\n",
    "    sent = sentences_tokenized[sent_idx]\n",
    "    # pick a random center word from the sentence\n",
    "    if len(sent) > L:\n",
    "        c_idx = np.random.randint(0,max(0,len(sent)-L))\n",
    "    else:\n",
    "        c_idx = 0\n",
    "\n",
    "    center_word = sent[c_idx]\n",
    "    context_words = sent[max(0,c_idx-L):c_idx] + sent[c_idx+1:c_idx+1+L]\n",
    "\n",
    "    if len(context_words) == 0:\n",
    "        return get_random_context(L)\n",
    "    else:\n",
    "        return center_word, context_words    \n",
    "\n",
    "\n",
    "def get_negative_samples(wpos_idx, k=10):\n",
    "    nsamples = 0\n",
    "    wnegs = []\n",
    "    # generate negative samples\n",
    "    while nsamples < k:\n",
    "        wneg_idx = np.random.choice(unigram_idx, size=1, p=P_alpha)[0]\n",
    "        if wneg_idx != wpos_idx:\n",
    "            wnegs.append(wneg_idx)\n",
    "            nsamples += 1\n",
    "    return wnegs    \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# skip-gram with negative samplice loss for a single (w,c) pair\n",
    "def compute_loss_and_grads(wpos_idx, c_idx, W, C):\n",
    "    # get negative samples\n",
    "    wnegs_idx = get_negative_samples(wpos_idx)\n",
    "    # get embedding vectors\n",
    "    V, D = W.shape\n",
    "    c = C[c_idx]   # shape: (D,)\n",
    "    w_pos = W[wpos_idx]  # shape: (D,)\n",
    "    w_negs = W[wnegs_idx]  # shape: (k,D)\n",
    "    \n",
    "    s_wpos_dot_c = sigmoid(np.dot(w_pos,c))  # shape: (1,)\n",
    "    s_wneg_dot_c = sigmoid(np.dot(w_negs,c)).reshape(w_negs.shape[0],1)  # shape: (k,1)\n",
    "\n",
    "    # compute loss\n",
    "    loss =  -np.log(s_wpos_dot_c) - np.log(1-s_wneg_dot_c).sum()\n",
    "\n",
    "    # compute gradients\n",
    "    grad_c = (s_wpos_dot_c-1) * w_pos +  (s_wneg_dot_c * w_negs).sum(axis=0)  # shape: (D,)\n",
    "    grad_wpos = (s_wpos_dot_c-1) * c  # shape: (D,)\n",
    "    grad_wnegs = s_wneg_dot_c * c  # shape: (k,D)\n",
    "    \n",
    "    return loss, grad_c, grad_wpos, grad_wnegs, wnegs_idx\n",
    "\n",
    "\n",
    "# compute total loss and accumulated gradients for a single context window\n",
    "def skipgram(center_word_idx, context_words_idx, W, C):\n",
    "    grad_C = np.zeros_like(C) \n",
    "    grad_W = np.zeros_like(W) \n",
    "    total_loss = 0.0\n",
    "\n",
    "    for wpos_idx in context_words_idx:\n",
    "        loss, grad_c, grad_wpos, grad_wnegs, wnegs_idx = compute_loss_and_grads(wpos_idx, center_word_idx, W, C) \n",
    "        total_loss += loss\n",
    "        grad_C[center_word_idx] += grad_c\n",
    "        grad_W[wnegs_idx] += grad_wnegs\n",
    "        grad_W[wpos_idx] += grad_wpos\n",
    "\n",
    "    return total_loss, grad_W, grad_C\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
