{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Word2Vec (with Negative Sampling)\n",
    "\n",
    "The skip-gram word2vec algorithm is a simple self-supervised model for learning dense word embedding vectors from a corpus of text. It is trained on the task of predicting the `probability distribution` over `context words` given a `center word`, for all possible center words from the vocabulary. The parameters of this model consist of two separate $|V| \\times D$ embedding matrices ($|V|$ is the vocab size and $D$ is the embedding dimensions): matrix $W$ whose rows are the embeddings of outside words and matrix $C$ whose rows are the embeddings of center words. (We could instead have one single embedding for both outside and context words, however this approach of having separate embeddings is more convenient). Then the model for computing the probability distribution is simply defined as follows:\n",
    "\n",
    "$P(w|c) = \\frac{exp(\\vec{w} \\cdot \\vec{c})}{\\sum_{w' \\in V} exp(\\vec{w}' \\cdot \\vec{c})}$\n",
    "\n",
    "where $\\vec{w}$ and $\\vec{c}$ are the embedding vectors of the context and center words resepectively. Note that this is just a softmax over all the dot products of every possible context word $w$ given a particluar center word $c$.\n",
    "\n",
    "Now due to large volcabulary size $|V|$, computing this probability distribution is very inefficient (because of the sum in the denominator). Instead of computing a probability distribution over all possible context words, we instead simplify our task into a `binary classification` problem. Given a pair of context word $w$ and center word $c$, our simplified task is to train a logistic regression classifier to predict whether $w$ actually occurs in the context of $c$ or not (we use label $1$ for True and $0$ for False). We define this simple binary classification problem as follows:\n",
    "\n",
    "$P(y=1|w_{pos},c) = \\sigma (\\vec{w}_{pos} \\cdot \\vec{c})$\n",
    "\n",
    "$P(y=0|w_{neg},c) = 1-\\sigma (\\vec{w}_{neg} \\cdot \\vec{c}) = \\sigma (-\\vec{w}_{neg} \\cdot \\vec{c})$\n",
    "\n",
    "where $\\sigma()$ is the sigmoid function, $w_{pos}$ denotes a true context word and $w_{neg}$ denotes a `noise` word which is not a true context word. For training this classifier, we will use $k$ times as many noise words than context words (this reflects the fact that each center word will have far fewer words from the vocab that appear in it's context than words that don't). During training, we will simply slide a context window of half-size $L$ over the training corpus, so at each position this gives us $2L$ different positive pairs $\\{(w^{i}_{pos},c) | i =1,2..,2L\\}$. For each of these positive pairs, we generate k negative samples by sampling from the unigram probability distribution over the vocabulary (making sure that none of these noise words match the positive word). Then we compute the negative log-likelihood loss for the each positive pair along with the k negative pairs:\n",
    "\n",
    "$L = -\\log(P(y=1|w_{pos},c)) - \\sum_{j=1}^k \\log(P(y=0|w^{j}_{neg},c)) = -\\log(\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})) - \\sum_{j=1}^k \\log(\\sigma (-\\vec{w}^{j}_{neg} \\cdot \\vec{c}))$\n",
    "\n",
    "We can think of each window position providing us with a batch of $L$ positive instances and $kL$ negative instances. Then we can minimize this loss via gradient descent. The gradients with are:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec c} = (\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})-1) \\vec{w}_{pos} + \\sum_{j=1}^k \\sigma (\\vec{w}^{j}_{neg} \\cdot \\vec{c}) \\vec{w}^{j}_{neg}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec{w}_{pos}} = (\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})-1) \\vec{c}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec{w}^{j}_{neg}} = \\sigma (\\vec{w}^{j}_{neg} \\cdot \\vec{c}) \\vec{c}$\n",
    "\n",
    "\n",
    "Note: When sampling the noise words, instead of using the unigram probability distribution $P(w)$ over the vocabulary words, it's better to use a weighted version of this distirbution $P_{\\alpha}(w)$:\n",
    "\n",
    "$P(w) = \\frac{count(w)}{ \\sum_{w'\\in V} count(w')}$\n",
    "\n",
    "which is the unigram distribution and the weighted unigram distribution is defined as:\n",
    "\n",
    "$P_{\\alpha}(w) = \\frac{count(w)^{\\alpha}}{ \\sum_{w'\\in V} count(w')^{\\alpha}}$\n",
    "\n",
    "where $\\alpha$ is an exponent between $0$ and $1$. This kind of weighting helps to slightly increase the probabilities of the rarer words and slightly suppressess the probability of the most common words. Empirically $\\alpha = 0.75$ tends to work well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement and train a skipgram word2vec model using the Stanford Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punc(w):\n",
    "    return any(c.isalpha() for c in w)\n",
    "\n",
    "# remove punctuations from list of words and apply lowercase folding \n",
    "def preprocess(s):\n",
    "    words = s.lower().strip().split()[1:]\n",
    "    words = [w for w in words if check_punc(w)]\n",
    "    return words\n",
    "\n",
    "# load dataset\n",
    "word_count = 0\n",
    "unigram_count = defaultdict(int)\n",
    "wierd_words = []\n",
    "with open('datasetSentences.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    # preprocessing\n",
    "    sentences = []\n",
    "    for line in lines[1:]:\n",
    "        words = preprocess(line)\n",
    "        s = []\n",
    "        for word in words:\n",
    "            if \"\\/\" in word:\n",
    "                ws = word.replace(\"\\/\", \" \").split()\n",
    "                for w in ws:\n",
    "                    s.append(w)\n",
    "                    unigram_count[w] += 1\n",
    "                    word_count += 1\n",
    "            else:\n",
    "                s.append(word)    \n",
    "                unigram_count[word] += 1\n",
    "                word_count += 1\n",
    "        sentences.append(s)        \n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The skipgram paper uses a subsampling strategy to get rid of the most frequent words, like stop words. Each word $w_i$ in the training corpus is discarded with a probability given by the following:\n",
    "\n",
    "$P(w_i) = 1 - \\sqrt{\\frac{T}{count(w_i)}}$ \n",
    "\n",
    "where $T$ is a threshold value which is a small fraction of the corpus total token count (~$10^{-5} \\times N$) and $count(w_i)$ is the frequency of that word in the corpus. For more frequent words, the square root term is very close to zero and so the word will get discarder with high probability.\n",
    "\n",
    "We also keep multiple copies of the same sentence to reduce the chances of entirely losing important words due to the subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_prob(word, t=1e-4):\n",
    "    p = max(0, 1 - np.sqrt(t*word_count/unigram_count[word]))\n",
    "    return p\n",
    "\n",
    "num_copies = 10\n",
    "discard_probs = {w:subsample_prob(w) for w in unigram_count.keys()}\n",
    "sentences_subsampled = [[word for word in s if np.random.random() >= discard_probs[word]] for s in sentences*num_copies]\n",
    "\n",
    "# remove zero length subsampled sentences\n",
    "sentences_subsampled = [s for s in sentences_subsampled if len(s) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling:  ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', 'conan', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal']\n",
      "After subsampling:  ['rock', 'destined', '21st', 'century', 'conan', 'going', 'splash', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean-claud', 'van', 'damme', 'steven', 'segal']\n",
      "Before subsampling:  ['the', 'gorgeously', 'elaborate', 'continuation', 'of', 'the', 'lord', 'of', 'the', 'rings', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer', 'director', 'peter', 'jackson', \"'s\", 'expanded', 'vision', 'of', 'j.r.r.', 'tolkien', \"'s\", 'middle-earth']\n",
      "After subsampling:  ['gorgeously', 'elaborate', 'continuation', 'lord', 'rings', 'trilogy', 'huge', 'column', 'words', 'adequately', 'describe', 'co-writer', 'peter', 'jackson', 'expanded', 'vision', 'j.r.r.', 'tolkien', 'middle-earth']\n",
      "Before subsampling:  ['effective', 'but', 'too-tepid', 'biopic']\n",
      "After subsampling:  ['effective', 'too-tepid', 'biopic']\n",
      "Before subsampling:  ['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', 'wasabi', 'is', 'a', 'good', 'place', 'to', 'start']\n",
      "After subsampling:  ['go', 'to', 'fun', 'wasabi', 'good', 'place', 'start']\n",
      "Before subsampling:  ['emerges', 'as', 'something', 'rare', 'an', 'issue', 'movie', 'that', \"'s\", 'so', 'honest', 'and', 'keenly', 'observed', 'that', 'it', 'does', \"n't\", 'feel', 'like', 'one']\n",
      "After subsampling:  ['emerges', 'rare', 'issue', 'honest', 'keenly', 'observed', \"n't\", 'one']\n",
      "Before subsampling:  ['the', 'film', 'provides', 'some', 'great', 'insight', 'into', 'the', 'neurotic', 'mindset', 'of', 'all', 'comics', 'even', 'those', 'who', 'have', 'reached', 'the', 'absolute', 'top', 'of', 'the', 'game']\n",
      "After subsampling:  ['provides', 'insight', 'neurotic', 'mindset', 'comics', 'even', 'have', 'reached', 'absolute', 'top', 'game']\n",
      "Before subsampling:  ['offers', 'that', 'rare', 'combination', 'of', 'entertainment', 'and', 'education']\n",
      "After subsampling:  ['offers', 'that', 'combination', 'education']\n"
     ]
    }
   ],
   "source": [
    "# compare before and after subsampling\n",
    "for i in range(7):\n",
    "    print(\"Before subsampling: \", sentences[i])\n",
    "    print(\"After subsampling: \", sentences_subsampled[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of the stop words are gone after subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 19332\n",
      "Num sentences: 117918\n",
      "Total number of tokens: 956449\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "vocab = sorted(list(set([word for sentence in sentences_subsampled for word in sentence])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Num sentences: {len(sentences_subsampled)}\")\n",
    "print(f\"Total number of tokens: {sum(len(s) for s in sentences_subsampled)}\")\n",
    "\n",
    "# tokenize the sentences\n",
    "sentences_tokenized = []\n",
    "for s in sentences_subsampled:\n",
    "    sentences_tokenized.append([word2idx[w] for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram weighted probability distribution\n",
    "alpha = 0.75\n",
    "P_alpha = np.zeros(shape=(vocab_size))\n",
    "for i,w in enumerate(vocab):\n",
    "    P_alpha[i] = unigram_count[w]**alpha\n",
    "P_alpha = P_alpha / P_alpha.sum()    \n",
    "unigram_idx = np.arange(0,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of word2vec model\n",
    "D = 32 # embedding dim\n",
    "L = 8  # context window half-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of sliding context window over every position from start to end of corpus, we will instead randomly select a batch of context windows on every epoch. We will also add some randomness to the context window size, by sampling a random size between [1,L]. This ensures that we get smaller context windows more often than longer windows which is helpful because context words that are closer should be related more strongly on the center word than context words that are farther away. Closer context words should therefore be sampled more often.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_context(L):\n",
    "    # first randomly select a sentence\n",
    "    sent_idx = np.random.randint(0,len(sentences_tokenized)-1)\n",
    "    sent = sentences_tokenized[sent_idx]\n",
    "    # pick random context window half-length between [1..L]\n",
    "    R = np.random.randint(1, L)\n",
    "    # pick a random center word from the sentence\n",
    "    if len(sent) > R:\n",
    "        c_idx = np.random.randint(0,len(sent)-1)\n",
    "    else:\n",
    "        c_idx = 0\n",
    "\n",
    "    center_word = sent[c_idx]\n",
    "    context_words = sent[max(0,c_idx-R):c_idx] + sent[c_idx+1:c_idx+1+R]\n",
    "\n",
    "    if len(context_words) == 0:\n",
    "        return get_random_context(L)\n",
    "    else:\n",
    "        return center_word, context_words    \n",
    "\n",
    "\n",
    "def get_negative_samples(wpos_idx, k=10):\n",
    "    nsamples = 0\n",
    "    wnegs = []\n",
    "    # generate negative samples\n",
    "    while nsamples < k:\n",
    "        wneg_idx = np.random.choice(unigram_idx, size=1, p=P_alpha)[0]\n",
    "        # make sure noise words don't match the positive word\n",
    "        if wneg_idx != wpos_idx:\n",
    "            wnegs.append(wneg_idx)\n",
    "            nsamples += 1\n",
    "    return wnegs    \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# skip-gram with negative samplice loss for a single (w,c) pair\n",
    "def compute_loss_and_grads(wpos_idx, c_idx, W, C):\n",
    "    # get negative samples\n",
    "    wnegs_idx = get_negative_samples(wpos_idx)\n",
    "    # get embedding vectors\n",
    "    V, D = W.shape\n",
    "    c = C[c_idx]   # shape: (D,)\n",
    "    w_pos = W[wpos_idx]  # shape: (D,)\n",
    "    w_negs = W[wnegs_idx]  # shape: (k,D)\n",
    "    \n",
    "    s_wpos_dot_c = sigmoid(np.dot(w_pos,c))  # shape: (1,)\n",
    "    s_wneg_dot_c = sigmoid(np.dot(w_negs,c)).reshape(w_negs.shape[0],1)  # shape: (k,1)\n",
    "\n",
    "    # compute loss\n",
    "    loss =  -np.log(s_wpos_dot_c) - np.log(1-s_wneg_dot_c).sum()\n",
    "\n",
    "    # compute gradients\n",
    "    grad_c = (s_wpos_dot_c-1) * w_pos +  (s_wneg_dot_c * w_negs).sum(axis=0)  # shape: (D,)\n",
    "    grad_wpos = (s_wpos_dot_c-1) * c  # shape: (D,)\n",
    "    grad_wnegs = s_wneg_dot_c * c  # shape: (k,D)\n",
    "    \n",
    "    return loss, grad_c, grad_wpos, grad_wnegs, wnegs_idx\n",
    "\n",
    "\n",
    "# compute total loss and accumulated gradients for a single context window\n",
    "def skipgram(center_word_idx, context_words_idx, W, C):\n",
    "    grad_C = np.zeros_like(C) \n",
    "    grad_W = np.zeros_like(W) \n",
    "    total_loss = 0.0\n",
    "\n",
    "    # compute loss and accumulate gradients for each positive context word and negative samples\n",
    "    for wpos_idx in context_words_idx:\n",
    "        loss, grad_c, grad_wpos, grad_wnegs, wnegs_idx = compute_loss_and_grads(wpos_idx, center_word_idx, W, C) \n",
    "        total_loss += loss\n",
    "        grad_C[center_word_idx] += grad_c\n",
    "        # negative samples could be repeated, so we need to be more careful about adding all the contirbutions\n",
    "        # can't use += operator which will only add repeated contirbutions once\n",
    "        np.add.at(grad_W, wnegs_idx, grad_wnegs)\n",
    "        grad_W[wpos_idx] += grad_wpos\n",
    "\n",
    "    return total_loss, grad_W, grad_C\n",
    "\n",
    "\n",
    "# perform gradient descent update of parameters over a mini batch\n",
    "def train_step(W, C, L, batch_size, alpha):\n",
    "    grad_C = np.zeros_like(C) \n",
    "    grad_W = np.zeros_like(W)\n",
    "    total_loss = 0 \n",
    "    for _ in range(batch_size):\n",
    "        # get a random context window\n",
    "        center_word_idx, context_words_idx = get_random_context(L)\n",
    "        # compute loss and gradients for this window \n",
    "        loss, grad_W_window, grad_C_window = skipgram(center_word_idx, context_words_idx, W, C)\n",
    "        # accumulate loss and grads\n",
    "        total_loss += loss\n",
    "        grad_W += grad_W_window\n",
    "        grad_C += grad_C_window\n",
    "\n",
    "    # average over mini-batch\n",
    "    total_loss /= batch_size\n",
    "    grad_W /= batch_size    \n",
    "    grad_C /= batch_size    \n",
    "\n",
    "    # perform sgd update of parameters\n",
    "    W -= alpha * grad_W\n",
    "    C -= alpha * grad_C\n",
    "\n",
    "    return W, C, total_loss\n",
    "\n",
    "# training loop\n",
    "def train(W, C, L, num_epochs=10, batch_size=32, alpha=0.01, print_every=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        W, C, loss = train_step(W, C, L, batch_size, alpha)\n",
    "        if epoch%print_every==0:\n",
    "            print(f\"Epoch #{epoch}, Train Loss: {loss}\")\n",
    "\n",
    "    return W, C    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Train Loss: 26.200831558512604\n",
      "Epoch #50, Train Loss: 23.382870880154247\n",
      "Epoch #100, Train Loss: 24.619770497523135\n",
      "Epoch #150, Train Loss: 24.43834585654396\n",
      "Epoch #200, Train Loss: 26.31312494344878\n",
      "Epoch #250, Train Loss: 24.96004441332466\n",
      "Epoch #300, Train Loss: 26.903127029699125\n",
      "Epoch #350, Train Loss: 25.32494968546413\n",
      "Epoch #400, Train Loss: 26.5285282611221\n",
      "Epoch #450, Train Loss: 25.17586418564848\n",
      "Epoch #500, Train Loss: 25.996913461858494\n",
      "Epoch #550, Train Loss: 27.228483777467112\n",
      "Epoch #600, Train Loss: 25.20039220782222\n",
      "Epoch #650, Train Loss: 22.765463235425944\n",
      "Epoch #700, Train Loss: 23.820647346142145\n",
      "Epoch #750, Train Loss: 26.113035287980914\n",
      "Epoch #800, Train Loss: 27.041191851181903\n",
      "Epoch #850, Train Loss: 23.575318658064297\n",
      "Epoch #900, Train Loss: 25.529429717116\n",
      "Epoch #950, Train Loss: 24.147396035317566\n",
      "Epoch #1000, Train Loss: 26.09767534426002\n",
      "Epoch #1050, Train Loss: 26.18397234267203\n",
      "Epoch #1100, Train Loss: 23.992082590350456\n",
      "Epoch #1150, Train Loss: 24.085493835074193\n",
      "Epoch #1200, Train Loss: 23.86682815317744\n",
      "Epoch #1250, Train Loss: 24.214619585951755\n",
      "Epoch #1300, Train Loss: 24.943077462651775\n",
      "Epoch #1350, Train Loss: 23.422375473820253\n",
      "Epoch #1400, Train Loss: 21.585753427204995\n",
      "Epoch #1450, Train Loss: 23.102201296628074\n",
      "Epoch #1500, Train Loss: 22.933360399304107\n",
      "Epoch #1550, Train Loss: 26.027897986436855\n",
      "Epoch #1600, Train Loss: 23.307879662321998\n",
      "Epoch #1650, Train Loss: 24.796411203593223\n",
      "Epoch #1700, Train Loss: 24.4500611135246\n",
      "Epoch #1750, Train Loss: 21.274210916829315\n",
      "Epoch #1800, Train Loss: 24.99560849494444\n",
      "Epoch #1850, Train Loss: 22.369753901208227\n",
      "Epoch #1900, Train Loss: 24.508328501478342\n",
      "Epoch #1950, Train Loss: 23.81727407761879\n",
      "Epoch #2000, Train Loss: 20.88731132864794\n",
      "Epoch #2050, Train Loss: 24.39870476202613\n",
      "Epoch #2100, Train Loss: 24.785796062852196\n",
      "Epoch #2150, Train Loss: 25.168799181293565\n",
      "Epoch #2200, Train Loss: 24.69093886462607\n",
      "Epoch #2250, Train Loss: 25.617551610141078\n",
      "Epoch #2300, Train Loss: 21.53656441882385\n",
      "Epoch #2350, Train Loss: 23.88326653262486\n",
      "Epoch #2400, Train Loss: 23.507952626748576\n",
      "Epoch #2450, Train Loss: 22.36578773370924\n",
      "Epoch #2500, Train Loss: 23.29038005036966\n",
      "Epoch #2550, Train Loss: 23.672238703708757\n",
      "Epoch #2600, Train Loss: 24.98721099441442\n",
      "Epoch #2650, Train Loss: 23.613798222528207\n",
      "Epoch #2700, Train Loss: 23.76163009699066\n",
      "Epoch #2750, Train Loss: 22.526289563291307\n",
      "Epoch #2800, Train Loss: 21.59640954687112\n",
      "Epoch #2850, Train Loss: 22.391286887211994\n",
      "Epoch #2900, Train Loss: 22.089639856318723\n",
      "Epoch #2950, Train Loss: 22.82749666384278\n",
      "Epoch #3000, Train Loss: 21.783154660470263\n",
      "Epoch #3050, Train Loss: 23.673513006359062\n",
      "Epoch #3100, Train Loss: 23.588021957521363\n",
      "Epoch #3150, Train Loss: 23.14441450916273\n",
      "Epoch #3200, Train Loss: 22.58985922936845\n",
      "Epoch #3250, Train Loss: 22.072452549370134\n",
      "Epoch #3300, Train Loss: 23.260571225160973\n",
      "Epoch #3350, Train Loss: 21.699797710488834\n",
      "Epoch #3400, Train Loss: 23.757960368854764\n",
      "Epoch #3450, Train Loss: 22.384276191760854\n",
      "Epoch #3500, Train Loss: 23.456182249403625\n",
      "Epoch #3550, Train Loss: 23.722350171093126\n",
      "Epoch #3600, Train Loss: 23.43395553383004\n",
      "Epoch #3650, Train Loss: 23.317051489812386\n",
      "Epoch #3700, Train Loss: 23.458755100589475\n",
      "Epoch #3750, Train Loss: 21.01269288283121\n",
      "Epoch #3800, Train Loss: 22.79348320837455\n",
      "Epoch #3850, Train Loss: 20.359979261045616\n",
      "Epoch #3900, Train Loss: 19.581932888639784\n",
      "Epoch #3950, Train Loss: 22.241435982488984\n",
      "Epoch #4000, Train Loss: 21.08372051202674\n",
      "Epoch #4050, Train Loss: 20.664206865738635\n",
      "Epoch #4100, Train Loss: 22.427532583613644\n",
      "Epoch #4150, Train Loss: 23.38663770858341\n",
      "Epoch #4200, Train Loss: 23.657092731683537\n",
      "Epoch #4250, Train Loss: 22.573713449039673\n",
      "Epoch #4300, Train Loss: 22.916420655031096\n",
      "Epoch #4350, Train Loss: 22.62160236994383\n",
      "Epoch #4400, Train Loss: 20.73345583364854\n",
      "Epoch #4450, Train Loss: 22.234822349720538\n",
      "Epoch #4500, Train Loss: 19.509735313238046\n",
      "Epoch #4550, Train Loss: 24.241093118129076\n",
      "Epoch #4600, Train Loss: 24.614823750257685\n",
      "Epoch #4650, Train Loss: 23.95629891674873\n",
      "Epoch #4700, Train Loss: 21.88314253097045\n",
      "Epoch #4750, Train Loss: 23.458447635264903\n",
      "Epoch #4800, Train Loss: 22.857439885700146\n",
      "Epoch #4850, Train Loss: 19.936642804686354\n",
      "Epoch #4900, Train Loss: 21.811729051251977\n",
      "Epoch #4950, Train Loss: 22.394328012135592\n"
     ]
    }
   ],
   "source": [
    "# parameters: embedding matrices\n",
    "#W = 0.001 * np.random.randn(vocab_size, D)\n",
    "#C = 0.001 * np.random.randn(vocab_size, D)\n",
    "\n",
    "W, C, = train(W, C, L, num_epochs=5000, batch_size=100, alpha=0.1, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Train Loss: 20.65272676847883\n",
      "Epoch #50, Train Loss: 22.239343190398717\n",
      "Epoch #100, Train Loss: 20.96164221369737\n",
      "Epoch #150, Train Loss: 21.99620678015732\n",
      "Epoch #200, Train Loss: 17.174604699019362\n",
      "Epoch #250, Train Loss: 17.275600106957167\n",
      "Epoch #300, Train Loss: 19.987688334956484\n",
      "Epoch #350, Train Loss: 19.86592998149786\n",
      "Epoch #400, Train Loss: 21.268760764781064\n",
      "Epoch #450, Train Loss: 19.423829854752015\n",
      "Epoch #500, Train Loss: 18.66069026663032\n",
      "Epoch #550, Train Loss: 20.522580413296136\n",
      "Epoch #600, Train Loss: 16.930430685389723\n",
      "Epoch #650, Train Loss: 18.07658792431563\n",
      "Epoch #700, Train Loss: 19.948015291515734\n",
      "Epoch #750, Train Loss: 18.506683751477393\n",
      "Epoch #800, Train Loss: 17.747519006052435\n",
      "Epoch #850, Train Loss: 19.271912766334747\n",
      "Epoch #900, Train Loss: 18.515824848053803\n",
      "Epoch #950, Train Loss: 18.96908352420442\n",
      "Epoch #1000, Train Loss: 17.384877709700653\n",
      "Epoch #1050, Train Loss: 18.376070687138114\n",
      "Epoch #1100, Train Loss: 16.632214649504657\n",
      "Epoch #1150, Train Loss: 18.65256429686137\n",
      "Epoch #1200, Train Loss: 17.7224393612787\n",
      "Epoch #1250, Train Loss: 19.449377952528316\n",
      "Epoch #1300, Train Loss: 20.218993628305792\n",
      "Epoch #1350, Train Loss: 18.70552697143611\n",
      "Epoch #1400, Train Loss: 20.055820954751038\n",
      "Epoch #1450, Train Loss: 18.79285507766746\n",
      "Epoch #1500, Train Loss: 18.350949215002228\n",
      "Epoch #1550, Train Loss: 18.612652997048418\n",
      "Epoch #1600, Train Loss: 17.724071979194655\n",
      "Epoch #1650, Train Loss: 19.599069703960875\n",
      "Epoch #1700, Train Loss: 18.803960138074796\n",
      "Epoch #1750, Train Loss: 16.11504710588883\n",
      "Epoch #1800, Train Loss: 16.780013647124946\n",
      "Epoch #1850, Train Loss: 15.90039545278622\n",
      "Epoch #1900, Train Loss: 18.733123067404257\n",
      "Epoch #1950, Train Loss: 17.13372611923065\n",
      "Epoch #2000, Train Loss: 17.054643795811046\n",
      "Epoch #2050, Train Loss: 17.05233994598127\n",
      "Epoch #2100, Train Loss: 17.091484362745646\n",
      "Epoch #2150, Train Loss: 16.72273213932388\n",
      "Epoch #2200, Train Loss: 15.984664399667068\n",
      "Epoch #2250, Train Loss: 17.4481441654146\n",
      "Epoch #2300, Train Loss: 17.38995245473296\n",
      "Epoch #2350, Train Loss: 17.58741786964011\n",
      "Epoch #2400, Train Loss: 17.843371710303316\n",
      "Epoch #2450, Train Loss: 17.7485271400846\n",
      "Epoch #2500, Train Loss: 16.46661638782461\n",
      "Epoch #2550, Train Loss: 17.754523549153326\n",
      "Epoch #2600, Train Loss: 17.3940640886242\n",
      "Epoch #2650, Train Loss: 19.236429807785342\n",
      "Epoch #2700, Train Loss: 17.52382418588412\n",
      "Epoch #2750, Train Loss: 16.57531355619968\n",
      "Epoch #2800, Train Loss: 18.747897058467398\n",
      "Epoch #2850, Train Loss: 15.843135503080754\n",
      "Epoch #2900, Train Loss: 17.70173308036568\n",
      "Epoch #2950, Train Loss: 16.2289984476035\n",
      "Epoch #3000, Train Loss: 16.523931782143688\n",
      "Epoch #3050, Train Loss: 17.111457107140943\n",
      "Epoch #3100, Train Loss: 16.68984094387223\n",
      "Epoch #3150, Train Loss: 16.602155828032608\n",
      "Epoch #3200, Train Loss: 16.775969798608777\n",
      "Epoch #3250, Train Loss: 15.804345090388626\n",
      "Epoch #3300, Train Loss: 15.324629441571314\n",
      "Epoch #3350, Train Loss: 15.971903749432386\n",
      "Epoch #3400, Train Loss: 17.38452399702171\n",
      "Epoch #3450, Train Loss: 16.98211786220462\n",
      "Epoch #3500, Train Loss: 18.020466085909888\n",
      "Epoch #3550, Train Loss: 16.540058323056478\n",
      "Epoch #3600, Train Loss: 15.657653408393353\n",
      "Epoch #3650, Train Loss: 17.046469940460856\n",
      "Epoch #3700, Train Loss: 15.340223359326297\n",
      "Epoch #3750, Train Loss: 16.94233987325029\n",
      "Epoch #3800, Train Loss: 17.27290635962464\n",
      "Epoch #3850, Train Loss: 17.23523753376332\n",
      "Epoch #3900, Train Loss: 18.390019157097893\n",
      "Epoch #3950, Train Loss: 16.635007701288426\n",
      "Epoch #4000, Train Loss: 15.629109705570933\n",
      "Epoch #4050, Train Loss: 17.987895475841583\n",
      "Epoch #4100, Train Loss: 16.544678469685497\n",
      "Epoch #4150, Train Loss: 16.000642827412673\n",
      "Epoch #4200, Train Loss: 15.293818856539165\n",
      "Epoch #4250, Train Loss: 17.85684208118722\n",
      "Epoch #4300, Train Loss: 17.20147700558721\n",
      "Epoch #4350, Train Loss: 16.9384614458192\n",
      "Epoch #4400, Train Loss: 15.907982737843291\n",
      "Epoch #4450, Train Loss: 16.095390812242496\n",
      "Epoch #4500, Train Loss: 17.19529032179992\n",
      "Epoch #4550, Train Loss: 15.74862359160977\n",
      "Epoch #4600, Train Loss: 15.04000293253636\n",
      "Epoch #4650, Train Loss: 15.36483080008346\n",
      "Epoch #4700, Train Loss: 16.83517509949247\n",
      "Epoch #4750, Train Loss: 15.523037958590926\n",
      "Epoch #4800, Train Loss: 16.926121318589097\n",
      "Epoch #4850, Train Loss: 16.486946101094617\n",
      "Epoch #4900, Train Loss: 16.03625831166887\n",
      "Epoch #4950, Train Loss: 14.126667156901417\n",
      "Epoch #5000, Train Loss: 16.030567130597333\n",
      "Epoch #5050, Train Loss: 16.43422617235046\n",
      "Epoch #5100, Train Loss: 19.197601937907283\n",
      "Epoch #5150, Train Loss: 14.857835322012145\n",
      "Epoch #5200, Train Loss: 16.25780409782261\n",
      "Epoch #5250, Train Loss: 15.653244171760464\n",
      "Epoch #5300, Train Loss: 16.76831678262257\n",
      "Epoch #5350, Train Loss: 17.304627949992195\n",
      "Epoch #5400, Train Loss: 15.730213566468395\n",
      "Epoch #5450, Train Loss: 16.83807572914136\n",
      "Epoch #5500, Train Loss: 16.98301310491726\n",
      "Epoch #5550, Train Loss: 15.220322171040332\n",
      "Epoch #5600, Train Loss: 15.774156565639396\n",
      "Epoch #5650, Train Loss: 18.960025916513715\n",
      "Epoch #5700, Train Loss: 15.683061159299848\n",
      "Epoch #5750, Train Loss: 15.926410461072278\n",
      "Epoch #5800, Train Loss: 16.91390277847242\n",
      "Epoch #5850, Train Loss: 16.831475222504036\n",
      "Epoch #5900, Train Loss: 16.68026227268547\n",
      "Epoch #5950, Train Loss: 17.022432741524142\n",
      "Epoch #6000, Train Loss: 16.44299495285181\n",
      "Epoch #6050, Train Loss: 16.61295048391811\n",
      "Epoch #6100, Train Loss: 17.3830695677514\n",
      "Epoch #6150, Train Loss: 16.29881078438943\n",
      "Epoch #6200, Train Loss: 15.431463628391498\n",
      "Epoch #6250, Train Loss: 15.896697491514917\n",
      "Epoch #6300, Train Loss: 15.774128672915568\n",
      "Epoch #6350, Train Loss: 16.057616712750196\n",
      "Epoch #6400, Train Loss: 17.163558232230248\n",
      "Epoch #6450, Train Loss: 16.08664214074757\n",
      "Epoch #6500, Train Loss: 15.332955432715561\n",
      "Epoch #6550, Train Loss: 15.564448951052325\n",
      "Epoch #6600, Train Loss: 15.1692962417291\n",
      "Epoch #6650, Train Loss: 15.939424521425446\n",
      "Epoch #6700, Train Loss: 16.623420175699565\n",
      "Epoch #6750, Train Loss: 17.31774880156001\n",
      "Epoch #6800, Train Loss: 16.412876053335086\n",
      "Epoch #6850, Train Loss: 16.106424093570798\n",
      "Epoch #6900, Train Loss: 16.855228498254746\n",
      "Epoch #6950, Train Loss: 16.587557662354893\n",
      "Epoch #7000, Train Loss: 16.185053919678456\n",
      "Epoch #7050, Train Loss: 14.159050555496565\n",
      "Epoch #7100, Train Loss: 14.358666272714013\n",
      "Epoch #7150, Train Loss: 15.276036645525716\n",
      "Epoch #7200, Train Loss: 17.110295510841087\n",
      "Epoch #7250, Train Loss: 15.308170087588149\n",
      "Epoch #7300, Train Loss: 16.475888351955597\n",
      "Epoch #7350, Train Loss: 15.978204132223325\n",
      "Epoch #7400, Train Loss: 15.591814143556173\n",
      "Epoch #7450, Train Loss: 16.822560787343335\n",
      "Epoch #7500, Train Loss: 16.929777144917754\n",
      "Epoch #7550, Train Loss: 15.639574149586688\n",
      "Epoch #7600, Train Loss: 14.396116583763288\n",
      "Epoch #7650, Train Loss: 17.875631409724242\n",
      "Epoch #7700, Train Loss: 15.878292327529103\n",
      "Epoch #7750, Train Loss: 16.095818401709302\n",
      "Epoch #7800, Train Loss: 16.58603565546429\n",
      "Epoch #7850, Train Loss: 16.013474585777217\n",
      "Epoch #7900, Train Loss: 17.00290517997202\n",
      "Epoch #7950, Train Loss: 15.612640968618889\n",
      "Epoch #8000, Train Loss: 14.950015570087299\n",
      "Epoch #8050, Train Loss: 15.972976683251478\n",
      "Epoch #8100, Train Loss: 15.346605023957268\n",
      "Epoch #8150, Train Loss: 15.713929966507985\n",
      "Epoch #8200, Train Loss: 16.17834315121756\n",
      "Epoch #8250, Train Loss: 15.753626934836445\n",
      "Epoch #8300, Train Loss: 14.954531396687964\n",
      "Epoch #8350, Train Loss: 16.33263788146526\n",
      "Epoch #8400, Train Loss: 16.64680900457881\n",
      "Epoch #8450, Train Loss: 17.146627260838596\n",
      "Epoch #8500, Train Loss: 16.52441575126362\n",
      "Epoch #8550, Train Loss: 15.400336131903662\n",
      "Epoch #8600, Train Loss: 15.897414571625204\n",
      "Epoch #8650, Train Loss: 16.920691683058187\n",
      "Epoch #8700, Train Loss: 14.930410451121645\n",
      "Epoch #8750, Train Loss: 14.307835094310667\n",
      "Epoch #8800, Train Loss: 15.628211017579426\n",
      "Epoch #8850, Train Loss: 16.321348304102383\n",
      "Epoch #8900, Train Loss: 16.52847613963805\n",
      "Epoch #8950, Train Loss: 15.76594499972894\n",
      "Epoch #9000, Train Loss: 15.314699607624355\n",
      "Epoch #9050, Train Loss: 15.815403650571689\n",
      "Epoch #9100, Train Loss: 15.447323834773464\n",
      "Epoch #9150, Train Loss: 15.150653776227847\n",
      "Epoch #9200, Train Loss: 14.92052896399048\n",
      "Epoch #9250, Train Loss: 14.613263269057532\n",
      "Epoch #9300, Train Loss: 16.56137153258653\n",
      "Epoch #9350, Train Loss: 15.542002489268903\n",
      "Epoch #9400, Train Loss: 16.00755828915817\n",
      "Epoch #9450, Train Loss: 15.321117400030557\n",
      "Epoch #9500, Train Loss: 14.920529426744206\n",
      "Epoch #9550, Train Loss: 16.815000866301073\n",
      "Epoch #9600, Train Loss: 14.25267399291492\n",
      "Epoch #9650, Train Loss: 15.31315541895169\n",
      "Epoch #9700, Train Loss: 14.670598702873713\n",
      "Epoch #9750, Train Loss: 15.372378098750907\n",
      "Epoch #9800, Train Loss: 16.653560168622246\n",
      "Epoch #9850, Train Loss: 14.892888353128749\n",
      "Epoch #9900, Train Loss: 16.148871581273603\n",
      "Epoch #9950, Train Loss: 16.45636725060739\n",
      "Epoch #10000, Train Loss: 15.049179715646446\n",
      "Epoch #10050, Train Loss: 14.049151912554018\n",
      "Epoch #10100, Train Loss: 16.24595544760607\n",
      "Epoch #10150, Train Loss: 12.772147820444765\n",
      "Epoch #10200, Train Loss: 16.49570600531019\n",
      "Epoch #10250, Train Loss: 16.962707301876705\n",
      "Epoch #10300, Train Loss: 15.651584386453969\n",
      "Epoch #10350, Train Loss: 16.463300557898936\n",
      "Epoch #10400, Train Loss: 14.849642321570167\n",
      "Epoch #10450, Train Loss: 15.435178978740147\n",
      "Epoch #10500, Train Loss: 14.443913508212544\n",
      "Epoch #10550, Train Loss: 16.743871578901167\n",
      "Epoch #10600, Train Loss: 15.811801722196542\n",
      "Epoch #10650, Train Loss: 15.01888195939106\n",
      "Epoch #10700, Train Loss: 16.77648220673433\n",
      "Epoch #10750, Train Loss: 17.483849683808916\n",
      "Epoch #10800, Train Loss: 15.049945062395631\n",
      "Epoch #10850, Train Loss: 14.251369264594286\n",
      "Epoch #10900, Train Loss: 15.698859421346329\n",
      "Epoch #10950, Train Loss: 15.865947239253702\n",
      "Epoch #11000, Train Loss: 15.50354727995908\n",
      "Epoch #11050, Train Loss: 14.711036587662084\n",
      "Epoch #11100, Train Loss: 14.02008007051898\n",
      "Epoch #11150, Train Loss: 15.111170978151641\n",
      "Epoch #11200, Train Loss: 15.594012519852438\n",
      "Epoch #11250, Train Loss: 16.22715067450714\n",
      "Epoch #11300, Train Loss: 15.690488990724013\n",
      "Epoch #11350, Train Loss: 15.206899545142821\n",
      "Epoch #11400, Train Loss: 14.149980866317689\n",
      "Epoch #11450, Train Loss: 16.496448716255774\n",
      "Epoch #11500, Train Loss: 16.60455281782815\n",
      "Epoch #11550, Train Loss: 14.598736993463092\n",
      "Epoch #11600, Train Loss: 15.307339739512294\n",
      "Epoch #11650, Train Loss: 17.100472174331575\n",
      "Epoch #11700, Train Loss: 15.222193603042284\n",
      "Epoch #11750, Train Loss: 15.86352247093425\n",
      "Epoch #11800, Train Loss: 15.05441785194352\n",
      "Epoch #11850, Train Loss: 16.437728911768502\n",
      "Epoch #11900, Train Loss: 15.328937708260192\n",
      "Epoch #11950, Train Loss: 14.641076969471127\n",
      "Epoch #12000, Train Loss: 16.59788801477829\n",
      "Epoch #12050, Train Loss: 15.235756788809475\n",
      "Epoch #12100, Train Loss: 15.746320034362284\n",
      "Epoch #12150, Train Loss: 14.935137635635062\n",
      "Epoch #12200, Train Loss: 15.61380028909186\n",
      "Epoch #12250, Train Loss: 14.984163554216618\n",
      "Epoch #12300, Train Loss: 15.500174659944257\n",
      "Epoch #12350, Train Loss: 13.996081254189788\n",
      "Epoch #12400, Train Loss: 15.261702011920947\n",
      "Epoch #12450, Train Loss: 15.239341811771496\n",
      "Epoch #12500, Train Loss: 14.522724921023334\n",
      "Epoch #12550, Train Loss: 16.237701940655633\n",
      "Epoch #12600, Train Loss: 15.537437988151567\n",
      "Epoch #12650, Train Loss: 13.679505263877186\n",
      "Epoch #12700, Train Loss: 16.866583465571818\n",
      "Epoch #12750, Train Loss: 13.083993240137637\n",
      "Epoch #12800, Train Loss: 15.833201458264194\n",
      "Epoch #12850, Train Loss: 13.847192077578766\n",
      "Epoch #12900, Train Loss: 14.054733823572679\n",
      "Epoch #12950, Train Loss: 15.398449718647196\n",
      "Epoch #13000, Train Loss: 14.60747433008707\n",
      "Epoch #13050, Train Loss: 14.69096018763418\n",
      "Epoch #13100, Train Loss: 16.423786438062166\n",
      "Epoch #13150, Train Loss: 15.207137966730246\n",
      "Epoch #13200, Train Loss: 16.46814815834044\n",
      "Epoch #13250, Train Loss: 17.046531918698467\n",
      "Epoch #13300, Train Loss: 15.504108154077278\n",
      "Epoch #13350, Train Loss: 15.724197393069794\n",
      "Epoch #13400, Train Loss: 15.914831595673467\n",
      "Epoch #13450, Train Loss: 17.299882923546676\n",
      "Epoch #13500, Train Loss: 15.730392133298082\n",
      "Epoch #13550, Train Loss: 15.246704400814167\n",
      "Epoch #13600, Train Loss: 15.693372710869959\n",
      "Epoch #13650, Train Loss: 15.77066879885403\n",
      "Epoch #13700, Train Loss: 16.222952155614582\n",
      "Epoch #13750, Train Loss: 16.115682855822424\n",
      "Epoch #13800, Train Loss: 15.16380171924125\n",
      "Epoch #13850, Train Loss: 15.061245562615591\n",
      "Epoch #13900, Train Loss: 16.05072180992244\n",
      "Epoch #13950, Train Loss: 14.586021234957455\n",
      "Epoch #14000, Train Loss: 15.515667775733876\n",
      "Epoch #14050, Train Loss: 14.099158235756933\n",
      "Epoch #14100, Train Loss: 13.991626177504886\n",
      "Epoch #14150, Train Loss: 14.742861687247688\n",
      "Epoch #14200, Train Loss: 17.279964310810655\n",
      "Epoch #14250, Train Loss: 15.865184652486779\n",
      "Epoch #14300, Train Loss: 14.321227651118885\n",
      "Epoch #14350, Train Loss: 15.709646812219905\n",
      "Epoch #14400, Train Loss: 16.967401284240086\n",
      "Epoch #14450, Train Loss: 16.148094005329845\n",
      "Epoch #14500, Train Loss: 15.854389928728216\n",
      "Epoch #14550, Train Loss: 15.977356832261561\n",
      "Epoch #14600, Train Loss: 16.40213749257556\n",
      "Epoch #14650, Train Loss: 15.174534231998884\n",
      "Epoch #14700, Train Loss: 16.63880980762418\n",
      "Epoch #14750, Train Loss: 14.538485518880432\n",
      "Epoch #14800, Train Loss: 15.47100837730425\n",
      "Epoch #14850, Train Loss: 15.930877853256977\n",
      "Epoch #14900, Train Loss: 14.52204354677678\n",
      "Epoch #14950, Train Loss: 15.796399958567374\n",
      "Epoch #15000, Train Loss: 16.778390861957476\n",
      "Epoch #15050, Train Loss: 17.546586537920334\n",
      "Epoch #15100, Train Loss: 12.95972888332184\n",
      "Epoch #15150, Train Loss: 13.750497504453692\n",
      "Epoch #15200, Train Loss: 14.160592824484606\n",
      "Epoch #15250, Train Loss: 16.670820268653028\n",
      "Epoch #15300, Train Loss: 16.27022530951625\n",
      "Epoch #15350, Train Loss: 15.740429756190608\n",
      "Epoch #15400, Train Loss: 15.112606703586332\n",
      "Epoch #15450, Train Loss: 16.23846170027784\n",
      "Epoch #15500, Train Loss: 15.187132463748714\n",
      "Epoch #15550, Train Loss: 14.76345877677795\n",
      "Epoch #15600, Train Loss: 14.725904183298441\n",
      "Epoch #15650, Train Loss: 14.574754991227021\n",
      "Epoch #15700, Train Loss: 15.232074294994877\n",
      "Epoch #15750, Train Loss: 14.994389993794165\n",
      "Epoch #15800, Train Loss: 16.533882443571912\n",
      "Epoch #15850, Train Loss: 16.837282830568544\n",
      "Epoch #15900, Train Loss: 15.142376624226818\n",
      "Epoch #15950, Train Loss: 15.266832677888265\n",
      "Epoch #16000, Train Loss: 15.781019301780143\n",
      "Epoch #16050, Train Loss: 15.579008331104083\n",
      "Epoch #16100, Train Loss: 15.318505925954423\n",
      "Epoch #16150, Train Loss: 16.147835888874663\n",
      "Epoch #16200, Train Loss: 15.252589604944484\n",
      "Epoch #16250, Train Loss: 15.84523198002177\n",
      "Epoch #16300, Train Loss: 15.233430864764523\n",
      "Epoch #16350, Train Loss: 15.55721538454089\n",
      "Epoch #16400, Train Loss: 14.526906463922568\n",
      "Epoch #16450, Train Loss: 16.26405505849972\n",
      "Epoch #16500, Train Loss: 15.60273088722495\n",
      "Epoch #16550, Train Loss: 16.648578021967083\n",
      "Epoch #16600, Train Loss: 16.556971679160945\n",
      "Epoch #16650, Train Loss: 15.177042260924072\n",
      "Epoch #16700, Train Loss: 15.219715142866256\n",
      "Epoch #16750, Train Loss: 14.369693566010328\n",
      "Epoch #16800, Train Loss: 14.447972890899699\n",
      "Epoch #16850, Train Loss: 14.313227131067425\n",
      "Epoch #16900, Train Loss: 14.429533706986962\n",
      "Epoch #16950, Train Loss: 16.054539804959635\n",
      "Epoch #17000, Train Loss: 15.697236865555144\n",
      "Epoch #17050, Train Loss: 14.622757921713065\n",
      "Epoch #17100, Train Loss: 14.813676280452997\n",
      "Epoch #17150, Train Loss: 15.429243765373126\n",
      "Epoch #17200, Train Loss: 15.659793063045196\n",
      "Epoch #17250, Train Loss: 15.689234703735686\n",
      "Epoch #17300, Train Loss: 16.20809055389829\n",
      "Epoch #17350, Train Loss: 16.465775648367607\n",
      "Epoch #17400, Train Loss: 16.712708510610856\n",
      "Epoch #17450, Train Loss: 14.995348980312212\n",
      "Epoch #17500, Train Loss: 15.123734846399703\n",
      "Epoch #17550, Train Loss: 16.366066666613957\n",
      "Epoch #17600, Train Loss: 15.304857293043735\n",
      "Epoch #17650, Train Loss: 13.85959071673651\n",
      "Epoch #17700, Train Loss: 15.372517778639835\n",
      "Epoch #17750, Train Loss: 16.569344346807178\n",
      "Epoch #17800, Train Loss: 15.083094609083025\n",
      "Epoch #17850, Train Loss: 15.447777543457526\n",
      "Epoch #17900, Train Loss: 15.607402618631594\n",
      "Epoch #17950, Train Loss: 15.095691452093742\n",
      "Epoch #18000, Train Loss: 15.136611670499674\n",
      "Epoch #18050, Train Loss: 15.353061661129976\n",
      "Epoch #18100, Train Loss: 16.550280271488244\n",
      "Epoch #18150, Train Loss: 17.37756691136682\n",
      "Epoch #18200, Train Loss: 16.24576558878648\n",
      "Epoch #18250, Train Loss: 16.772245038713137\n",
      "Epoch #18300, Train Loss: 17.408735236226974\n",
      "Epoch #18350, Train Loss: 15.258330699204043\n",
      "Epoch #18400, Train Loss: 15.439504604584222\n",
      "Epoch #18450, Train Loss: 17.101734786874914\n",
      "Epoch #18500, Train Loss: 15.68457396470162\n",
      "Epoch #18550, Train Loss: 15.2773729998393\n",
      "Epoch #18600, Train Loss: 15.078266947285808\n",
      "Epoch #18650, Train Loss: 16.312521543037914\n",
      "Epoch #18700, Train Loss: 16.439156811040977\n",
      "Epoch #18750, Train Loss: 14.053824108093902\n",
      "Epoch #18800, Train Loss: 14.058546313519395\n",
      "Epoch #18850, Train Loss: 15.237157818003814\n",
      "Epoch #18900, Train Loss: 14.374268014204379\n",
      "Epoch #18950, Train Loss: 16.06322809049459\n",
      "Epoch #19000, Train Loss: 14.362928796880647\n",
      "Epoch #19050, Train Loss: 15.671924695668473\n",
      "Epoch #19100, Train Loss: 15.007930369754318\n",
      "Epoch #19150, Train Loss: 15.967848603645166\n",
      "Epoch #19200, Train Loss: 15.369575613987433\n",
      "Epoch #19250, Train Loss: 15.113511664588968\n",
      "Epoch #19300, Train Loss: 14.212822869718712\n",
      "Epoch #19350, Train Loss: 15.964849218812653\n",
      "Epoch #19400, Train Loss: 16.199939956405608\n",
      "Epoch #19450, Train Loss: 16.168612567206345\n",
      "Epoch #19500, Train Loss: 15.476685959232215\n",
      "Epoch #19550, Train Loss: 14.631087060052819\n",
      "Epoch #19600, Train Loss: 15.035850819735062\n",
      "Epoch #19650, Train Loss: 16.448179038678983\n",
      "Epoch #19700, Train Loss: 15.039068273077719\n",
      "Epoch #19750, Train Loss: 16.063280398692456\n",
      "Epoch #19800, Train Loss: 16.280733625448207\n",
      "Epoch #19850, Train Loss: 14.80311457320829\n",
      "Epoch #19900, Train Loss: 15.3987512173736\n",
      "Epoch #19950, Train Loss: 15.639498318306416\n"
     ]
    }
   ],
   "source": [
    "W, C, = train(W, C, L, num_epochs=20000, batch_size=100, alpha=0.5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('word2vec_params.npz', arr1=W, arr2=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays = np.load('word2vec_params.npz')\n",
    "#W, C = arrays['arr1'], arrays['arr2'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total embedding\n",
    "E = np.array(C)\n",
    "\n",
    "# normalize lengths of vectors\n",
    "\n",
    "E = E / np.linalg.norm(E, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word, n=20):\n",
    "    print(f\"Finding most similar words for idx: {word2idx[word]}\")\n",
    "    # get the embedding of this word (sum of context and center embeddings)\n",
    "    w_emb = E[word2idx[word]]\n",
    "    # compute dot product with all other words\n",
    "    similarity_scores = np.dot(E,w_emb)\n",
    "    # find the indices sorted from largest to smallest score\n",
    "    idx = np.argsort(similarity_scores)[::-1]\n",
    "    print(f\"best idx: {idx[:n]}\")\n",
    "\n",
    "    # get the n highest scoring words\n",
    "    best = []\n",
    "    for i in range(n):\n",
    "        best.append(vocab[idx[i]])\n",
    "    return best, similarity_scores[idx[:n]]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding most similar words for idx: 1243\n",
      "best idx: [ 1243 18186 12156  2939 11258  3169  3565  6976 10361  9626  2414 15552\n",
      "  8749 16437 18801 12605  8677  6551 13061 18090]\n",
      "Words most similar to 'awesome': ['awesome', 'unsentimental', 'passionate', 'clashing', 'neo-nazism', 'color', 'controlled', 'generous', 'masterful', 'lean', 'campanella', 'sluggish', 'instincts', 'sturdy', 'well-edited', 'plotted', 'innocence', 'forcefully', 'profile', 'uniquely']\n",
      "scores: [1.         0.99989264 0.99987931 0.99986776 0.99986762 0.99986141\n",
      " 0.99985859 0.99985747 0.99985041 0.99984668 0.99984502 0.99983811\n",
      " 0.99983667 0.99983664 0.99983146 0.99983003 0.99982877 0.99982836\n",
      " 0.99982625 0.99982193]\n"
     ]
    }
   ],
   "source": [
    "w = \"awesome\"\n",
    "similar_words, scores = find_most_similar(w)\n",
    "print(f\"Words most similar to '{w}': {similar_words}\")\n",
    "print(f\"scores: {scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorchified implementation of skipgram word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch dataset for neatly packaging the Stanford Treebank Sentiment sentences\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, window_size=6, num_negatives=20, subsample=True, subsample_t=1e-4):\n",
    "        self.L = window_size   # context window size\n",
    "        self.k = num_negatives # number of negative samples per positive\n",
    "        self.subsample_t = subsample_t\n",
    "\n",
    "        # get sentences from file\n",
    "        sentences, self.unigram_count, self.word_count = self.get_sentences()\n",
    "        # subsample the sentences\n",
    "        if subsample:\n",
    "            sentences_subsampled = self.subsample_sentences(sentences)\n",
    "        else:\n",
    "            sentences_subsampled = sentences\n",
    "        # create vocabulary\n",
    "        self.vocab = sorted(list(set([word for sentence in sentences_subsampled for word in sentence])))\n",
    "        self.word2idx = {w:i for i,w in enumerate(self.vocab)}\n",
    "        print(f\"Vocab size: {len(self.vocab)}, word_count: {self.word_count}\")\n",
    "        print(f\"Num sentences: {len(sentences_subsampled)}\")\n",
    "        print(f\"Total number of tokens after subsampling: {sum(len(s) for s in sentences_subsampled)}\")\n",
    "        # tokenize the sentences\n",
    "        self.sentences_tokenized = []\n",
    "        for s in sentences_subsampled:\n",
    "            self.sentences_tokenized.append([self.word2idx[w] for w in s])\n",
    "        # generate all positive pairs: (w_pos, c)\n",
    "        self.pos_pairs = self.generate_positive_pairs()\n",
    "        # compute weighted unigram probability distribution\n",
    "        self.P_alpha = self.compute_weighted_unigram_dist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get positive pair\n",
    "        pos_pair = self.pos_pairs[idx]\n",
    "        # generate negative words\n",
    "        wnegs = self.get_negative_samples(pos_pair[0])\n",
    "        return {\"center\": pos_pair[1], \"positive\": pos_pair[0], \"negatives\": wnegs}\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # generate new pos pairs\n",
    "        self.pos_pairs = self.generate_positive_pairs()\n",
    "        random.shuffle(self.pos_pairs)\n",
    "\n",
    "    def get_negative_samples(self, wpos_idx):\n",
    "        # generate negative samples (generate extra's to account for removals of matches with wpos_idx)\n",
    "        wnegs = torch.multinomial(self.P_alpha, self.k+50)\n",
    "        # remove matches with the positive word\n",
    "        wnegs = wnegs[wnegs != wpos_idx]    \n",
    "        \n",
    "        # if not enough negative samples, generate more\n",
    "        while  len(wnegs) < self.k:\n",
    "            # generate more extras\n",
    "            negs_extra = torch.multinomial(self.P_alpha, self.k+50)\n",
    "            # remove matches with positive word\n",
    "            negs_extra = negs_extra[negs_extra != wpos_idx] \n",
    "            wnegs = torch.cat([wnegs,negs_extra])\n",
    "    \n",
    "        return wnegs[:self.k].tolist()    \n",
    "\n",
    "    def compute_weighted_unigram_dist(self, alpha=0.75):\n",
    "        P_alpha = torch.zeros(size=(len(self.vocab),))\n",
    "        for i,w in enumerate(self.vocab):\n",
    "            P_alpha[i] = self.unigram_count[w]**alpha\n",
    "        P_alpha = P_alpha / P_alpha.sum()    \n",
    "        return P_alpha\n",
    "\n",
    "    def generate_positive_pairs(self):\n",
    "        pos_pairs = []\n",
    "        for s in self.sentences_tokenized:\n",
    "            for i, w in enumerate(s):\n",
    "                # randomly pick a context window size between 1..L\n",
    "                R = torch.randint(0,self.L+1, size=(1,)).item()\n",
    "                c = w # center_word \n",
    "                context_words = s[max(0,i-R):i] + s[i+1:i+1+R]\n",
    "                for w_pos in context_words:\n",
    "                    pos_pairs.append((w_pos,c)) \n",
    "        return pos_pairs\n",
    "    \n",
    "    def check_punc(self, w):\n",
    "        return any(c.isalpha() for c in w)\n",
    "\n",
    "    # remove punctuations from list of words and apply lowercase folding \n",
    "    def preprocess(self, s):\n",
    "        words = s.lower().strip().split()[1:]\n",
    "        words = [w for w in words if self.check_punc(w)]\n",
    "        return words\n",
    "\n",
    "    def get_sentences(self):\n",
    "        # load sentences from file\n",
    "        word_count = 0\n",
    "        unigram_count = defaultdict(int)\n",
    "        with open('datasetSentences.txt','r') as file:\n",
    "            lines = file.readlines()\n",
    "            # preprocessing\n",
    "            sentences = []\n",
    "            for line in lines[1:]:\n",
    "                words = self.preprocess(line)\n",
    "                s = []\n",
    "                for word in words:\n",
    "                    if \"\\/\" in word:\n",
    "                        ws = word.replace(\"\\/\", \" \").split()\n",
    "                        for w in ws:\n",
    "                            s.append(w)\n",
    "                            unigram_count[w] += 1\n",
    "                            word_count += 1\n",
    "                    else:\n",
    "                        s.append(word)    \n",
    "                        unigram_count[word] += 1\n",
    "                        word_count += 1\n",
    "                sentences.append(s)   \n",
    "\n",
    "        return sentences, unigram_count, word_count\n",
    "\n",
    "    def subsample_sentences(self, sentences, num_copies=10):\n",
    "        num_copies = 10\n",
    "        discard_probs = {w:self.subsample_prob(w) for w in self.unigram_count.keys()}\n",
    "        sentences_subsampled = [[word for word in s if np.random.random() >= discard_probs[word]] for s in sentences*num_copies]\n",
    "        # remove zero length subsampled sentences\n",
    "        sentences_subsampled = [s for s in sentences_subsampled if len(s) > 0]\n",
    "        return sentences_subsampled\n",
    "\n",
    "    def subsample_prob(self, word):\n",
    "        p = max(0, 1 - np.sqrt(self.subsample_t*self.word_count/self.unigram_count[word]))\n",
    "        return p\n",
    "\n",
    "\n",
    "\n",
    "# create pytorch dataset for neatly packaging the Brown Corpus\n",
    "class BrownCorpus(Dataset):\n",
    "    def __init__(self, window_size=6, num_negatives=20, remove_stopwords=False, subsample=False, subsample_t=1e-4):\n",
    "        self.L = window_size   # context window size\n",
    "        self.k = num_negatives # number of negative samples per positive\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.subsample_t = subsample_t\n",
    "        if remove_stopwords:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # get text\n",
    "        text, self.unigram_count, self.word_count = self.get_text()\n",
    "\n",
    "        # subsample the text\n",
    "        if subsample:\n",
    "            text_subsampled = self.subsample_text(text)\n",
    "        else:\n",
    "            text_subsampled = text\n",
    "        # create vocabulary\n",
    "        self.vocab = sorted(list(set(text_subsampled)))\n",
    "        self.word2idx = {w:i for i,w in enumerate(self.vocab)}\n",
    "        print(f\"Vocab size: {len(self.vocab)}, word_count: {self.word_count}\")\n",
    "        print(f\"Total number of tokens after subsampling: {len(text_subsampled)}\")\n",
    "        # tokenize the text\n",
    "        self.text_tokenized = [self.word2idx[w] for w in text_subsampled]\n",
    "        # generate all positive pairs: (w_pos, c)\n",
    "        self.pos_pairs = self.generate_positive_pairs()\n",
    "        # compute weighted unigram probability distribution\n",
    "        self.P_alpha = self.compute_weighted_unigram_dist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pos_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get positive pair\n",
    "        pos_pair = self.pos_pairs[idx]\n",
    "        # generate negative words\n",
    "        wnegs = self.get_negative_samples(pos_pair[0])\n",
    "        return {\"center\": pos_pair[1], \"positive\": pos_pair[0], \"negatives\": wnegs}\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # generate new pos pairs\n",
    "        self.pos_pairs = self.generate_positive_pairs()\n",
    "        random.shuffle(self.pos_pairs)\n",
    "\n",
    "    def get_negative_samples(self, wpos_idx):\n",
    "        # generate negative samples (generate extra's to account for removals of matches with wpos_idx)\n",
    "        wnegs = torch.multinomial(self.P_alpha, self.k+50)\n",
    "        # remove matches with the positive word\n",
    "        wnegs = wnegs[wnegs != wpos_idx]    \n",
    "        \n",
    "        # if not enough negative samples, generate more\n",
    "        while  len(wnegs) < self.k:\n",
    "            # generate more extras\n",
    "            negs_extra = torch.multinomial(self.P_alpha, self.k+50)\n",
    "            # remove matches with positive word\n",
    "            negs_extra = negs_extra[negs_extra != wpos_idx] \n",
    "            wnegs = torch.cat([wnegs,negs_extra])\n",
    "    \n",
    "        return wnegs[:self.k].tolist()    \n",
    "\n",
    "    def compute_weighted_unigram_dist(self, alpha=0.75):\n",
    "        P_alpha = torch.zeros(size=(len(self.vocab),))\n",
    "        for i,w in enumerate(self.vocab):\n",
    "            P_alpha[i] = self.unigram_count[w]**alpha\n",
    "        P_alpha = P_alpha / P_alpha.sum()    \n",
    "        return P_alpha\n",
    "\n",
    "    def generate_positive_pairs(self):\n",
    "        pos_pairs = []\n",
    "        for i, w in enumerate(self.text_tokenized):\n",
    "            # randomly pick a context window size between 1..L\n",
    "            R = torch.randint(0,self.L+1, size=(1,)).item()\n",
    "            c = w # center_word \n",
    "            context_words = self.text_tokenized[max(0,i-R):i] + self.text_tokenized[i+1:i+1+R]\n",
    "            for w_pos in context_words:\n",
    "                pos_pairs.append((w_pos,c)) \n",
    "        return pos_pairs\n",
    "    \n",
    "    def check_punc(self, w):\n",
    "        return any(c.isalpha() for c in w)\n",
    "\n",
    "    # remove punctuations from list of words and apply lowercase folding \n",
    "    def preprocess(self, s):\n",
    "        words = [w.lower() for w in s if self.check_punc(w)]\n",
    "        if self.remove_stopwords:\n",
    "            words = [w for w in words if not w in self.stop_words]\n",
    "        return words\n",
    "\n",
    "    def get_text(self):\n",
    "        # preprocessing\n",
    "        text = self.preprocess(brown.words())\n",
    "        unigram_count = defaultdict(int)\n",
    "        for word in text:\n",
    "            unigram_count[word] += 1\n",
    "        word_count = len(text)\n",
    "\n",
    "        return text, unigram_count, word_count\n",
    "\n",
    "    def subsample_text(self, text, num_copies=10):\n",
    "        num_copies = 10\n",
    "        discard_probs = {w:self.subsample_prob(w) for w in self.unigram_count.keys()}\n",
    "        text_subsampled = [word for word in text*num_copies if np.random.random() >= discard_probs[word]]\n",
    "        return text_subsampled\n",
    "\n",
    "    def subsample_prob(self, word):\n",
    "        p = max(0, 1 - np.sqrt(self.subsample_t*self.word_count/self.unigram_count[word]))\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center: election\n",
      "center: friday\n",
      "negatives: ['administered', 'leningrad-kirov', 'first', 'af', 'historical', 'attention', 'water', 'preparations', 'scandal', 'traffic', 'remember', 'mattered', 'undermine', 'supposed', 'faithful', 'age-old', 'constrictors', 'extra', 'method', 'starting']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "brown_data = BrownCorpus(remove_stopwords=True)\n",
    "d = brown_data[55]\n",
    "c = d[\"center\"]\n",
    "wpos = d[\"positive\"]\n",
    "wnegs = d[\"negatives\"]\n",
    "print(f\"center: {brown_data.vocab[c]}\")\n",
    "print(f\"center: {brown_data.vocab[wpos]}\")\n",
    "print(f\"negatives: {[brown_data.vocab[w] for w in wnegs]}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader for our custom dataset\n",
    "def collate_fn(batch):\n",
    "    collated_batch = {}\n",
    "    for key in batch[0]:\n",
    "        collated_batch[key] = torch.stack([torch.tensor(item[key]) for item in batch]).unsqueeze(-1)\n",
    "    return collated_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom pytorch model for skipgram word2vec\n",
    "class Word2Vec(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims):\n",
    "        super().__init__()\n",
    "        self.emb_C = torch.nn.Embedding(vocab_size, embedding_dims)        \n",
    "        self.emb_W = torch.nn.Embedding(vocab_size, embedding_dims)        \n",
    "        # intialize small random weights\n",
    "        c = 0.01 / embedding_dims\n",
    "        self.emb_C.weight.data.uniform_(-c, c)\n",
    "        self.emb_W.weight.data.uniform_(-c, c)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, center, positive, negatives):\n",
    "        # get the embedding vectors\n",
    "        center_embeds = self.emb_C(center.squeeze()) # shape: (B,D)\n",
    "        pos_embeds = self.emb_W(positive.squeeze()) # shape: (B,D)\n",
    "        negs_embeds = self.emb_W(negatives.squeeze()) # shape: (B,k,D)\n",
    "        B, k, D = negs_embeds.shape\n",
    "        # reshape the tensors so that we can perform batch matrix multiply\n",
    "        center_embeds = center_embeds.view(B,D,1)\n",
    "        pos_embeds = pos_embeds.view(B,1,D)\n",
    "        # compute logits\n",
    "        pos_logits = torch.bmm(pos_embeds, center_embeds).squeeze()  # shape: (B,)\n",
    "        negs_logits = torch.bmm(negs_embeds, center_embeds).squeeze() # shape: (B,k)\n",
    "        # set up labels\n",
    "        pos_labels = torch.ones_like(pos_logits)\n",
    "        neg_labels = torch.zeros_like(negs_logits)\n",
    "        # compute binary cross entropy loss\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(pos_logits, pos_labels, reduction='none') # shape: (B,)\n",
    "        negs_loss = F.binary_cross_entropy_with_logits(negs_logits, neg_labels, reduction='none') # shape: (B,k)\n",
    "        loss = torch.mean(pos_loss + negs_loss.sum(dim=1))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "# training loop\n",
    "def train(model, optimizer, train_data, train_dataloader, device=\"cpu\", num_epochs=10):\n",
    "    avg_loss = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            center, positive, negatives = batch[\"center\"], batch[\"positive\"], batch[\"negatives\"]\n",
    "            # move batch to device\n",
    "            center, positive, negatives = center.to(device), positive.to(device), negatives.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            loss = model(center, positive, negatives)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, Batch Loss: {loss.item():.3f}, Moving Average Loss: {avg_loss:.3f}\")  \n",
    "\n",
    "        train_data.on_epoch_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 47892, word_count: 530090\n",
      "Total number of tokens after subsampling: 530090\n"
     ]
    }
   ],
   "source": [
    "#train_data = SSTDataset(subsample=False)\n",
    "train_data = BrownCorpus(remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64\n",
    "B = 256\n",
    "vocab_size = len(train_data.vocab)\n",
    "learning_rate = 1e-2\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = Word2Vec(vocab_size=vocab_size, embedding_dims=D).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=False, collate_fn=collate_fn, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch Loss: 4.938, Moving Average Loss: 5.114: 100%|██████████| 3995/3995 [00:35<00:00, 112.30it/s]\n",
      "Epoch 2, Batch Loss: 3.901, Moving Average Loss: 3.828: 100%|██████████| 3990/3990 [00:44<00:00, 90.52it/s]\n",
      "Epoch 3, Batch Loss: 3.746, Moving Average Loss: 3.729: 100%|██████████| 4010/4010 [00:41<00:00, 95.88it/s] \n",
      "Epoch 4, Batch Loss: 3.471, Moving Average Loss: 3.628: 100%|██████████| 4008/4008 [00:43<00:00, 92.35it/s] \n",
      "Epoch 5, Batch Loss: 3.470, Moving Average Loss: 3.627: 100%|██████████| 3999/3999 [00:39<00:00, 101.30it/s]\n",
      "Epoch 6, Batch Loss: 3.893, Moving Average Loss: 3.624: 100%|██████████| 4001/4001 [00:42<00:00, 94.68it/s] \n",
      "Epoch 7, Batch Loss: 3.748, Moving Average Loss: 3.609: 100%|██████████| 4003/4003 [00:38<00:00, 103.13it/s]\n",
      "Epoch 8, Batch Loss: 3.409, Moving Average Loss: 3.550: 100%|██████████| 4005/4005 [00:44<00:00, 89.67it/s] \n",
      "Epoch 9, Batch Loss: 3.583, Moving Average Loss: 3.573: 100%|██████████| 4005/4005 [00:38<00:00, 103.17it/s]\n",
      "Epoch 10, Batch Loss: 3.781, Moving Average Loss: 3.548: 100%|██████████| 3998/3998 [00:45<00:00, 87.82it/s]\n",
      "Epoch 11, Batch Loss: 3.775, Moving Average Loss: 3.590: 100%|██████████| 3996/3996 [00:40<00:00, 98.47it/s] \n",
      "Epoch 12, Batch Loss: 3.644, Moving Average Loss: 3.602: 100%|██████████| 3996/3996 [00:43<00:00, 92.03it/s] \n",
      "Epoch 13, Batch Loss: 3.593, Moving Average Loss: 3.574: 100%|██████████| 4001/4001 [00:40<00:00, 97.90it/s] \n",
      "Epoch 14, Batch Loss: 3.697, Moving Average Loss: 3.595: 100%|██████████| 3996/3996 [00:41<00:00, 95.36it/s] \n",
      "Epoch 15, Batch Loss: 3.410, Moving Average Loss: 3.582: 100%|██████████| 4004/4004 [00:43<00:00, 92.37it/s] \n",
      "Epoch 16, Batch Loss: 3.391, Moving Average Loss: 3.542: 100%|██████████| 3994/3994 [00:36<00:00, 108.18it/s]\n",
      "Epoch 17, Batch Loss: 3.744, Moving Average Loss: 3.540: 100%|██████████| 3996/3996 [00:41<00:00, 95.72it/s] \n",
      "Epoch 18, Batch Loss: 3.765, Moving Average Loss: 3.586: 100%|██████████| 3994/3994 [00:39<00:00, 100.23it/s]\n",
      "Epoch 19, Batch Loss: 3.614, Moving Average Loss: 3.578: 100%|██████████| 3995/3995 [00:42<00:00, 94.49it/s] \n",
      "Epoch 20, Batch Loss: 3.455, Moving Average Loss: 3.574: 100%|██████████| 3994/3994 [00:40<00:00, 98.53it/s] \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_data, train_dataloader, device=DEVICE, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copies of embedding matrices\n",
    "C = model.emb_C.weight.clone().detach()\n",
    "W = model.emb_W.weight.clone().detach()\n",
    "\n",
    "# average the context and center embeddings\n",
    "#E = C\n",
    "#E = W\n",
    "E = 0.5 * (C + W)\n",
    "norms = torch.norm(E, dim=1, keepdim=True)\n",
    "E = E / norms\n",
    "\n",
    "def find_most_similar(word, n=10):\n",
    "    print(f\"Finding most similar words for idx: {train_data.word2idx[word]}\")\n",
    "    # get the embedding of this word (sum of context and center embeddings)\n",
    "    w_emb = E[train_data.word2idx[word]].view(-1,1)\n",
    "    # compute dot product similarity with all other words\n",
    "    scores = torch.mm(E,w_emb).view(-1)\n",
    "    scores[norms.view(-1)==0] = 0\n",
    "    # find the indices sorted from largest to smallest score\n",
    "    _, idx = torch.sort(scores, descending=True) \n",
    "\n",
    "    # get the n highest scoring words\n",
    "    best = []\n",
    "    for i in range(n):\n",
    "        best.append(train_data.vocab[idx[i]])\n",
    "    return best, scores[idx[:n]].tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding most similar words for idx: 14700\n",
      "Target word: scream\n",
      "Most similar words:  ['scream', 'budget', 'improvise', 'geriatric', 'charmer', 'two-actor', 'consciousness', 'signpost', 'groggy', 'seeping']\n",
      "Similarity scores:  [1.0000001192092896, 0.5037898421287537, 0.47533127665519714, 0.4743828773498535, 0.4240070879459381, 0.368988960981369, 0.3607769310474396, 0.36068689823150635, 0.35166338086128235, 0.34774455428123474]\n"
     ]
    }
   ],
   "source": [
    "word = \"scream\"\n",
    "best_words, scores = find_most_similar(word)\n",
    "print(f\"Target word: {word}\")\n",
    "print(\"Most similar words: \", best_words)\n",
    "print(\"Similarity scores: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
