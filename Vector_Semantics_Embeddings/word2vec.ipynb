{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram Word2Vec (with Negative Sampling)\n",
    "\n",
    "The skip-gram word2vec algorithm is a simple self-supervised model for learning dense word embedding vectors from a corpus of text. It is trained on the task of predicting the `probability distribution` over `context words` given a `center word`, for all possible center words from the vocabulary. The parameters of this model consist of two separate $|V| \\times D$ embedding matrices ($|V|$ is the vocab size and $D$ is the embedding dimensions): matrix $W$ whose rows are the embeddings of outside words and matrix $C$ whose rows are the embeddings of center words. (We could instead have one single embedding for both outside and context words, however this approach of having separate embeddings is more convenient). Then the model for computing the probability distribution is simply defined as follows:\n",
    "\n",
    "$P(w|c) = \\frac{exp(\\vec{w} \\cdot \\vec{c})}{\\sum_{w' \\in V} exp(\\vec{w}' \\cdot \\vec{c})}$\n",
    "\n",
    "where $\\vec{w}$ and $\\vec{c}$ are the embedding vectors of the context and center words resepectively. Note that this is just a softmax over all the dot products of every possible context word $w$ given a particluar center word $c$.\n",
    "\n",
    "Now due to large volcabulary size $|V|$, computing this probability distribution is very inefficient (because of the sum in the denominator). Instead of computing a probability distribution over all possible context words, we instead simplify our task into a `binary classification` problem. Given a pair of context word $w$ and center word $c$, our simplified task is to train a binary classifier to predict whether $w$ actually occurs in the context of $c$ or not (we use label $1$ for True and $0$ for False). We define this simple binary classification problem as follows:\n",
    "\n",
    "$P(y=1|w_{pos},c) = \\sigma (\\vec{w}_{pos} \\cdot \\vec{c})$\n",
    "\n",
    "$P(y=0|w_{neg},c) = 1-\\sigma (\\vec{w}_{neg} \\cdot \\vec{c}) = \\sigma (-\\vec{w}_{neg} \\cdot \\vec{c})$\n",
    "\n",
    "where $\\sigma()$ is the saigmoid function, $w_{pos}$ denotes a true context word and $w_{neg}$ denotes a `noise` word which is not a true context word. For training this classifier, we will use $k$ times as many noise words than context words (this reflects the fact that each center word will have far fewer words from the vocab that appear in it's context than words that don't). During training, we will simply slide a context window of half-size $L$ over the training corpus, so at each position this gives us $2L$ different positive pairs $\\{(w^{i}_{pos},c) | i =1,2..,2L\\}$. For each of these positive pairs, we generate k negative samples by sampling from the unigram probability distribution over the vocabulary (making sure that none of these noise words match the positive word). Then we compute the negative log-likelihood loss for the each positive pair along with the k negative pairs:\n",
    "\n",
    "$L = -\\log(P(y=1|w_{pos},c)) - \\sum_{j=1}^k \\log(P(y=0|w^{j}_{neg},c)) = -\\log(\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})) - \\sum_{j=1}^k \\log(\\sigma (-\\vec{w}^{j}_{neg} \\cdot \\vec{c}))$\n",
    "\n",
    "We can think of each window position providing us with a batch of $L$ positive instances and $kL$ negative instances. Then we can minimize this loss via gradient descent. The gradients with are:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec c} = (\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})-1) \\vec{w}_{pos} + \\sum_{j=1}^k \\sigma (\\vec{w}^{j}_{neg} \\cdot \\vec{c}) \\vec{w}^{j}_{neg}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec{w}_{pos}} = (\\sigma (\\vec{w}_{pos} \\cdot \\vec{c})-1) \\vec{c}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\vec{w}^{j}_{neg}} = \\sigma (\\vec{w}^{j}_{neg} \\cdot \\vec{c}) \\vec{c}$\n",
    "\n",
    "\n",
    "Note: When sampling the noise words, instead of using the unigram probability distribution $P(w)$ over the vocabulary words, it's better to use a weighted version of this distirbution $P_{\\alpha}(w)$:\n",
    "\n",
    "$P(w) = \\frac{count(w)}{ \\sum_{w'\\in V} count(w')}$\n",
    "\n",
    "which is the unigram distribution and the weighted unigram distribution is defined as:\n",
    "\n",
    "$P_{\\alpha}(w) = \\frac{count(w)^{\\alpha}}{ \\sum_{w'\\in V} count(w')^{\\alpha}}$\n",
    "\n",
    "where $\\alpha$ is an exponent between $0$ and $1$. This kind of weighting helps to slightly increase the probabilities of the rarer words and slightly suppressess the probability of the most common words. Empirically $\\alpha = 0.75$ tends to work well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement and train a skipgram word2vec model using the Stanford Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_punc(w):\n",
    "    return any(c.isalpha() for c in w)\n",
    "\n",
    "# remove punctuations from list of words and apply lowercase folding \n",
    "def preprocess(s):\n",
    "    words = s.lower().strip().split()[1:]\n",
    "    words = [w for w in words if check_punc(w)]\n",
    "    return words\n",
    "\n",
    "# load dataset\n",
    "word_count = 0\n",
    "unigram_count = defaultdict(int)\n",
    "wierd_words = []\n",
    "with open('datasetSentences.txt','r') as file:\n",
    "    lines = file.readlines()\n",
    "    # lowercase folding and tokenize\n",
    "    sentences = []\n",
    "    for line in lines[1:]:\n",
    "        words = preprocess(line)\n",
    "        s = []\n",
    "        for word in words:\n",
    "            if \"\\/\" in word:\n",
    "                ws = word.replace(\"\\/\", \" \").split()\n",
    "                for w in ws:\n",
    "                    s.append(w)\n",
    "                    unigram_count[w] += 1\n",
    "                    word_count += 1\n",
    "            else:\n",
    "                s.append(word)    \n",
    "                unigram_count[word] += 1\n",
    "                word_count += 1\n",
    "        sentences.append(s)        \n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The skipgram paper uses a subsampling strategy to get rid of the most frequent words, like stop words. Each word $w_i$ in the training corpus is discarded with a probability given by the following:\n",
    "\n",
    "$P(w_i) = 1 - \\sqrt{\\frac{T}{count(w_i)}}$ \n",
    "\n",
    "where $T$ is a threshold value which is a small fraction of the corpus total token count (~$10^{-5} \\times N$) and $count(w_i)$ is the frequency of that word in the corpus. For more frequent words, the square root term is very close to zero and so the word will get discarder with high probability.\n",
    "\n",
    "We also keep multiple copies of the same sentence to reduce the chances of entirely losing important words due to the subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_prob(word, t=1e-4):\n",
    "    p = max(0, 1 - np.sqrt(t*word_count/unigram_count[word]))\n",
    "    return p\n",
    "\n",
    "num_copies = 10\n",
    "discard_probs = {w:subsample_prob(w) for w in unigram_count.keys()}\n",
    "sentences_subsampled = [[word for word in s if np.random.random() >= discard_probs[word]] for s in sentences*num_copies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before subsampling:  ['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', 'conan', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal']\n",
      "After subsampling:  ['destined', '21st', 'century', 'new', 'conan', 'splash', 'greater', 'arnold', 'schwarzenegger', 'jean-claud', 'van', 'damme', 'segal']\n",
      "Before subsampling:  ['the', 'gorgeously', 'elaborate', 'continuation', 'of', 'the', 'lord', 'of', 'the', 'rings', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer', 'director', 'peter', 'jackson', \"'s\", 'expanded', 'vision', 'of', 'j.r.r.', 'tolkien', \"'s\", 'middle-earth']\n",
      "After subsampling:  ['gorgeously', 'elaborate', 'continuation', 'lord', 'rings', 'trilogy', 'is', 'huge', 'column', 'adequately', 'describe', 'co-writer', 'director', 'peter', 'expanded', 'vision', 'j.r.r.', 'tolkien', 'middle-earth']\n",
      "Before subsampling:  ['effective', 'but', 'too-tepid', 'biopic']\n",
      "After subsampling:  ['effective', 'too-tepid', 'biopic']\n",
      "Before subsampling:  ['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', 'wasabi', 'is', 'a', 'good', 'place', 'to', 'start']\n",
      "After subsampling:  ['fun', 'wasabi', 'a']\n",
      "Before subsampling:  ['emerges', 'as', 'something', 'rare', 'an', 'issue', 'movie', 'that', \"'s\", 'so', 'honest', 'and', 'keenly', 'observed', 'that', 'it', 'does', \"n't\", 'feel', 'like', 'one']\n",
      "After subsampling:  ['emerges', 'an', 'issue', 'keenly', 'observed', 'does', \"n't\", 'feel', 'like', 'one']\n",
      "Before subsampling:  ['the', 'film', 'provides', 'some', 'great', 'insight', 'into', 'the', 'neurotic', 'mindset', 'of', 'all', 'comics', 'even', 'those', 'who', 'have', 'reached', 'the', 'absolute', 'top', 'of', 'the', 'game']\n",
      "After subsampling:  ['provides', 'some', 'into', 'neurotic', 'mindset', 'all', 'comics', 'who', 'reached', 'absolute', 'top', 'game']\n",
      "Before subsampling:  ['offers', 'that', 'rare', 'combination', 'of', 'entertainment', 'and', 'education']\n",
      "After subsampling:  ['combination', 'entertainment', 'education']\n"
     ]
    }
   ],
   "source": [
    "# compare before and after subsampling\n",
    "for i in range(7):\n",
    "    print(\"Before subsampling: \", sentences[i])\n",
    "    print(\"After subsampling: \", sentences_subsampled[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that most of the stop words are gone after subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 19333\n",
      "Num sentences: 11855\n",
      "Total number of tokens: 201802\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "unk_token = \"<UNK>\"\n",
    "vocab = [unk_token] + sorted(list(set([word for sentence in sentences for word in sentence])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Num sentences: {len(sentences)}\")\n",
    "print(f\"Total number of tokens: {word_count}\")\n",
    "\n",
    "# tokenize the sentences\n",
    "sentences_tokenized = []\n",
    "for s in sentences:\n",
    "    sentences_tokenized.append([word2idx[w] for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram weighted probability distribution\n",
    "alpha = 0.75\n",
    "P_alpha = np.zeros(shape=(vocab_size))\n",
    "for i,w in enumerate(vocab):\n",
    "    P_alpha[i] = unigram_count[w]**alpha\n",
    "P_alpha = P_alpha / P_alpha.sum()    \n",
    "unigram_idx = np.arange(0,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters of word2vec model\n",
    "D = 32 # embedding dim\n",
    "L = 8  # context window half-size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of sliding context window over every position from start to end of corpus, we will instead randomly select a batch of context windows on every epoch. We will also add some randomness to the context window size, by sampling a random size between [1,L]. This ensures that we get smaller context windows more often than longer windows which is helpful because context words that are closer should be related more strongly on the center word than context words that are farther away. Closer context words should therefore be sampled more often.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_context(L):\n",
    "    # first randomly select a sentence\n",
    "    sent_idx = np.random.randint(0,len(sentences_tokenized)-1)\n",
    "    sent = sentences_tokenized[sent_idx]\n",
    "    # pick random context window half-length between [1..L]\n",
    "    R = np.random.randint(1, L)\n",
    "    # pick a random center word from the sentence\n",
    "    if len(sent) > R:\n",
    "        c_idx = np.random.randint(0,max(0,len(sent)-R))\n",
    "    else:\n",
    "        c_idx = 0\n",
    "\n",
    "    center_word = sent[c_idx]\n",
    "    context_words = sent[max(0,c_idx-R):c_idx] + sent[c_idx+1:c_idx+1+R]\n",
    "\n",
    "    if len(context_words) == 0:\n",
    "        return get_random_context(L)\n",
    "    else:\n",
    "        return center_word, context_words    \n",
    "\n",
    "\n",
    "def get_negative_samples(wpos_idx, k=10):\n",
    "    nsamples = 0\n",
    "    wnegs = []\n",
    "    # generate negative samples\n",
    "    while nsamples < k:\n",
    "        wneg_idx = np.random.choice(unigram_idx, size=1, p=P_alpha)[0]\n",
    "        # make sure noise words don't match the positive word\n",
    "        if wneg_idx != wpos_idx:\n",
    "            wnegs.append(wneg_idx)\n",
    "            nsamples += 1\n",
    "    return wnegs    \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# skip-gram with negative samplice loss for a single (w,c) pair\n",
    "def compute_loss_and_grads(wpos_idx, c_idx, W, C):\n",
    "    # get negative samples\n",
    "    wnegs_idx = get_negative_samples(wpos_idx)\n",
    "    # get embedding vectors\n",
    "    V, D = W.shape\n",
    "    c = C[c_idx]   # shape: (D,)\n",
    "    w_pos = W[wpos_idx]  # shape: (D,)\n",
    "    w_negs = W[wnegs_idx]  # shape: (k,D)\n",
    "    \n",
    "    s_wpos_dot_c = sigmoid(np.dot(w_pos,c))  # shape: (1,)\n",
    "    s_wneg_dot_c = sigmoid(np.dot(w_negs,c)).reshape(w_negs.shape[0],1)  # shape: (k,1)\n",
    "\n",
    "    # compute loss\n",
    "    loss =  -np.log(s_wpos_dot_c) - np.log(1-s_wneg_dot_c).sum()\n",
    "\n",
    "    # compute gradients\n",
    "    grad_c = (s_wpos_dot_c-1) * w_pos +  (s_wneg_dot_c * w_negs).sum(axis=0)  # shape: (D,)\n",
    "    grad_wpos = (s_wpos_dot_c-1) * c  # shape: (D,)\n",
    "    grad_wnegs = s_wneg_dot_c * c  # shape: (k,D)\n",
    "    \n",
    "    return loss, grad_c, grad_wpos, grad_wnegs, wnegs_idx\n",
    "\n",
    "\n",
    "# compute total loss and accumulated gradients for a single context window\n",
    "def skipgram(center_word_idx, context_words_idx, W, C):\n",
    "    grad_C = np.zeros_like(C) \n",
    "    grad_W = np.zeros_like(W) \n",
    "    total_loss = 0.0\n",
    "\n",
    "    # compute loss and accumulate gradients for each positive context word and negative samples\n",
    "    for wpos_idx in context_words_idx:\n",
    "        loss, grad_c, grad_wpos, grad_wnegs, wnegs_idx = compute_loss_and_grads(wpos_idx, center_word_idx, W, C) \n",
    "        total_loss += loss\n",
    "        grad_C[center_word_idx] += grad_c\n",
    "        grad_W[wnegs_idx] += grad_wnegs\n",
    "        grad_W[wpos_idx] += grad_wpos\n",
    "\n",
    "    return total_loss, grad_W, grad_C\n",
    "\n",
    "\n",
    "# perform gradient descent update of parameters over a mini batch\n",
    "def train_step(W, C, L, batch_size, alpha):\n",
    "    grad_C = np.zeros_like(C) \n",
    "    grad_W = np.zeros_like(W)\n",
    "    total_loss = 0 \n",
    "    for _ in range(batch_size):\n",
    "        # get a random context window\n",
    "        center_word_idx, context_words_idx = get_random_context(L)\n",
    "        # compute loss and gradients for this window \n",
    "        loss, grad_W_window, grad_C_window = skipgram(center_word_idx, context_words_idx, W, C)\n",
    "        # accumulate loss and grads\n",
    "        total_loss += loss\n",
    "        grad_W += grad_W_window\n",
    "        grad_C += grad_C_window\n",
    "\n",
    "    # average over mini-batch\n",
    "    total_loss /= batch_size\n",
    "    grad_W /= batch_size    \n",
    "    grad_C /= batch_size    \n",
    "\n",
    "    # perform sgd update of parameters\n",
    "    W -= alpha * grad_W\n",
    "    C -= alpha * grad_C\n",
    "\n",
    "    return W, C, total_loss\n",
    "\n",
    "# training loop\n",
    "def train(W, C, L, num_epochs=10, batch_size=32, alpha=0.01, print_every=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        W, C, loss = train_step(W, C, L, batch_size, alpha)\n",
    "        if epoch%print_every==0:\n",
    "            print(f\"Epoch #{epoch}, Train Loss: {loss}\")\n",
    "\n",
    "    return W, C    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, Train Loss: 46.66267452867085\n"
     ]
    }
   ],
   "source": [
    "# parameters: embedding matrices\n",
    "W = 0.001 * np.random.randn(vocab_size, D)\n",
    "C = 0.001 * np.random.randn(vocab_size, D)\n",
    "\n",
    "W, C, = train(W, C, L, num_epochs=5000, batch_size=100, alpha=0.2, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
