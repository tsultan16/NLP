{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an Oracle for a Neural Greedy Transition-Based Parser\n",
    "\n",
    "We will now train an `oracle` model for a greedy (arc-standard) transition based-parser. The model extracts (local) features from the state of a parse and uses these features to predict a probability distribution over all possible next actions. For unlabeled arcs, we only have three possible actions: `LEFTARC`, `RIGHTARC` and `SHIFT`. The model architecture is shown below (diagram borrowed from Jurafsky-Martin textbook):\n",
    "\n",
    "<img src=\"neural_oracle.png\" width=\"550\" height=\"320\">\n",
    "\n",
    "The most important features needed to predict the next action are usually the top few words on the stack and buffer (and dependents of these words), which is why for this model, we will construct a simple feature vector by concatenating the contextualized embeddings (from A BERT encoder) of the top two words from the stack and the top word from the buffer. We will designate the BERT `[CLS]` token as the `ROOT` and we will use the zero vector to represent `NULL` (e.g. if the buffer is empty or the stack con tains less than 2 words, we fill the empty positions with NULL).\n",
    "\n",
    "The feature vector is then passed through a basic 2-layer `feed-forward network` with a softmax at the output to obtain the predicted probability distribution over actions. In order to accomodate labelled arcs, we have two options: \n",
    "\n",
    "1) Augment the action with the label, i.e. for each `label`, we have two actions: `LEFTARC-label`, `RIGHTARC-label`\n",
    "\n",
    "2) Create a separate feed-forward network which predicts the label, independent from the action. Since the `SHIFT` action has no associated label, we will define a special `NULL label` that goes along with it.\n",
    "\n",
    "We will use Option 2 because it makes the learning task easier (because there are fewer class labels to predict) and is more efficient and could potentially lead to better performance (since it requires fewer parameters and so there's less chance of overfitting).\n",
    "\n",
    "We will set up our training oracle such that it returns the state and action pairs needed for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "random.seed(10)\n",
    "import psutil\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trainind and vaidation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading CoNLL parse files\n",
    "def read_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Read a CoNLL-U file and return a list of sentences, where each sentence is a list of dictionaries, one for each token.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentences = f.read().strip().split('\\n\\n')\n",
    "        examples = []\n",
    "        for sentence in sentences:\n",
    "            token_dicts = []\n",
    "            for line in sentence.split('\\n'):\n",
    "                if line[0] == '#':\n",
    "                    continue    \n",
    "                token_dict = list(zip(['id', 'form', 'lemma', 'upostag', 'xpostag' , 'feats', 'head', 'deprel', 'deps', 'misc'], line.split('\\t')))\n",
    "                # only keep form, xpostag, head, and deprel\n",
    "                token_dicts.append(dict([token_dict[1], token_dict[4], token_dict[6], token_dict[7]]))\n",
    "            examples.append(token_dicts)\n",
    "        return examples\n",
    "    \n",
    "\n",
    "# function for extracting all the tokens and labelled head-dependency relations from the data\n",
    "def get_tokens_relations(data_instance):\n",
    "    \"\"\"\n",
    "    Extract all the labeled dependency relations from the data.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    relations = []\n",
    "    for token_id, token in enumerate(data_instance):\n",
    "        head_id = int(token['head'])\n",
    "        if head_id == 0:\n",
    "            head = 'ROOT'\n",
    "        else:\n",
    "            head = data_instance[head_id - 1]['form']\n",
    "        dependent = token['form']\n",
    "        tokens.append((dependent, token_id+1))\n",
    "        relation = token['deprel']\n",
    "        relations.append(((head, head_id), (dependent, token_id+1), relation))\n",
    "    return tokens, relations\n",
    "\n",
    "\n",
    "# training oracle returns the state-action pairs from every step of the parsing process\n",
    "# the state only consists of the top two words on the stack and top word on the buffer\n",
    "def training_oracle(data_instance, return_states=False, max_iters=100, verbose=False):\n",
    "    # get the tokens and relations for the refenrence parse \n",
    "    tokens, Rp = get_tokens_relations(data_instance)\n",
    "    sentence_words = [t[0] for t in tokens]\n",
    "    if verbose: \n",
    "        print(f\"Sentence: {sentence_words}\")\n",
    "        print(f\"Reference parse: {Rp}\")\n",
    "\n",
    "    head_dep = [(r[0], r[1]) for r in Rp]\n",
    "\n",
    "    # intialize the stack and buffer\n",
    "    stack = [('ROOT', 0), tokens[0]]\n",
    "    buffer = tokens[1:]\n",
    "    Rc = []\n",
    "    states = None\n",
    "    if return_states:\n",
    "        states = [([('ROOT', 0)], tokens[0])]\n",
    "    actions = ['SHIFT']\n",
    "    labels = ['null']\n",
    "    # parse the sentence to get the sequence of states and actions\n",
    "    niters = 0\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"\\nStack: {stack}\")\n",
    "        print(f\"Buffer: {buffer}\")    \n",
    "\n",
    "    while (buffer or len(stack) > 1) and niters < max_iters:\n",
    "        # get top two elements of stack\n",
    "        S1 = stack[-1]\n",
    "        S2 = stack[-2] \n",
    "        niters += 1\n",
    "\n",
    "        if return_states:\n",
    "            if len(buffer) > 0:\n",
    "                states.append((stack[-2:] , buffer[0]))\n",
    "            else:\n",
    "                states.append((stack[-2:], None))\n",
    "\n",
    "        # check if LEFTARC possible\n",
    "        if (S1, S2) in head_dep:\n",
    "            # remove second element of stack\n",
    "            stack.pop(-2)\n",
    "            rel = Rp[head_dep.index((S1, S2))]\n",
    "            Rc.append(rel)\n",
    "            next_action = 'LEFTARC' \n",
    "            next_label = rel[2]\n",
    "            arc = (S1, S2, rel[2])\n",
    "\n",
    "        # check if RIGHTARC possible\n",
    "        elif (S2, S1) in head_dep:\n",
    "            # get all head-dependent relations with S1 as head\n",
    "            S1_rels = [r for r in Rp if r[0] == S1]\n",
    "            # check if all dependents of S1 are in Rc\n",
    "            if all([r in Rc for r in S1_rels]):\n",
    "                stack.pop(-1)\n",
    "                rel = Rp[head_dep.index((S2, S1))]\n",
    "                Rc.append(rel)\n",
    "                next_action = 'RIGHTARC' \n",
    "                next_label = rel[2]\n",
    "                arc = (S2, S1, rel[2])\n",
    "            else:\n",
    "                if len(buffer)==0:\n",
    "                    if verbose: print(f\"Error! Parse failed, no valid action available!\")\n",
    "                    return None, None, None, None\n",
    "                stack.append(buffer.pop(0))\n",
    "                next_action = 'SHIFT'\n",
    "                next_label = 'null'\n",
    "                arc = None\n",
    "\n",
    "        # otherwise SHIFT    \n",
    "        else:\n",
    "            if len(buffer)==0:\n",
    "                    if verbose: print(f\"Error! Parse failed, no valid action available!\")\n",
    "                    return None, None, None, None\n",
    "            stack.append(buffer.pop(0))\n",
    "            next_action = 'SHIFT'\n",
    "            next_label = 'null'\n",
    "            arc = None\n",
    "\n",
    "        actions.append(next_action)\n",
    "        labels.append(next_label)\n",
    "        if verbose:\n",
    "            print(f\"Action: {next_action}, Arc: {arc}\")\n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")\n",
    "            print(f\"Rc: {Rc}\")      \n",
    "\n",
    "    # make sure Rc and Rp are consistent\n",
    "    assert all([r in Rc for r in Rp]) and len(Rc)==len(Rp), \"Rc not consistent with Rp\"\n",
    "\n",
    "    if niters == max_iters:\n",
    "        print(\"Maximum number of iterations reached!\")  \n",
    "\n",
    "    return states, actions, labels, sentence_words    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 39832\n",
      "Number of sentences in the validation data: 1700\n"
     ]
    }
   ],
   "source": [
    "data_train = read_conllu(os.path.join('data', 'train.conll'))\n",
    "data_val = read_conllu(os.path.join('data', 'dev.conll'))\n",
    "\n",
    "print(f\"Number of sentences in the training data: {len(data_train)}\")\n",
    "print(f\"Number of sentences in the validation data: {len(data_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use training oracle to get the sequence of state-action pairs for each parser step for every sentence. Note that a small number of parses will fail, probably due to non-projectivity of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed: 100%|██████████| 39832/39832 [00:05<00:00, 6769.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed parses: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed: 100%|██████████| 1700/1700 [00:00<00:00, 11476.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed parses: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lets get the state-action pairs for all sentences\n",
    "state_action_label_train = []\n",
    "sentence_words_train = []\n",
    "failed_train = []\n",
    "pbar = tqdm(data_train, desc=\"Sentences parsed\")\n",
    "for i, example in enumerate(pbar):\n",
    "    #print(\"Parsing sentence\", i, \"of\", len(data_train))\n",
    "    states, actions, labels, sentence_words  = training_oracle(example, return_states=True, max_iters=100000)\n",
    "    if actions is None:\n",
    "        failed_train.append(i)\n",
    "    else:\n",
    "        state_action_label_train.append((states, actions, labels))\n",
    "        sentence_words_train.append(sentence_words)\n",
    "print(f\"Number of failed parses: {len(failed_train)}\")\n",
    "\n",
    "state_action_label_val = []\n",
    "sentence_words_val = []\n",
    "failed_val = []\n",
    "pbar = tqdm(data_val, desc=\"Sentences parsed\")\n",
    "for i, example in enumerate(pbar):\n",
    "    #print(\"Parsing sentence\", i, \"of\", len(data_val))\n",
    "    states, actions, labels, sentence_words = training_oracle(example, return_states=True, max_iters=100000)\n",
    "    if actions is None:\n",
    "        failed_val.append(i)\n",
    "    else:\n",
    "        state_action_label_val.append((states, actions, labels))\n",
    "        sentence_words_val.append(sentence_words)   \n",
    "print(f\"Number of failed parses: {len(failed_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels and actions to indices\n",
    "action2idx = {'LEFTARC': 0, 'RIGHTARC': 1, 'SHIFT': 2}\n",
    "labels = list(set([l for item in (state_action_label_train+state_action_label_val) for l in item[2]]))\n",
    "label2idx = {l: i for i, l in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\\n\\nsentence = sentence_words_train[0]\\ninput_encoding = tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\\ninput_idx = input_encoding['input_ids']\\nword_ids = input_encoding.word_ids()\\nprint(tokenizer.convert_ids_to_tokens(input_idx))\\nprint(word_ids)\\n\\nstate, action, label = state_action_label_train[0]\\nstack_words, buffer_word = state[0]\\nprint(stack_words, buffer_word)\\n\\nstate_words_idx = [tokenizer.pad_token_id] * 3\\nprint(state_words_idx)\\nfor i in range(len(stack_words)):\\n    if stack_words[i][0] == 'ROOT':\\n        state_words_idx[i] = tokenizer.cls_token_id\\n    else:\\n        state_words_idx[i] = word_ids.index(stack_words[i][1]-1)\\n\\nif buffer_word is not None:\\n    state_words_idx[2] = word_ids.index(buffer_word[1]-1)\\nprint(state_words_idx)   \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "sentence = sentence_words_train[0]\n",
    "input_encoding = tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "input_idx = input_encoding['input_ids']\n",
    "word_ids = input_encoding.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(input_idx))\n",
    "print(word_ids)\n",
    "\n",
    "state, action, label = state_action_label_train[0]\n",
    "stack_words, buffer_word = state[0]\n",
    "print(stack_words, buffer_word)\n",
    "\n",
    "state_words_idx = [tokenizer.pad_token_id] * 3\n",
    "print(state_words_idx)\n",
    "for i in range(len(stack_words)):\n",
    "    if stack_words[i][0] == 'ROOT':\n",
    "        state_words_idx[i] = tokenizer.cls_token_id\n",
    "    else:\n",
    "        state_words_idx[i] = word_ids.index(stack_words[i][1]-1)\n",
    "\n",
    "if buffer_word is not None:\n",
    "    state_words_idx[2] = word_ids.index(buffer_word[1]-1)\n",
    "print(state_words_idx)   \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can set up a pytorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyParseDataset(Dataset):\n",
    "    def __init__(self, sentences, state_action_label, action2idx, label2idx, block_size=256):\n",
    "        self.sentences = sentences\n",
    "        self.state_action_label = state_action_label\n",
    "        self.action2idx = action2idx\n",
    "        self.label2idx = label2idx\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get sentence \n",
    "        sentence = self.sentences[idx]\n",
    "        # get states, actions, and labels\n",
    "        states, actions, labels = self.state_action_label[idx]\n",
    "\n",
    "        assert len(states) == len(actions) == len(labels), \"Lengths of states, actions, and labels do not match.\"\n",
    "\n",
    "        # tokenize the sentence\n",
    "        input_encoding = self.tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "        input_idx = input_encoding['input_ids']\n",
    "        word_ids = input_encoding.word_ids()\n",
    "\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise ValueError(f\"Tokenized sentence {idx} is too long: {len(input_idx)}. Truncation unsupported.\")\n",
    "\n",
    "        # map state words to index of first subword token\n",
    "        state_idx = []\n",
    "        for stack_words, buffer_word in states:\n",
    "            state_words_idx = [self.tokenizer.pad_token_id] * 3  # missing words are filled with PAD token\n",
    "            for i in range(len(stack_words)):\n",
    "                if stack_words[i][0] == 'ROOT':\n",
    "                    state_words_idx[i] = self.tokenizer.cls_token_id  # ROOT is represented by CLS token\n",
    "                else:\n",
    "                    state_words_idx[i] = word_ids.index(stack_words[i][1]-1)\n",
    "            \n",
    "            if buffer_word is not None:\n",
    "                state_words_idx[2] = word_ids.index(buffer_word[1]-1)\n",
    "            \n",
    "            state_idx.append(state_words_idx)\n",
    "\n",
    "        # map actions and labels to indices\n",
    "        action_idx = [self.action2idx[a] for a in actions]\n",
    "        label_idx = [self.label2idx[l] for l in labels]    \n",
    "\n",
    "        # add padding \n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "        # create attention mask \n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask) \n",
    "        \n",
    "        return input_idx, input_attn_mask, state_idx, action_idx, label_idx   \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the tensors and the dictionaries\n",
    "    input_idxs, input_attn_masks, state_idx, action_idx, label_idx = zip(*batch)\n",
    "\n",
    "    # Default collate the tensors\n",
    "    input_idxs = torch.stack(input_idxs)\n",
    "    input_attn_masks = torch.stack(input_attn_masks)\n",
    "\n",
    "    return input_idxs, input_attn_masks, state_idx, action_idx, label_idx \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define the oracle model. The model consist of a pre-trained BERT encoder and two MLP classification heads, one head for classifying the unlabeled-action and the other head for classifying the arc-label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_ORACLE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_actions, num_labels, num_features=3, unlabeled_arcs=True, dropout_rate=0.1, mlp_hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.unlabeled_arcs = unlabeled_arcs\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define action classifier head (2 layer MLP)\n",
    "        self.classifier_head_action = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features * self.bert_encoder.config.hidden_size, mlp_hidden_size),\n",
    "            torch.nn.LayerNorm(mlp_hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(mlp_hidden_size, num_actions)\n",
    "        )\n",
    "        if not self.unlabeled_arcs:\n",
    "            # define arc-label classifier head (2 layer MLP)\n",
    "            self.classifier_head_label = torch.nn.Sequential(\n",
    "                torch.nn.Linear(num_features * self.bert_encoder.config.hidden_size, mlp_hidden_size),\n",
    "                torch.nn.LayerNorm(mlp_hidden_size),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(mlp_hidden_size, num_labels)\n",
    "            )\n",
    "\n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def get_features(self, bert_output, state_idx, batch_idx):\n",
    "        features_states = []\n",
    "        for i in range(len(state_idx[batch_idx])):\n",
    "            # get BERT embeddings for the top two words on the stack and the top word on the buffer\n",
    "            # (the embedding of a word is being represented by the embedding of it's first subword token)\n",
    "            stack1 = bert_output[batch_idx, state_idx[batch_idx][i][0], :] # shape: (hidden_size,)\n",
    "            stack2 = bert_output[batch_idx, state_idx[batch_idx][i][1], :] # shape: (hidden_size,)\n",
    "            buffer = bert_output[batch_idx, state_idx[batch_idx][i][2], :] # shape: (hidden_size,)\n",
    "            # concatenate the embeddings\n",
    "            features = torch.cat([stack1, stack2, buffer], dim=0) # shape: (3*hidden_size,)\n",
    "            features_states.append(features) \n",
    "        # stack up the features for all states into a single tensor\n",
    "        features = torch.stack(features_states) # shape: (num_states, 3*hidden_size)   \n",
    "        return features\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, state_idx, target_action_idx=None, target_label_idx=None):\n",
    "        # compute BERT embeddings for input tokens\n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask)\n",
    "        bert_output = self.dropout(bert_output.last_hidden_state) # shape: (batch_size, block_size, hidden_size)\n",
    "\n",
    "        loss = 0.0\n",
    "        batch_action_logits = []\n",
    "        batch_label_logits = []\n",
    "        # iterate over each sentence in the batch\n",
    "        for batch_idx in range(len(input_idx)):  \n",
    "            # get the features for all parse states\n",
    "            features = self.get_features(bert_output, state_idx, batch_idx) # shape: (num_states, 3*hidden_size)\n",
    "            # compute action logits and cross-entropy loss\n",
    "            action_logits = self.classifier_head_action(features) # shape: (num_states, num_actions)\n",
    "            batch_action_logits.append(action_logits)\n",
    "            if target_action_idx is not None:\n",
    "                action_targets = torch.tensor(target_action_idx[batch_idx], dtype=torch.long, device=input_idx.device)\n",
    "                loss += F.cross_entropy(action_logits, action_targets)\n",
    "            if not self.unlabeled_arcs:\n",
    "                # compute arc-label logits and cross-entropy loss \n",
    "                label_logits = self.classifier_head_label(features) # shape: (num_states, num_labels)\n",
    "                batch_label_logits.append(label_logits)\n",
    "                if target_label_idx is not None:\n",
    "                    label_targets = torch.tensor(target_label_idx[batch_idx], dtype=torch.long, device=input_idx.device)\n",
    "                    loss += F.cross_entropy(label_logits, label_targets)\n",
    "\n",
    "        # average loss over the batch\n",
    "        loss = loss/len(input_idx)    \n",
    "\n",
    "        return loss, batch_action_logits, batch_label_logits    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_uas = 0\n",
    "        train_las = 0\n",
    "        val_loss = 0\n",
    "        val_uas = 0\n",
    "        val_las = 0\n",
    "        num_instances = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for step, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, state_idx, target_action_idx, target_label_idx = batch\n",
    "            # move tensors to device\n",
    "            input_idx, input_attn_mask = input_idx.to(device), input_attn_mask.to(device)\n",
    "            # forward pass\n",
    "            loss, batch_action_logits, batch_label_logits = model(input_idx, input_attn_mask, state_idx, target_action_idx, target_label_idx)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "\n",
    "            # compute unlabeled and labeled attachment scores\n",
    "            for batch_idx in range(len(input_idx)):\n",
    "                action_logits = batch_action_logits[batch_idx]\n",
    "                action_idx = target_action_idx[batch_idx]\n",
    "                if not model.unlabeled_arcs:\n",
    "                    label_logits = batch_label_logits[batch_idx]\n",
    "                    label_idx = target_label_idx[batch_idx]\n",
    "                # compute UAS and LAS\n",
    "                sentence_uas = 0\n",
    "                sentence_las = 0\n",
    "                for i in range(len(action_idx)):\n",
    "                    if action_idx[i] == torch.argmax(action_logits[i]):\n",
    "                        sentence_uas += 1\n",
    "                        if not model.unlabeled_arcs:\n",
    "                            if label_idx[i] == torch.argmax(label_logits[i]):\n",
    "                                sentence_las += 1                \n",
    "                sentence_uas = sentence_uas/len(action_idx)\n",
    "                train_uas += sentence_uas\n",
    "                if not model.unlabeled_arcs:\n",
    "                    sentence_las = sentence_las/len(action_idx)\n",
    "                    train_las += sentence_las\n",
    "                num_instances += 1    \n",
    "\n",
    "            if val_every is not None:\n",
    "                if (step+1)%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_uas, val_las = validation(model, val_dataloader, device=device)\n",
    "                    pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train UAS, LAS: ({train_uas/num_instances: .3f}, {train_las/num_instances: .3f}), Val Loss: {val_loss: .3f}, Val UAS, LAS: ({val_uas: .3f}, {val_las: .3f})\")  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train UAS, LAS: ({train_uas/num_instances: .3f}, {train_las/num_instances: .3f}), Val Loss: {val_loss: .3f}, Val UAS, LAS: ({val_uas: .3f}, {val_las: .3f})\")\n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\":loss.item(), \"Moving Avg Loss\":avg_loss, \"Train UAS\":train_uas/num_instances, \"Train LAS\":train_las/num_instances,\"Val Loss\": val_loss, \"Val UAS\":val_uas, \"Val LAS\":val_las}   \n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        val_uas = 0\n",
    "        val_las = 0\n",
    "        num_instances = 0\n",
    "        for step,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, state_idx, target_action_idx, target_label_idx = batch\n",
    "            input_idx, input_attn_mask = input_idx.to(device), input_attn_mask.to(device)\n",
    "            loss, batch_action_logits, batch_label_logits = model(input_idx, input_attn_mask, state_idx, target_action_idx, target_label_idx)\n",
    "            \n",
    "            # compute unlabeled and labeled attachment scores\n",
    "            for batch_idx in range(len(input_idx)):\n",
    "                action_logits = batch_action_logits[batch_idx]\n",
    "                action_idx = target_action_idx[batch_idx]\n",
    "                if not model.unlabeled_arcs:\n",
    "                    label_logits = batch_label_logits[batch_idx]\n",
    "                    label_idx = target_label_idx[batch_idx]\n",
    "                # compute UAS and LAS\n",
    "                sentence_uas = 0\n",
    "                sentence_las = 0\n",
    "                for i in range(len(action_idx)):\n",
    "                    if action_idx[i] == torch.argmax(action_logits[i]):\n",
    "                        sentence_uas += 1\n",
    "                        if not model.unlabeled_arcs:\n",
    "                            if label_idx[i] == torch.argmax(label_logits[i]):\n",
    "                                sentence_las += 1                \n",
    "                sentence_uas = sentence_uas/len(action_idx)\n",
    "                val_uas += sentence_uas\n",
    "                if not model.unlabeled_arcs:\n",
    "                    sentence_las = sentence_las/len(action_idx)\n",
    "                    val_las += sentence_las\n",
    "                num_instances += 1  \n",
    "\n",
    "            val_losses[step] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_uas = val_uas/num_instances\n",
    "    val_las = val_las/num_instances\n",
    "    return val_loss, val_uas, val_las\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename='BERT_TRANSITION_PARSER_checkpoint.pth'):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer=None,  filename='BERT_TRANSITION_PARSER_checkpoint.pth'):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. First, we will train without the arc labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training batches: 4964\n",
      "Num validation batches: 212\n"
     ]
    }
   ],
   "source": [
    "B = 8\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 1e-5\n",
    "epochs = 3\n",
    "\n",
    "train_dataset = DependencyParseDataset(sentence_words_train, state_action_label_train, action2idx, label2idx)\n",
    "val_dataset = DependencyParseDataset(sentence_words_val, state_action_label_val, action2idx, label2idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Num training batches: {len(train_dataloader)}\")\n",
    "print(f\"Num validation batches: {len(val_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 66.658563 M\n",
      "RAM used: 3229.03 MB\n"
     ]
    }
   ],
   "source": [
    "model = BERT_ORACLE(num_actions=len(action2idx), num_labels=len(label2idx), unlabeled_arcs=True).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "warmup_steps = int(len(train_dataloader) * 0.1 *  epochs) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Dependency_Grammars_Parsing/wandb/run-20240207_171045-vb4bfh6f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser/runs/vb4bfh6f' target=\"_blank\">winter-armadillo-2</a></strong> to <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser/runs/vb4bfh6f' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser/runs/vb4bfh6f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"BERT Transition Dependency Parser\", \n",
    "    config={\n",
    "        \"model\": \"DistillBERT\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"Penn Treebank\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 1.477, Train UAS, LAS: ( 0.908,  0.000), Val Loss:  1.451, Val UAS, LAS: ( 0.972,  0.000): 100%|██████████| 2482/2482 [25:00<00:00,  1.65it/s]   \n",
      "Epoch 2, EMA Train Loss: 1.115, Train UAS, LAS: ( 0.977,  0.000), Val Loss:  1.251, Val UAS, LAS: ( 0.976,  0.000): 100%|██████████| 2482/2482 [25:09<00:00,  1.64it/s]  \n",
      "Epoch 3, EMA Train Loss: 0.978, Train UAS, LAS: ( 0.980,  0.000), Val Loss:  1.174, Val UAS, LAS: ( 0.978,  0.000): 100%|██████████| 2482/2482 [25:06<00:00,  1.65it/s]  \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=epochs, scheduler=scheduler, save_every=None, val_every=200, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, the unlabelled attachment score on the validation set is over 97%. Now, let's try training the modeled with arc labelling as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 66.959019 M\n",
      "RAM used: 3212.10 MB\n"
     ]
    }
   ],
   "source": [
    "model = BERT_ORACLE(num_actions=len(action2idx), num_labels=len(label2idx), unlabeled_arcs=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "warmup_steps = int(len(train_dataloader) * 0.1 *  epochs) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Dependency_Grammars_Parsing/wandb/run-20240207_184015-86s3o2ak</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels/runs/86s3o2ak' target=\"_blank\">helpful-dew-2</a></strong> to <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels/runs/86s3o2ak' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels/runs/86s3o2ak</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"BERT Transition Dependency Parser with Arc Labels\", \n",
    "    config={\n",
    "        \"model\": \"DistillBERT\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"Penn Treebank\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 3.751, Train UAS, LAS: ( 0.914,  0.794), Val Loss:  3.512, Val UAS, LAS: ( 0.973,  0.934): 100%|██████████| 4964/4964 [26:49<00:00,  3.08it/s]    \n",
      "Epoch 2, EMA Train Loss: 2.521, Train UAS, LAS: ( 0.976,  0.943), Val Loss:  2.390, Val UAS, LAS: ( 0.976,  0.946): 100%|██████████| 4964/4964 [26:54<00:00,  3.07it/s]  \n",
      "Epoch 3, EMA Train Loss: 2.313, Train UAS, LAS: ( 0.978,  0.949), Val Loss:  2.230, Val UAS, LAS: ( 0.976,  0.948): 100%|██████████| 4964/4964 [26:37<00:00,  3.11it/s]  \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=epochs, scheduler=scheduler, save_every=None, val_every=200, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The arc-labeled model also acheived high UAS (97.6%) and LAS (94.8%) on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1a6b3fa1b747eeb7be547b477b17ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch loss</td><td>█▆▅▄▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Moving Avg Loss</td><td>█▆▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train LAS</td><td>▁▃▄▄▅▅▆▆▆▆▆▇▇███████████████████████████</td></tr><tr><td>Train UAS</td><td>▁▂▄▅▅▆▆▆▇▇▇▇▇███████████████████████████</td></tr><tr><td>Val LAS</td><td>▁▄▅▆▇▇▇██████▁██████████████████████████</td></tr><tr><td>Val Loss</td><td>▁█▆▄▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>Val UAS</td><td>▁▅▇██████████▁██████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch loss</td><td>1.94826</td></tr><tr><td>Moving Avg Loss</td><td>2.31301</td></tr><tr><td>Train LAS</td><td>0.94861</td></tr><tr><td>Train UAS</td><td>0.97752</td></tr><tr><td>Val LAS</td><td>0.94843</td></tr><tr><td>Val Loss</td><td>2.23043</td></tr><tr><td>Val UAS</td><td>0.97592</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">helpful-dew-2</strong> at: <a href='https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels/runs/86s3o2ak' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Transition%20Dependency%20Parser%20with%20Arc%20Labels/runs/86s3o2ak</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240207_184015-86s3o2ak/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save_model_checkpoint(model, optimizer)\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
