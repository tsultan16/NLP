{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Transition-Based Parser\n",
    "\n",
    "We will now train an `oracle` model for a greedy (arc-standard) transition based-parser. The model extracts features from the state of a parse and uses these features to predict a probability distribution over all possible next actions. For unlabeled arcs, we only have three possible actions: `LEFTARC`, `RIGHTARC` and `SHIFT`. The model architecture is shown below (diagram borrowed from Jurafsky-Martin textbook):\n",
    "\n",
    "<img src=\"neural_oracle.png\" width=\"550\" height=\"320\">\n",
    "\n",
    "The most important features needed to predict the next action are usually the top few words on the stack and buffer (and dependents of these words), which is why for this model, we will construct a simple feature vector by concatenating the contextualized embeddings (from A BERT encoder) of the top two words from the stack and the top word from the buffer. We will designate the BERT `[CLS]` token as the `ROOT` and we will use the zero vector to represent `NULL` (e.g. if the buffer is empty or the stack con tains less than 2 words, we fill the empty positions with NULL).\n",
    "\n",
    "The feature vector is then passed through a basic 2-layer `feed-forward network` with a softmax at the output to obtain the predicted probability distribution over actions. In order to accomodate labelled arcs, we have two options: \n",
    "\n",
    "1) Augment the action with the label, i.e. for each `label`, we have two actions: `LEFTARC-label`, `RIGHTARC-label`\n",
    "\n",
    "2) Create a separate feed-forward network which predicts the label, independent from the action. Since the `SHIFT` action has no associated label, we will define a special `NULL label` that goes along with it.\n",
    "\n",
    "We will use Option 2 because it makes the learning task easier (because there are fewer class labels to predict) and is more efficient and could potentially lead to better performance (since it requires fewer parameters and so there's less chance of overfitting).\n",
    "\n",
    "We will set up our training oracle such that it returns the state and action pairs needed for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "random.seed(10)\n",
    "import psutil\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trainind and vaidation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for reading CoNLL parse files\n",
    "def read_conllu(file_path):\n",
    "    \"\"\"\n",
    "    Read a CoNLL-U file and return a list of sentences, where each sentence is a list of dictionaries, one for each token.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentences = f.read().strip().split('\\n\\n')\n",
    "        examples = []\n",
    "        for sentence in sentences:\n",
    "            token_dicts = []\n",
    "            for line in sentence.split('\\n'):\n",
    "                if line[0] == '#':\n",
    "                    continue    \n",
    "                token_dict = list(zip(['id', 'form', 'lemma', 'upostag', 'xpostag' , 'feats', 'head', 'deprel', 'deps', 'misc'], line.split('\\t')))\n",
    "                # only keep form, xpostag, head, and deprel\n",
    "                token_dicts.append(dict([token_dict[1], token_dict[4], token_dict[6], token_dict[7]]))\n",
    "            examples.append(token_dicts)\n",
    "        return examples\n",
    "    \n",
    "\n",
    "# function for extracting all the tokens and labelled head-dependency relations from the data\n",
    "def get_tokens_relations(data_instance):\n",
    "    \"\"\"\n",
    "    Extract all the labeled dependency relations from the data.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    relations = []\n",
    "    for token_id, token in enumerate(data_instance):\n",
    "        head_id = int(token['head'])\n",
    "        if head_id == 0:\n",
    "            head = 'ROOT'\n",
    "        else:\n",
    "            head = data_instance[head_id - 1]['form']\n",
    "        dependent = token['form']\n",
    "        tokens.append((dependent, token_id+1))\n",
    "        relation = token['deprel']\n",
    "        relations.append(((head, head_id), (dependent, token_id+1), relation))\n",
    "    return tokens, relations\n",
    "\n",
    "\n",
    "# training oracle returns the state-action pairs from every step of the parsing process\n",
    "# the state only consists of the top two words on the stack and top word on the buffer\n",
    "def training_oracle(data_instance, return_states=False, max_iters=100, verbose=False):\n",
    "    # get the tokens and relations for the refenrence parse \n",
    "    tokens, Rp = get_tokens_relations(data_instance)\n",
    "    sentence_words = [t[0] for t in tokens]\n",
    "    if verbose: \n",
    "        print(f\"Sentence: {sentence_words}\")\n",
    "        print(f\"Reference parse: {Rp}\")\n",
    "\n",
    "    head_dep = [(r[0], r[1]) for r in Rp]\n",
    "\n",
    "    # intialize the stack and buffer\n",
    "    stack = [('ROOT', 0), tokens[0]]\n",
    "    buffer = tokens[1:]\n",
    "    Rc = []\n",
    "    states = None\n",
    "    if return_states:\n",
    "        states = [([('ROOT', 0)], tokens[0])]\n",
    "    actions = ['SHIFT']\n",
    "    labels = ['null']\n",
    "    # parse the sentence to get the sequence of states and actions\n",
    "    niters = 0\n",
    "    \n",
    "    if verbose: \n",
    "        print(f\"\\nStack: {stack}\")\n",
    "        print(f\"Buffer: {buffer}\")    \n",
    "\n",
    "    while (buffer or len(stack) > 1) and niters < max_iters:\n",
    "        # get top two elements of stack\n",
    "        S1 = stack[-1]\n",
    "        S2 = stack[-2] \n",
    "        niters += 1\n",
    "\n",
    "        if return_states:\n",
    "            if len(buffer) > 0:\n",
    "                states.append((stack[-2:] , buffer[0]))\n",
    "            else:\n",
    "                states.append((stack[-2:], None))\n",
    "\n",
    "        # check if LEFTARC possible\n",
    "        if (S1, S2) in head_dep:\n",
    "            # remove second element of stack\n",
    "            stack.pop(-2)\n",
    "            rel = Rp[head_dep.index((S1, S2))]\n",
    "            Rc.append(rel)\n",
    "            next_action = 'LEFTARC' \n",
    "            next_label = rel[2]\n",
    "            arc = (S1, S2, rel[2])\n",
    "\n",
    "        # check if RIGHTARC possible\n",
    "        elif (S2, S1) in head_dep:\n",
    "            # get all head-dependent relations with S1 as head\n",
    "            S1_rels = [r for r in Rp if r[0] == S1]\n",
    "            # check if all dependents of S1 are in Rc\n",
    "            if all([r in Rc for r in S1_rels]):\n",
    "                stack.pop(-1)\n",
    "                rel = Rp[head_dep.index((S2, S1))]\n",
    "                Rc.append(rel)\n",
    "                next_action = 'RIGHTARC' \n",
    "                next_label = rel[2]\n",
    "                arc = (S2, S1, rel[2])\n",
    "            else:\n",
    "                if len(buffer)==0:\n",
    "                    if verbose: print(f\"Error! Parse failed, no valid action available!\")\n",
    "                    return None, None, None, None\n",
    "                stack.append(buffer.pop(0))\n",
    "                next_action = 'SHIFT'\n",
    "                next_label = 'null'\n",
    "                arc = None\n",
    "\n",
    "        # otherwise SHIFT    \n",
    "        else:\n",
    "            if len(buffer)==0:\n",
    "                    if verbose: print(f\"Error! Parse failed, no valid action available!\")\n",
    "                    return None, None, None, None\n",
    "            stack.append(buffer.pop(0))\n",
    "            next_action = 'SHIFT'\n",
    "            next_label = 'null'\n",
    "            arc = None\n",
    "\n",
    "        actions.append(next_action)\n",
    "        labels.append(next_label)\n",
    "        if verbose:\n",
    "            print(f\"Action: {next_action}, Arc: {arc}\")\n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")\n",
    "            print(f\"Rc: {Rc}\")      \n",
    "\n",
    "    # make sure Rc and Rp are consistent\n",
    "    assert all([r in Rc for r in Rp]) and len(Rc)==len(Rp), \"Rc not consistent with Rp\"\n",
    "\n",
    "    if niters == max_iters:\n",
    "        print(\"Maximum number of iterations reached!\")  \n",
    "\n",
    "    return states, actions, labels, sentence_words    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 39832\n",
      "Number of sentences in the validation data: 1700\n"
     ]
    }
   ],
   "source": [
    "data_train = read_conllu(os.path.join('data', 'train.conll'))\n",
    "data_val = read_conllu(os.path.join('data', 'dev.conll'))\n",
    "\n",
    "print(f\"Number of sentences in the training data: {len(data_train)}\")\n",
    "print(f\"Number of sentences in the validation data: {len(data_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use training oracle to get the sequence of state-action pairs for each sentence. Note that a small number of parses will fail, probably due to non-projectivity of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed: 100%|██████████| 39832/39832 [00:06<00:00, 6179.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed parses: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed: 100%|██████████| 1700/1700 [00:00<00:00, 11617.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed parses: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# lets get the state-action pairs for all sentences\n",
    "state_action_label_train = []\n",
    "sentence_words_train = []\n",
    "failed_train = []\n",
    "pbar = tqdm(data_train, desc=\"Sentences parsed\")\n",
    "for i, example in enumerate(pbar):\n",
    "    #print(\"Parsing sentence\", i, \"of\", len(data_train))\n",
    "    states, actions, labels, sentence_words  = training_oracle(example, return_states=True, max_iters=100000)\n",
    "    if actions is None:\n",
    "        failed_train.append(i)\n",
    "    else:\n",
    "        state_action_label_train.append((states, actions, labels))\n",
    "        sentence_words_train.append(sentence_words)\n",
    "print(f\"Number of failed parses: {len(failed_train)}\")\n",
    "\n",
    "state_action_label_val = []\n",
    "sentence_words_val = []\n",
    "failed_val = []\n",
    "pbar = tqdm(data_val, desc=\"Sentences parsed\")\n",
    "for i, example in enumerate(pbar):\n",
    "    #print(\"Parsing sentence\", i, \"of\", len(data_val))\n",
    "    states, actions, labels, sentence_words = training_oracle(example, return_states=True, max_iters=100000)\n",
    "    if actions is None:\n",
    "        failed_val.append(i)\n",
    "    else:\n",
    "        state_action_label_val.append((states, actions, labels))\n",
    "        sentence_words_val.append(sentence_words)   \n",
    "print(f\"Number of failed parses: {len(failed_val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels and actions to indices\n",
    "action2idx = {'LEFTARC': 0, 'RIGHTARC': 1, 'SHIFT': 2}\n",
    "labels = list(set([l for item in (state_action_label_train+state_action_label_val) for l in item[2]]))\n",
    "label2idx = {l: i for i, l in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'in', 'an', 'oct', '.', '19', 'review', 'of', '`', '`', 'the', 'mis', '##ant', '##hr', '##ope', \"'\", \"'\", 'at', 'chicago', \"'\", 's', 'goodman', 'theatre', '-', 'l', '##rb', '-', '`', '`', 'rev', '##ital', '##ized', 'classics', 'take', 'the', 'stage', 'in', 'windy', 'city', ',', \"'\", \"'\", 'leisure', '&', 'arts', '-', 'rr', '##b', '-', ',', 'the', 'role', 'of', 'ce', '##lim', '##ene', ',', 'played', 'by', 'kim', 'cat', '##tral', '##l', ',', 'was', 'mistakenly', 'attributed', 'to', 'christina', 'ha', '##ag', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 2, 3, 4, 5, 6, 6, 7, 8, 8, 8, 8, 9, 9, 10, 11, 12, 12, 13, 14, 15, 15, 15, 15, 16, 16, 17, 17, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27, 28, 29, 30, 30, 30, 30, 31, 32, 33, 34, 35, 35, 35, 36, 37, 38, 39, 40, 40, 40, 41, 42, 43, 44, 45, 46, 47, 47, 48, None]\n"
     ]
    }
   ],
   "source": [
    "sentence = sentence_words_train[0]\n",
    "input_encoding = tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "input_idx = input_encoding['input_ids']\n",
    "word_ids = input_encoding.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(input_idx))\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ROOT', 0)] ('In', 1)\n"
     ]
    }
   ],
   "source": [
    "state, action, label = state_action_label_train[0]\n",
    "stack_words, buffer_word = state[0]\n",
    "print(stack_words, buffer_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "[101, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "state_words_idx = [tokenizer.pad_token_id] * 3\n",
    "print(state_words_idx)\n",
    "for i in range(len(stack_words)):\n",
    "    if stack_words[i][0] == 'ROOT':\n",
    "        state_words_idx[i] = tokenizer.cls_token_id\n",
    "    else:\n",
    "        state_words_idx[i] = word_ids.index(stack_words[i][1]-1)\n",
    "\n",
    "if buffer_word is not None:\n",
    "    state_words_idx[2] = word_ids.index(buffer_word[1]-1)\n",
    "print(state_words_idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can set up a pytorch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyParseDataset(Dataset):\n",
    "    def __init__(self, sentences, state_action_label, action2idx, label2idx, block_size=256):\n",
    "        self.sentences = sentences\n",
    "        self.state_action_label = state_action_label\n",
    "        self.action2idx = action2idx\n",
    "        self.label2idx = label2idx\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get sentence \n",
    "        sentence = self.sentences[idx]\n",
    "        # get states, actions, and labels\n",
    "        states, actions, labels = self.state_action_label[idx]\n",
    "\n",
    "        # tokenize the sentence\n",
    "        input_encoding = self.tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "        input_idx = input_encoding['input_ids']\n",
    "        word_ids = input_encoding.word_ids()\n",
    "\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise ValueError(f\"Tokenized sentence {idx} is too long: {len(input_idx)}. Truncation unsupported.\")\n",
    "\n",
    "        # map state words to index of first subword token\n",
    "        state_idx = []\n",
    "        for stack_words, buffer_word in states:\n",
    "            state_words_idx = [self.tokenizer.pad_token_id] * 3  # missing words are filled with PAD token\n",
    "            for i in range(len(stack_words)):\n",
    "                if stack_words[i][0] == 'ROOT':\n",
    "                    state_words_idx[i] = self.tokenizer.cls_token_id  # ROOT is represented by CLS token\n",
    "                else:\n",
    "                    state_words_idx[i] = word_ids.index(stack_words[i][1]-1)\n",
    "            \n",
    "            if buffer_word is not None:\n",
    "                state_words_idx[2] = word_ids.index(buffer_word[1]-1)\n",
    "            \n",
    "            state_idx.append(state_words_idx)\n",
    "\n",
    "        # map actions and labels to indices\n",
    "        action_idx = [self.action2idx[a] for a in actions]\n",
    "        label_idx = [self.label2idx[l] for l in labels]    \n",
    "\n",
    "        # add padding \n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "        # create attention mask \n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask) \n",
    "        \n",
    "        return input_idx, input_attn_mask, state_idx, action_idx, label_idx   \n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the tensors and the dictionaries\n",
    "    input_idxs, input_attn_masks, state_idx, action_idx, label_idx = zip(*batch)\n",
    "\n",
    "    # Default collate the tensors\n",
    "    input_idxs = torch.stack(input_idxs)\n",
    "    input_attn_masks = torch.stack(input_attn_masks)\n",
    "\n",
    "    return input_idxs, input_attn_masks, state_idx, action_idx, label_idx \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DependencyParseDataset(sentence_words_train, state_action_label_train, action2idx, label2idx)\n",
    "val_dataset = DependencyParseDataset(sentence_words_val, state_action_label_val, action2idx, label2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
