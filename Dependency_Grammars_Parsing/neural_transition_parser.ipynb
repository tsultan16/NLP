{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy (arc-standard) Transition Dependency Parser\n",
    "\n",
    "We will now use the trained neural oracle to perform (arc-standard) dependency parsing. Given a sentence, we initialize a buffer containing the words and punctuation symbols of the sentence, a stack containing the `ROOT` and an empty dependency relations list. Starting from this initial state, we perform parse steps by applying actions chosen by the oracle and updating the system state. When the terminal state is reached (i.e. the buffer is empty and the stack only contains the `ROOT`), the complete dependency parse is contained in the dependency relations list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from parse_utils import *\n",
    "import wandb\n",
    "import pickle \n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load the validation set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the validation data: 1700\n"
     ]
    }
   ],
   "source": [
    "data_val = read_conllu(os.path.join('data', 'dev.conll'))\n",
    "print(f\"Number of sentences in the validation data: {len(data_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset object from file\n",
    "with open('val_dataset_pytorch.pkl', 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "action2idx = val_dataset.action2idx\n",
    "label2idx = val_dataset.label2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the trained oracle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint!\n",
      "Total number of parameters in transformer network: 66.959019 M\n",
      "RAM used: 2780.11 MB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "learning_rate = 1e-5\n",
    "model = BERT_ORACLE(num_actions=len(action2idx), num_labels=len(label2idx), unlabeled_arcs=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nval_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\\nvalidation(model, val_dataloader, device=DEVICE)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "validation(model, val_dataloader, device=DEVICE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implement the greedy arc-standard transition parser. This implementation is very strict and causes parser to fail whenever oracle predicts an invalid action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Greedy arc-standard transition parser\n",
    "\"\"\"\n",
    "def parser(sentence_words, oracle, dataset, verbose=False):\n",
    "    tokens = [(word, i+1) for i,word in enumerate(sentence_words)]\n",
    "\n",
    "    # initialize the state\n",
    "    stack = [('ROOT', 0)]\n",
    "    buffer = tokens.copy()\n",
    "    arcs = []\n",
    "\n",
    "    # convert sentence to model input tensors\n",
    "    input_idx, input_attn_mask, word_ids = dataset.tokenize_sentence(sentence_words)\n",
    "    input_idx = input_idx.unsqueeze(0).to(DEVICE)\n",
    "    input_attn_mask = input_attn_mask.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # set model to eval mode\n",
    "    oracle.eval()\n",
    "    # compute BERT encoding of sentence tokens\n",
    "    with torch.no_grad():\n",
    "        bert_output = oracle.get_bert_encoding(input_idx, input_attn_mask)\n",
    "\n",
    "    labels = list(dataset.label2idx.keys())\n",
    "    actions = list(dataset.action2idx.keys())\n",
    "\n",
    "    if verbose: \n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")   \n",
    "\n",
    "    # begin parsing\n",
    "    while len(buffer) > 0 or len(stack) > 1:\n",
    "\n",
    "        if len(buffer) > 0:\n",
    "            state = [(stack[-2:] , buffer[0])]\n",
    "        else:\n",
    "            state = [(stack[-2:], None)]\n",
    "        state_idx = [dataset.tokenize_state(state, word_ids)]    \n",
    "\n",
    "        # get the oracle action and label scores\n",
    "        action_logits, label_logits = oracle.predict(bert_output, state_idx)\n",
    "        \n",
    "        # pick highest scoring action and label\n",
    "        best_action = actions[torch.argmax(action_logits[0][0])]\n",
    "        best_label = labels[torch.argmax(label_logits[0][0])]\n",
    "\n",
    "        # perform the action\n",
    "        if best_action == 'LEFTARC':\n",
    "            # LEFTARC\n",
    "            if len(stack) > 1:\n",
    "                arcs.append((stack[-1], stack[-2], best_label))\n",
    "                stack.pop(-2)\n",
    "            else:\n",
    "                best_action = 'SHIFT'    \n",
    "\n",
    "        if best_action == 'RIGHTARC':\n",
    "            # RIGHTARC\n",
    "            if len(stack) > 1:\n",
    "                arcs.append((stack[-2], stack[-1], best_label))\n",
    "                stack.pop(-1) \n",
    "            else:\n",
    "                best_action = 'SHIFT'    \n",
    "\n",
    "        if best_action == 'SHIFT':\n",
    "            # SHIFT\n",
    "            if len(buffer) > 0:\n",
    "                stack.append(buffer.pop(0))\n",
    "           \n",
    "        if verbose:\n",
    "            print(f\"Best action: {best_action}, best label: {best_label}\")\n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")\n",
    "            print(f\"Arcs: {arcs}\")        \n",
    "\n",
    "    return arcs                       \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Evaluate the predicted arcs against the gold arcs by computing unlabeled and labeled attachment scores\n",
    "\"\"\"\n",
    "def evaluate(gold_arcs, predicted_arcs):\n",
    "    uas = 0\n",
    "    las = 0\n",
    "    gold_head_deps = [(r[0], r[1]) for r in gold_arcs]\n",
    "\n",
    "    for r in predicted_arcs:\n",
    "        if (r[0], r[1]) in gold_head_deps:\n",
    "            uas += 1\n",
    "            if r in gold_arcs:\n",
    "                las += 1\n",
    "\n",
    "    uas = uas / len(gold_arcs)\n",
    "    las = las / len(gold_arcs)            \n",
    "    return uas, las    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS: 0.9459459459459459, LAS: 0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "# get a test sentence from the validation set and its gold standard parse\n",
    "test_data_instance = data_val[0]\n",
    "gold_states, gold_actions, gold_labels, sentence_words, gold_arcs  = training_oracle(test_data_instance, return_states=True, max_iters=100000)\n",
    "\n",
    "# predict the parse using the oracle\n",
    "precicted_arcs = parser(sentence_words, model, val_dataset, verbose=False)\n",
    "\n",
    "# compare the gold standard and predicted arcs\n",
    "uas, las = evaluate(gold_arcs, precicted_arcs)\n",
    "print(f\"UAS: {uas}, LAS: {las}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed:   0%|          | 0/1700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed: 100%|██████████| 1700/1700 [00:45<00:00, 37.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average UAS: 0.9398265620708417, Average LAS: 0.9016825769288151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the greedy parser on the validation set\n",
    "num_instances = 0\n",
    "uas_tot = 0\n",
    "las_tot = 0\n",
    "# give me a pbar\n",
    "pbar = tqdm(data_val, desc=\"Sentences parsed\")\n",
    "for data_instance in pbar:\n",
    "    gold_states, gold_actions, gold_labels, sentence_words, gold_arcs = training_oracle(data_instance, return_states=True, max_iters=100000)\n",
    "    if gold_states is None:\n",
    "        continue\n",
    "    else:\n",
    "        precicted_arcs = parser(sentence_words, model, val_dataset, verbose=False)\n",
    "        uas, las = evaluate(gold_arcs, precicted_arcs)\n",
    "        uas_tot += uas\n",
    "        las_tot += las\n",
    "        num_instances += 1\n",
    "uas = uas_tot / num_instances\n",
    "las = las_tot / num_instances\n",
    "print(f\"Average UAS: {uas}, Average LAS: {las}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not bad, the greedy parser gets average UAS of over 93% and LAS of 90% on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search\n",
    "\n",
    "To improve the accuracy of the parser, we can use beam search instead of greedily choosing the best possible action at each step. \n",
    "\n",
    "In beam search, we define a beam width $k$ and maintain a search tree and use breadth-first search. The root of the tree is designated to be the initial state. We then expand the root node by exploring all valid actions and generate the resulting states. We define a state score such that the score of the initial state is $0$ and the score of newly generated states is the score of the predecessor state plus the score of the action taken to generate the successor state:\n",
    "\n",
    "$StateScore(s_0) = 0$\n",
    "\n",
    "$StateScore(s_i) = StateScore(s_{i-1}) + Score(s_{i-1}, a)$\n",
    "\n",
    "Then we expand each of these successor states and prune the tree to keep only the  top-$k$ successor states with highest state scores. We continue expanding every node in the beam until it they have all reached a terminal state. Then the best parse is given by the terminal state with the highest state score.\n",
    "\n",
    "The beam search parse algorithm is shown below (borrowed from Jurafsky-Martin textbook):\n",
    "\n",
    "<img src=\"beam_search.png\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_contains_non_final_states(beam):\n",
    "    for state in beam:\n",
    "        stack, buffer, arcs, score = state\n",
    "        if len(buffer) > 0 or len(stack) > 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def add_state_to_beam(beam, state, k):\n",
    "    if len(beam) < k:\n",
    "        beam.append(state)\n",
    "    else:\n",
    "        # replace the state with the lowest score\n",
    "        scores = [s[3] for s in beam]\n",
    "        min_score = min(scores)\n",
    "        if state[3] > min_score:\n",
    "            min_idx = scores.index(min_score)\n",
    "            beam[min_idx] = state\n",
    "    return beam\n",
    "\n",
    "\n",
    "def valid_actions(stack, buffer):\n",
    "    actions = []\n",
    "    if len(buffer) > 0:\n",
    "        actions.append('SHIFT')\n",
    "    if len(stack) > 1:\n",
    "        if stack[-2][0] != 'ROOT':\n",
    "            # ROOT cannont be a dependent\n",
    "            actions.append('LEFTARC')\n",
    "        actions.append('RIGHTARC')\n",
    "    return actions\n",
    "\n",
    "\n",
    "def generate_successor_state(state, action, action_score, best_label):\n",
    "    stack, buffer, arcs, score = state\n",
    "    # copy the state\n",
    "    stack = stack.copy()\n",
    "    buffer = buffer.copy()\n",
    "    arcs = arcs.copy()\n",
    "    # perform the action\n",
    "    if action == 'LEFTARC':\n",
    "        arcs.append((stack[-1], stack[-2], best_label))\n",
    "        stack.pop(-2)            \n",
    "    elif action == 'RIGHTARC':\n",
    "        arcs.append((stack[-2], stack[-1], best_label))\n",
    "        stack.pop(-1) \n",
    "    else:\n",
    "        stack.append(buffer.pop(0))\n",
    "\n",
    "    return (stack, buffer, arcs, score+action_score)    \n",
    "\n",
    "\n",
    "def get_best_state(beam):\n",
    "    best_score = float('-inf')\n",
    "    best_state = None\n",
    "    for state in beam:\n",
    "        if state[3] > best_score:\n",
    "            best_score = state[3]\n",
    "            best_state = state\n",
    "    return best_state\n",
    "\n",
    "\n",
    "def beam_parser(sentence_words, oracle, dataset, k=10, return_beam=False, verbose=False):\n",
    "    tokens = [(word, i+1) for i,word in enumerate(sentence_words)]\n",
    "\n",
    "    # initialize the state\n",
    "    stack = [('ROOT', 0)]\n",
    "    buffer = tokens.copy()\n",
    "    arcs = []\n",
    "    score = 0.0\n",
    "    state = (stack, buffer, arcs, score)\n",
    "    # initialize the beam\n",
    "    beam = [state]\n",
    "\n",
    "    # convert sentence to model input tensors\n",
    "    input_idx, input_attn_mask, word_ids = dataset.tokenize_sentence(sentence_words)\n",
    "    input_idx = input_idx.unsqueeze(0).to(DEVICE)\n",
    "    input_attn_mask = input_attn_mask.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # set model to eval mode\n",
    "    oracle.eval()\n",
    "    # compute BERT encoding of sentence tokens\n",
    "    with torch.no_grad():\n",
    "        bert_output = oracle.get_bert_encoding(input_idx, input_attn_mask)\n",
    "\n",
    "    labels = list(dataset.label2idx.keys())\n",
    "    actions = list(dataset.action2idx.keys())\n",
    "\n",
    "    # begin beam search\n",
    "    while beam_contains_non_final_states(beam):\n",
    "        beam_successors = []\n",
    "        for state in beam:\n",
    "            stack, buffer, arcs, score = state\n",
    "            # get all valid actions\n",
    "            actions = valid_actions(stack, buffer)\n",
    "            # compute actions scores for this state\n",
    "            if len(buffer) > 0:\n",
    "                oracle_state = [(stack[-2:] , buffer[0])]\n",
    "            else:\n",
    "                oracle_state = [(stack[-2:], None)]\n",
    "            state_idx = [dataset.tokenize_state(oracle_state, word_ids)]    \n",
    "            action_logits, label_logits = oracle.predict(bert_output, state_idx)\n",
    "            best_label = labels[torch.argmax(label_logits[0][0])]\n",
    "\n",
    "            if verbose: \n",
    "                print(f\"\\nStack: {stack}\")\n",
    "                print(f\"Buffer: {buffer}\")   \n",
    "                print(f\"Valid actions: {actions}\")\n",
    "\n",
    "            # expand the state using each valid action\n",
    "            for action in actions:\n",
    "                # get the score for this action\n",
    "                action_score = action_logits[0][0][dataset.action2idx[action]].item()\n",
    "                # apply the action to get the successor state\n",
    "                label = 'null' if action == 'SHIFT' else best_label\n",
    "                successor_state = generate_successor_state(state, action, action_score, label)\n",
    "                # add to beam\n",
    "                add_state_to_beam(beam_successors, successor_state, k)\n",
    "            \n",
    "        beam = beam_successors\n",
    "\n",
    "    if return_beam:\n",
    "        # first sort the beam states by score\n",
    "        beam.sort(key=lambda x: x[3], reverse=True)\n",
    "        # return sorted beam states\n",
    "        return beam\n",
    "    else:\n",
    "        # return the highest scoring state from the beam\n",
    "        best_state = get_best_state(beam)\n",
    "        return best_state[2]                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Beam state# 0, Score: 278.052568256855\n",
      "UAS: 0.918918918918919, LAS: 0.918918918918919\n",
      "\n",
      "Beam state# 1, Score: 277.1197240948677\n",
      "UAS: 0.918918918918919, LAS: 0.8918918918918919\n",
      "\n",
      "Beam state# 2, Score: 276.94162875413895\n",
      "UAS: 0.972972972972973, LAS: 0.918918918918919\n",
      "\n",
      "Beam state# 3, Score: 276.78090941905975\n",
      "UAS: 0.9459459459459459, LAS: 0.9459459459459459\n",
      "\n",
      "Beam state# 4, Score: 275.84806525707245\n",
      "UAS: 0.9459459459459459, LAS: 0.918918918918919\n",
      "\n",
      "Beam state# 5, Score: 275.6699699163437\n",
      "UAS: 1.0, LAS: 0.9459459459459459\n",
      "\n",
      "Beam state# 6, Score: 275.2449088692665\n",
      "UAS: 0.918918918918919, LAS: 0.918918918918919\n",
      "\n",
      "Beam state# 7, Score: 274.4522104859352\n",
      "UAS: 0.918918918918919, LAS: 0.8918918918918919\n",
      "\n",
      "Beam state# 8, Score: 273.5721064209938\n",
      "UAS: 0.8918918918918919, LAS: 0.8648648648648649\n",
      "\n",
      "Beam state# 9, Score: 273.18055164813995\n",
      "UAS: 0.9459459459459459, LAS: 0.918918918918919\n"
     ]
    }
   ],
   "source": [
    "# get a test sentence from the validation set and its gold standard parse\n",
    "test_data_instance = data_val[0]\n",
    "gold_states, gold_actions, gold_labels, sentence_words, gold_arcs = training_oracle(test_data_instance, return_states=True, max_iters=100000)\n",
    "\n",
    "# predict the parse using the oracle\n",
    "beam = beam_parser(sentence_words, model, val_dataset, k=10, return_beam=True)\n",
    "\n",
    "# compare the gold standard and predicted arcs for each state in the beam, from highest to lowest score\n",
    "for i, state in enumerate(beam):\n",
    "    print(f\"\\nBeam state# {i}, Score: {state[3]}\")\n",
    "    predicted_arcs = state[2]\n",
    "    uas, las = evaluate(gold_arcs, predicted_arcs)\n",
    "    print(f\"UAS: {uas}, LAS: {las}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the beam search parser on the entire validation set and compare the average unlabeled and labeled attachment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed:   0%|          | 0/1700 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences parsed: 100%|██████████| 1700/1700 [10:25<00:00,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average UAS: 0.936265873465607, Average LAS: 0.8945757274674179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the greedy parser on the validation set\n",
    "num_instances = 0\n",
    "uas_tot = 0\n",
    "las_tot = 0\n",
    "# give me a pbar\n",
    "pbar = tqdm(data_val, desc=\"Sentences parsed\")\n",
    "for data_instance in pbar:\n",
    "    gold_states, gold_actions, gold_labels, sentence_words, gold_arcs = training_oracle(data_instance, return_states=True, max_iters=100000)\n",
    "    if gold_states is None:\n",
    "        continue\n",
    "    else:\n",
    "        precicted_arcs = beam_parser(sentence_words, model, val_dataset, k=20, verbose=False)\n",
    "        uas, las = evaluate(gold_arcs, precicted_arcs)\n",
    "        uas_tot += uas\n",
    "        las_tot += las\n",
    "        num_instances += 1\n",
    "uas = uas_tot / num_instances\n",
    "las = las_tot / num_instances\n",
    "print(f\"Average UAS: {uas}, Average LAS: {las}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
