{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy (arc-standard) Transition Dependency Parser\n",
    "\n",
    "We will now use the trained neural oracle to perform (arc-standard) dependency parsing. Given a sentence, we initialize a buffer containing the words and punctuation symbols of the sentence, a stack containing the `ROOT` and an empty dependency relations list. Starting from this initial state, we perform parse steps by applying actions chosen by the oracle and updating the system state. When the terminal state is reached (i.e. the buffer is empty and the stack only contains the `ROOT`), the complete dependency parse is contained in the dependency relations list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from parse_utils import *\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load the validation set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the validation data: 1700\n"
     ]
    }
   ],
   "source": [
    "data_val = read_conllu(os.path.join('data', 'dev.conll'))\n",
    "print(f\"Number of sentences in the validation data: {len(data_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implement the greedy arc-standard transition parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(sentence_words, oracle):\n",
    "    tokens = [(word, i+1) for i,word in enumerate(sentence_words)]\n",
    "\n",
    "    # initialize the state\n",
    "    stack = [('ROOT', 0)]\n",
    "    buffer = tokens.copy()\n",
    "    arcs = []\n",
    "\n",
    "    # convert sentence to model input tensors\n",
    "    input_idx, input_attn_mask, word_ids = dataset.tokenize_sentence(sentence_words)\n",
    "    # set model to eval mode\n",
    "    oracle.eval()\n",
    "    # compute BERT encoding of sentence tokens\n",
    "    with torch.no_grad():\n",
    "        bert_output = oracle.get_bert_encoding(input_idx, input_attn_mask)\n",
    "\n",
    "    idx2label = {v:k for k,v in dataset.label2idx.items()}    \n",
    "\n",
    "    # begin parsing\n",
    "    while len(buffer) > 0 or len(stack) > 1:\n",
    "\n",
    "        if len(buffer) > 0:\n",
    "            state = [(stack[-2:] , buffer[0])]\n",
    "        else:\n",
    "            state = [(stack[-2:], None)]\n",
    "        state_idx = dataset.tokenize_state(state, word_ids)    \n",
    "\n",
    "        # get the oracle action and label scores\n",
    "        action_logits, label_logits = oracle.predict(bert_output, state_idx)\n",
    "        \n",
    "        # pick highest scoring action and label\n",
    "        best_action = torch.argmax(action_logits[0][0])\n",
    "        best_label = idx2label[dataset.torch.argmax(label_logits[0][0])]\n",
    "\n",
    "        # perform the action\n",
    "        if best_action == 0:\n",
    "            # LEFTARC\n",
    "            if len(stack) > 1:\n",
    "                if stack[-2][0] != 'ROOT':\n",
    "                    arcs.append((stack[-1], stack[-2], best_label))\n",
    "                    stack.pop(-2)\n",
    "                else:\n",
    "                    raise ValueError(\"Cannot perform LEFTARC action with ROOT as dependent. Parse failed.\")    \n",
    "            else:\n",
    "                raise ValueError(\"Cannot perform LEFTARC action with stack length <= 1. Parse failed.\")\n",
    "\n",
    "        elif best_action == 1:\n",
    "            # RIGHTARC\n",
    "            if len(stack) > 1:\n",
    "                arcs.append((stack[-2], stack[-1], best_label))\n",
    "                stack.pop(-1) \n",
    "            else:\n",
    "                raise ValueError(\"Cannot perform RIGHTARC action with stack length <= 1. Parse failed.\")         \n",
    "\n",
    "        else:\n",
    "            # SHIFT\n",
    "            if len(buffer) > 0:\n",
    "                stack.append(buffer.pop(0))\n",
    "            else:\n",
    "                raise ValueError(\"Cannot perform SHIFT action with buffer length <= 0. Parse failed.\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
