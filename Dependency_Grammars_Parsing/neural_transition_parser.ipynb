{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy (arc-standard) Transition Dependency Parser\n",
    "\n",
    "We will now use the trained neural oracle to perform (arc-standard) dependency parsing. Given a sentence, we initialize a buffer containing the words and punctuation symbols of the sentence, a stack containing the `ROOT` and an empty dependency relations list. Starting from this initial state, we perform parse steps by applying actions chosen by the oracle and updating the system state. When the terminal state is reached (i.e. the buffer is empty and the stack only contains the `ROOT`), the complete dependency parse is contained in the dependency relations list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from parse_utils import *\n",
    "import wandb\n",
    "import pickle \n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load the validation set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the validation data: 1700\n"
     ]
    }
   ],
   "source": [
    "data_val = read_conllu(os.path.join('data', 'dev.conll'))\n",
    "print(f\"Number of sentences in the validation data: {len(data_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pytorch dataset object from file\n",
    "with open('val_dataset_pytorch.pkl', 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "action2idx = val_dataset.action2idx\n",
    "label2idx = val_dataset.label2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the trained oracle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint!\n",
      "Total number of parameters in transformer network: 66.959019 M\n",
      "RAM used: 2707.07 MB\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "learning_rate = 1e-5\n",
    "model = BERT_ORACLE(num_actions=len(action2idx), num_labels=len(label2idx), unlabeled_arcs=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2752455174922943, 0.9773752207663278, 0.9538105168222937)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "validation(model, val_dataloader, device=DEVICE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implement the greedy arc-standard transition parser. This implementation is very strict and causes parser to fail whenever oracle predicts an invalid action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Greedy arc-standard transition parser\n",
    "\"\"\"\n",
    "def parser(sentence_words, oracle, dataset, verbose=False):\n",
    "    tokens = [(word, i+1) for i,word in enumerate(sentence_words)]\n",
    "\n",
    "    # initialize the state\n",
    "    stack = [('ROOT', 0)]\n",
    "    buffer = tokens.copy()\n",
    "    arcs = []\n",
    "\n",
    "    # convert sentence to model input tensors\n",
    "    input_idx, input_attn_mask, word_ids = dataset.tokenize_sentence(sentence_words)\n",
    "    input_idx = input_idx.unsqueeze(0).to(DEVICE)\n",
    "    input_attn_mask = input_attn_mask.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # set model to eval mode\n",
    "    oracle.eval()\n",
    "    # compute BERT encoding of sentence tokens\n",
    "    with torch.no_grad():\n",
    "        bert_output = oracle.get_bert_encoding(input_idx, input_attn_mask)\n",
    "\n",
    "    labels = list(dataset.label2idx.keys())\n",
    "    actions = list(dataset.action2idx.keys())\n",
    "\n",
    "    if verbose: \n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")   \n",
    "\n",
    "    # begin parsing\n",
    "    while len(buffer) > 0 or len(stack) > 1:\n",
    "\n",
    "        if len(buffer) > 0:\n",
    "            state = [(stack[-2:] , buffer[0])]\n",
    "        else:\n",
    "            state = [(stack[-2:], None)]\n",
    "        state_idx = [dataset.tokenize_state(state, word_ids)]    \n",
    "\n",
    "        # get the oracle action and label scores\n",
    "        action_logits, label_logits = oracle.predict(bert_output, state_idx)\n",
    "        \n",
    "        # pick highest scoring action and label\n",
    "        best_action = actions[torch.argmax(action_logits[0][0])]\n",
    "        best_label = labels[torch.argmax(label_logits[0][0])]\n",
    "\n",
    "        # perform the action\n",
    "        if best_action == 'LEFTARC':\n",
    "            # LEFTARC\n",
    "            if len(stack) > 1:\n",
    "                if stack[-2][0] != 'ROOT':\n",
    "                    arcs.append((stack[-1], stack[-2], best_label))\n",
    "                    stack.pop(-2)\n",
    "                else:\n",
    "                    raise ValueError(\"Cannot perform LEFTARC action with ROOT as dependent. Parse failed.\")    \n",
    "            else:\n",
    "                raise ValueError(\"Cannot perform LEFTARC action with stack length <= 1. Parse failed.\")\n",
    "\n",
    "        elif best_action == 'RIGHTARC':\n",
    "            # RIGHTARC\n",
    "            if len(stack) > 1:\n",
    "                arcs.append((stack[-2], stack[-1], best_label))\n",
    "                stack.pop(-1) \n",
    "            else:\n",
    "                raise ValueError(\"Cannot perform RIGHTARC action with stack length <= 1. Parse failed.\")         \n",
    "\n",
    "        else:\n",
    "            # SHIFT\n",
    "            if len(buffer) > 0:\n",
    "                stack.append(buffer.pop(0))\n",
    "            else:\n",
    "                raise ValueError(\"Cannot perform SHIFT action with buffer length <= 0. Parse failed.\")         \n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Best action: {best_action}, best label: {best_label}\")\n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")\n",
    "            print(f\"Arcs: {arcs}\")        \n",
    "\n",
    "    return arcs                       \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Evaluate the predicted arcs against the gold arcs by computing unlabeled and labeled attachment scores\n",
    "\"\"\"\n",
    "def evaluate(gold_arcs, predicted_arcs):\n",
    "    uas = 0\n",
    "    las = 0\n",
    "    gold_head_deps = [(r[0], r[1]) for r in gold_arcs]\n",
    "\n",
    "    for r in predicted_arcs:\n",
    "        if (r[0], r[1]) in gold_head_deps:\n",
    "            uas += 1\n",
    "            if r in gold_arcs:\n",
    "                las += 1\n",
    "\n",
    "    uas = uas / len(gold_arcs)\n",
    "    las = las / len(gold_arcs)            \n",
    "    return uas, las    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test sentence from the validation set and its gold standard parse\n",
    "test_data_instance = data_val[10]\n",
    "gold_states, gold_actions, gold_labels, sentence_words, gold_arcs  = training_oracle(test_data_instance, return_states=True, max_iters=100000)\n",
    "\n",
    "# predict the parse using the oracle\n",
    "precicted_arcs = parser(sentence_words, model, val_dataset, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAS: 1.0, LAS: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# compare the gold standard and predicted arcs\n",
    "uas, las = evaluate(gold_arcs, precicted_arcs)\n",
    "print(f\"UAS: {uas}, LAS: {las}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search\n",
    "\n",
    "To improve the accuracy of the parser, we can use beam search instead of greedily choosing the best possible action at each step. \n",
    "\n",
    "In beam search, we define a beam width $k$ and maintain a search tree and use breadth-first search. The root of the tree is designated to be the initial state. We then expand the root node by exploring all valid actions and generate the resulting states. We define a state score such that the score of the initial state is $0$ and the score of newly generated states is the score of the predecessor state plus the score of the action taken to generate the successor state:\n",
    "\n",
    "$StateScore(s_0) = 0$\n",
    "\n",
    "$StateScore(s_i) = StateScore(s_{i-1}) + Score(s_{i-1}, a)$\n",
    "\n",
    "Then we expand each of these successor states and prune the tree to keep only the  top-$k$ successor states with highest state scores. We continue expanding every node in the beam until it they have all reached a terminal state. Then the best parse is given by the terminal state with the highest state score.\n",
    "\n",
    "The beam dsearch parse algorithm is shown below (borrowed from Jurafsky-Martin textbook):\n",
    "\n",
    "<img src=\"beam_search.png\" width=\"600\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_contains_non_final_states(beam):\n",
    "    for state in beam:\n",
    "        stack, buffer, arcs, score = state\n",
    "        if len(buffer) > 0 or len(stack) > 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def add_state_to_beam(beam, state, k):\n",
    "    if len(beam) < k:\n",
    "        beam.append(state)\n",
    "    else:\n",
    "        # replace the state with the lowest score\n",
    "        min_score = min([s[3] for s in beam])\n",
    "        if state[3] > min_score:\n",
    "            min_idx = [s[3] for s in beam].index(min_score)\n",
    "            beam[min_idx] = state\n",
    "    return beam\n",
    "\n",
    "\n",
    "def valid_actions(stack, buffer):\n",
    "    actions = []\n",
    "    if len(buffer) > 0:\n",
    "        actions.append('SHIFT')\n",
    "    if len(stack) > 1:\n",
    "        actions.append('LEFTARC')\n",
    "        actions.append('RIGHTARC')\n",
    "    return actions\n",
    "\n",
    "\n",
    "def generate_successor_state(state, action, action_score, best_label):\n",
    "    stack, buffer, arcs, score = state\n",
    "    # perform the action\n",
    "    if action == 'LEFTARC':\n",
    "        arcs.append((stack[-1], stack[-2], best_label))\n",
    "        stack.pop(-2)            \n",
    "    elif action == 'RIGHTARC':\n",
    "        arcs.append((stack[-2], stack[-1], best_label))\n",
    "        stack.pop(-1) \n",
    "    else:\n",
    "        stack.append(buffer.pop(0))\n",
    "\n",
    "    return (stack, buffer, arcs, score + action_score)    \n",
    "\n",
    "\n",
    "def get_best_state(beam):\n",
    "    best_score = -np.inf\n",
    "    best_state = None\n",
    "    for state in beam:\n",
    "        if state[3] > best_score:\n",
    "            best_score = state[3]\n",
    "            best_state = state\n",
    "    return best_state\n",
    "\n",
    "\n",
    "def beam_parser(sentence_words, oracle, dataset, k=10, verbose=False):\n",
    "    tokens = [(word, i+1) for i,word in enumerate(sentence_words)]\n",
    "\n",
    "    # initialize the state\n",
    "    stack = [('ROOT', 0)]\n",
    "    buffer = tokens.copy()\n",
    "    arcs = []\n",
    "    score = 0.0\n",
    "    state = [(stack, buffer, arcs, score)]\n",
    "    # initialize the beam\n",
    "    beam = [state]\n",
    "\n",
    "    # convert sentence to model input tensors\n",
    "    input_idx, input_attn_mask, word_ids = dataset.tokenize_sentence(sentence_words)\n",
    "    input_idx = input_idx.unsqueeze(0).to(DEVICE)\n",
    "    input_attn_mask = input_attn_mask.unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    # set model to eval mode\n",
    "    oracle.eval()\n",
    "    # compute BERT encoding of sentence tokens\n",
    "    with torch.no_grad():\n",
    "        bert_output = oracle.get_bert_encoding(input_idx, input_attn_mask)\n",
    "\n",
    "    labels = list(dataset.label2idx.keys())\n",
    "    actions = list(dataset.action2idx.keys())\n",
    "\n",
    "    if verbose: \n",
    "            print(f\"\\nStack: {stack}\")\n",
    "            print(f\"Buffer: {buffer}\")   \n",
    "\n",
    "    # begin beam search\n",
    "    while beam_contains_non_final_states(beam):\n",
    "        beam_successors = []\n",
    "        for state in beam:\n",
    "            stack, buffer, arcs, score = state\n",
    "            # get all valid actions\n",
    "            actions = valid_actions(stack, buffer)\n",
    "            # compute actions scores for this state\n",
    "            if len(buffer) > 0:\n",
    "                oracle_state = [(stack[-2:] , buffer[0])]\n",
    "            else:\n",
    "                oracle_state = [(stack[-2:], None)]\n",
    "            state_idx = [dataset.tokenize_state(oracle_state, word_ids)]    \n",
    "            action_logits, label_logits = oracle.predict(bert_output, state_idx)\n",
    "            best_label = labels[torch.argmax(label_logits[0][0])]\n",
    "\n",
    "            # expand the state using each valid action\n",
    "            for action in actions:\n",
    "                # get the score for this action\n",
    "                action_score = action_logits[0][0][dataset.action2idx[action]].item()\n",
    "                # apply the action to get the successor state\n",
    "                successor_state = generate_successor_state(state, action, action_score, best_label)\n",
    "                # add to beam\n",
    "                add_state_to_beam(beam_successors, successor_state, k)\n",
    "            \n",
    "        beam = beam_successors\n",
    "\n",
    "        # get the best state from the beam\n",
    "        best_state = get_best_state(beam)\n",
    "\n",
    "    return best_state[2]                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
