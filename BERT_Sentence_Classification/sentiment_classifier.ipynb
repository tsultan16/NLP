{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning a BERT Model for Sentiment Classification\n",
    "\n",
    "We will take the original `BERT` model trained on the masked language modeling task and `finetune` it for `sentiment classification` on the Stanford Sentiment Tree and CFIMDB datasets. The `pretrained` BERT model takes in an input sequence of integer tokens and outputs a corresponding sequence of contextualized encoded vectors (768 dimensional). A special `[CLS]` token is appended at the start of the input sequence and the coressponding encoded output vector of this token represents a `pooled representation` of the entire sequence. This pooled representation vector can then be used by a feedforward network to perform a sentence classification task. All parameters in this combined model (consisting of the BERT and the feedforward classifier) can be trained together to optimize the model for the sentence classification task, this process is called `finetuning`, because it involves adapting the pretrained parameters of BERT for this specialized task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the WordPiece tokenizer and the pre-trained BERT provided by the Hugginface transformers library. First, lets try out the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  Yay, I'm excited to try out this BERT model from Huggingface!\n",
      "Subword tokens:  ['ya', '##y', ',', 'i', \"'\", 'm', 'excited', 'to', 'try', 'out', 'this', 'bert', 'model', 'from', 'hugging', '##face', '!']\n",
      "Encoded sentence:  [101, 8038, 2100, 1010, 1045, 1005, 1049, 7568, 2000, 3046, 2041, 2023, 14324, 2944, 2013, 17662, 12172, 999, 102]\n",
      "Idx back to tokens:  ['[CLS]', 'ya', '##y', ',', 'i', \"'\", 'm', 'excited', 'to', 'try', 'out', 'this', 'bert', 'model', 'from', 'hugging', '##face', '!', '[SEP]']\n",
      "Decoded sentence:  [CLS] yay, i'm excited to try out this bert model from huggingface! [SEP]\n",
      "\n",
      "Special tokens with their integer id:\n",
      "[UNK]  <-->  100\n",
      "[SEP]  <-->  102\n",
      "[PAD]  <-->  0\n",
      "[CLS]  <-->  101\n",
      "[MASK]  <-->  103\n"
     ]
    }
   ],
   "source": [
    "# load the prettrained WordPiece tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# use it on a test sentence\n",
    "sentence = \"Yay, I'm excited to try out this BERT model from Huggingface!\"\n",
    "tokens_subwords = tokenizer.tokenize(sentence)\n",
    "tokens_idx = tokenizer.encode(sentence)\n",
    "idx_to_tokens = tokenizer.convert_ids_to_tokens(tokens_idx)\n",
    "decoded_sentence = tokenizer.decode(tokens_idx)\n",
    "print(\"Original sentence: \", sentence)\n",
    "print(\"Subword tokens: \", tokens_subwords)\n",
    "print(\"Encoded sentence: \", tokens_idx)\n",
    "print(\"Idx back to tokens: \", idx_to_tokens)\n",
    "print(\"Decoded sentence: \", decoded_sentence)\n",
    "\n",
    "# let's also take a look at all the special tokens\n",
    "print(\"\\nSpecial tokens with their integer id:\")\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for t in special_tokens:\n",
    "    print(t,\" <--> \" ,tokenizer.convert_tokens_to_ids(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that since we're using the `uncased` version of the tokenizer, everything becomes lowercase.\n",
    "\n",
    "Now let's load the SST dataset from file and package it inside a pytroch dataset class. Each data instance is a sentence-sentiment value pair, there are 5 different sentiment labels: NEGATIVE (0), SOMEWHAT NEGATIVE (1), NEUTRAL (2), SOMEWHAT POSITIVE (3), POSITIVE (4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sst(split=\"train\"):\n",
    "    if split == \"test\":\n",
    "        filename = \"data/ids-sst-test-student.csv\"    \n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for record in csv.DictReader(f, delimiter='\\t'):\n",
    "                sent = record['sentence'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                data.append((sent,sent_id))\n",
    "        return data          \n",
    "    else:\n",
    "        if split == \"train\":\n",
    "            filename = \"data/ids-sst-train.csv\"\n",
    "        elif split== \"dev\":\n",
    "            filename = \"data/ids-sst-dev.csv\"   \n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for record in csv.DictReader(f, delimiter='\\t'):\n",
    "                sent = record['sentence'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                label = int(record['sentiment'].strip())\n",
    "                data.append((sent,label,sent_id))\n",
    "                labels.append(label)\n",
    "        label_distribution = Counter(labels)        \n",
    "        return data, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8544\n",
      "Train Label distribution: Counter({3: 2322, 1: 2218, 2: 1624, 4: 1288, 0: 1092})\n",
      "Number of dev examples: 1101\n",
      "Dev Label distribution: Counter({1: 289, 3: 279, 2: 229, 4: 165, 0: 139})\n"
     ]
    }
   ],
   "source": [
    "sst_train, train_label_distribution = load_data_sst(split=\"train\")\n",
    "sst_dev, dev_label_distribution = load_data_sst(split=\"dev\")\n",
    "\n",
    "print(f\"Number of training examples: {len(sst_train)}\")\n",
    "print(f\"Train Label distribution: {train_label_distribution}\")\n",
    "print(f\"Number of dev examples: {len(sst_dev)}\")\n",
    "print(f\"Dev Label distribution: {dev_label_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, data, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    # collate function for padding the sentences to the same length and creating attention masks\n",
    "    def collate_fn(self, batch):\n",
    "        sents = [x[0] for x in batch]\n",
    "        labels = [x[1] for x in batch]\n",
    "        encoded = self.tokenizer.batch_encode_plus(sents, add_special_tokens=True, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        input_idx = encoded['input_ids']\n",
    "        attn_mask = encoded['attention_mask']   \n",
    "        #token_type_idx = encoded['token_type_ids'] # don't need this since we only have one sentence\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_idx, labels, attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we define our sentiment classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_classes=5, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head\n",
    "        self.classifier_head = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, labels, attn_mask):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask)\n",
    "        # extract the `[CLS]` encoding (first element of the sequence)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        cls_encoding = bert_output[:, 0] # shape: (batch_size, hidden_size)\n",
    "        # apply dropout \n",
    "        cls_encoding = self.dropout(cls_encoding) \n",
    "        # compute classifier logits\n",
    "        logits = self.classifier_head(cls_encoding)  # shape: (batch_size, num_classes)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets, attn_mask = batch\n",
    "            # move batch to device\n",
    "            inputs, targets, attn_mask = inputs.to(device), targets.to(device), attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets, attn_mask)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += y_pred.eq(targets.view(-1)).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets, attn_mask = batch\n",
    "            inputs, targets, attn_mask = inputs.to(device), targets.to(device), attn_mask.to(device)\n",
    "            logits, loss = model(inputs, targets, attn_mask)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += y_pred.eq(targets.view(-1)).sum().item()            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'sentiment_classifier_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('sentiment_classifier_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's train a model without finetuning the BERT weights, i.e. we keep the BERT weights frozen and only learn the weights for the classifier head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.486085 M\n",
      "RAM used: 972.72 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "max_length = 128\n",
    "learning_rate = 5e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataset = SSTDataset(sst_train, max_length=max_length)\n",
    "val_dataset = SSTDataset(sst_dev, max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTSentimentClassifier(finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/BERT/wandb/run-20240114_154504-rzwgs2o7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/rzwgs2o7' target=\"_blank\">lucky-monkey-1</a></strong> to <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/rzwgs2o7' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/rzwgs2o7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"BERT_Sentiment_Classification\", \n",
    "    config={\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 30,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"Stanford Sentiment Tree\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 1.372, Train Accuracy:  0.364, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 267/267 [00:30<00:00,  8.75it/s]\n",
      "Epoch 2, EMA Train Loss: 1.327, Train Accuracy:  0.436, Val Loss:  1.378, Val Accuracy:  0.417: 100%|██████████| 267/267 [00:29<00:00,  9.04it/s]\n",
      "Epoch 3, EMA Train Loss: 1.268, Train Accuracy:  0.449, Val Loss:  1.282, Val Accuracy:  0.436: 100%|██████████| 267/267 [00:29<00:00,  9.00it/s]\n",
      "Epoch 4, EMA Train Loss: 1.272, Train Accuracy:  0.456, Val Loss:  1.273, Val Accuracy:  0.431: 100%|██████████| 267/267 [00:30<00:00,  8.73it/s]\n",
      "Epoch 5, EMA Train Loss: 1.277, Train Accuracy:  0.464, Val Loss:  1.246, Val Accuracy:  0.452: 100%|██████████| 267/267 [00:30<00:00,  8.90it/s]\n",
      "Epoch 6, EMA Train Loss: 1.254, Train Accuracy:  0.472, Val Loss:  1.232, Val Accuracy:  0.473: 100%|██████████| 267/267 [00:29<00:00,  8.94it/s]\n",
      "Epoch 7, EMA Train Loss: 1.209, Train Accuracy:  0.480, Val Loss:  1.252, Val Accuracy:  0.440: 100%|██████████| 267/267 [00:29<00:00,  8.95it/s]\n",
      "Epoch 8, EMA Train Loss: 1.229, Train Accuracy:  0.483, Val Loss:  1.214, Val Accuracy:  0.466: 100%|██████████| 267/267 [00:29<00:00,  8.95it/s]\n",
      "Epoch 9, EMA Train Loss: 1.203, Train Accuracy:  0.479, Val Loss:  1.217, Val Accuracy:  0.458: 100%|██████████| 267/267 [00:29<00:00,  8.95it/s]\n",
      "Epoch 10, EMA Train Loss: 1.194, Train Accuracy:  0.480, Val Loss:  1.217, Val Accuracy:  0.449: 100%|██████████| 267/267 [00:29<00:00,  8.97it/s]\n",
      "Epoch 11, EMA Train Loss: 1.177, Train Accuracy:  0.480, Val Loss:  1.237, Val Accuracy:  0.460: 100%|██████████| 267/267 [00:29<00:00,  8.96it/s]\n",
      "Epoch 12, EMA Train Loss: 1.204, Train Accuracy:  0.482, Val Loss:  1.233, Val Accuracy:  0.453: 100%|██████████| 267/267 [00:29<00:00,  8.96it/s]\n",
      "Epoch 13, EMA Train Loss: 1.220, Train Accuracy:  0.480, Val Loss:  1.219, Val Accuracy:  0.471: 100%|██████████| 267/267 [00:29<00:00,  8.96it/s]\n",
      "Epoch 14, EMA Train Loss: 1.235, Train Accuracy:  0.481, Val Loss:  1.210, Val Accuracy:  0.471: 100%|██████████| 267/267 [00:29<00:00,  8.94it/s]\n",
      "Epoch 15, EMA Train Loss: 1.153, Train Accuracy:  0.490, Val Loss:  1.213, Val Accuracy:  0.462: 100%|██████████| 267/267 [00:29<00:00,  8.93it/s]\n",
      "Epoch 16, EMA Train Loss: 1.186, Train Accuracy:  0.485, Val Loss:  1.214, Val Accuracy:  0.467: 100%|██████████| 267/267 [00:30<00:00,  8.79it/s]\n",
      "Epoch 17, EMA Train Loss: 1.189, Train Accuracy:  0.492, Val Loss:  1.209, Val Accuracy:  0.459: 100%|██████████| 267/267 [00:29<00:00,  8.90it/s]\n",
      "Epoch 18, EMA Train Loss: 1.168, Train Accuracy:  0.490, Val Loss:  1.216, Val Accuracy:  0.469: 100%|██████████| 267/267 [00:29<00:00,  8.92it/s]\n",
      "Epoch 19, EMA Train Loss: 1.148, Train Accuracy:  0.495, Val Loss:  1.211, Val Accuracy:  0.480: 100%|██████████| 267/267 [00:29<00:00,  8.91it/s]\n",
      "Epoch 20, EMA Train Loss: 1.201, Train Accuracy:  0.491, Val Loss:  1.207, Val Accuracy:  0.455: 100%|██████████| 267/267 [00:30<00:00,  8.73it/s]\n",
      "Epoch 21, EMA Train Loss: 1.143, Train Accuracy:  0.488, Val Loss:  1.228, Val Accuracy:  0.460: 100%|██████████| 267/267 [00:29<00:00,  8.91it/s]\n",
      "Epoch 22, EMA Train Loss: 1.166, Train Accuracy:  0.491, Val Loss:  1.212, Val Accuracy:  0.459: 100%|██████████| 267/267 [00:29<00:00,  8.92it/s]\n",
      "Epoch 23, EMA Train Loss: 1.179, Train Accuracy:  0.489, Val Loss:  1.233, Val Accuracy:  0.454: 100%|██████████| 267/267 [00:30<00:00,  8.75it/s]\n",
      "Epoch 24, EMA Train Loss: 1.191, Train Accuracy:  0.497, Val Loss:  1.225, Val Accuracy:  0.462: 100%|██████████| 267/267 [00:29<00:00,  8.94it/s]\n",
      "Epoch 25, EMA Train Loss: 1.188, Train Accuracy:  0.489, Val Loss:  1.211, Val Accuracy:  0.456: 100%|██████████| 267/267 [00:30<00:00,  8.82it/s]\n",
      "Epoch 26, EMA Train Loss: 1.132, Train Accuracy:  0.486, Val Loss:  1.211, Val Accuracy:  0.457: 100%|██████████| 267/267 [00:29<00:00,  8.93it/s]\n",
      "Epoch 27, EMA Train Loss: 1.203, Train Accuracy:  0.496, Val Loss:  1.235, Val Accuracy:  0.475: 100%|██████████| 267/267 [00:29<00:00,  8.93it/s]\n",
      "Epoch 28, EMA Train Loss: 1.171, Train Accuracy:  0.489, Val Loss:  1.215, Val Accuracy:  0.450: 100%|██████████| 267/267 [00:29<00:00,  8.92it/s]\n",
      "Epoch 29, EMA Train Loss: 1.202, Train Accuracy:  0.484, Val Loss:  1.216, Val Accuracy:  0.478: 100%|██████████| 267/267 [00:29<00:00,  8.93it/s]\n",
      "Epoch 30, EMA Train Loss: 1.232, Train Accuracy:  0.497, Val Loss:  1.211, Val Accuracy:  0.449: 100%|██████████| 267/267 [00:29<00:00,  8.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=30, save_every=50, val_every=1, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a0a60a48654eb2aeb727372f7777e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The validation accuracy reaches about 45% with no finetuning. Now let's train the model with finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 1530.46 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "max_length = 128\n",
    "learning_rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataset = SSTDataset(sst_train, max_length=max_length)\n",
    "val_dataset = SSTDataset(sst_dev, max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTSentimentClassifier(finetune=True).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/BERT/wandb/run-20240114_171541-o6mg4u9v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/o6mg4u9v' target=\"_blank\">divine-wind-5</a></strong> to <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/o6mg4u9v' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/o6mg4u9v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"BERT_Sentiment_Classification\", \n",
    "    config={\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"Stanford Sentiment Tree\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/267 [00:00<?, ?it/s]../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "../aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "Epochs:   0%|          | 0/267 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, scheduler, device, num_epochs, val_every, save_every, log_metrics)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# optimizer step\u001b[39;00m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39m avg_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m B, _ \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# shape (B,)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=10, save_every=50, val_every=1, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With finetuning, the validation accuracy has increased to just over 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's try the CFIMDB dataset conraining movie reviews which have two labels: positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_cfimdb(split=\"train\"):\n",
    "    if split == \"train\":\n",
    "        filename = \"data/ids-cfimdb-train.csv\"\n",
    "    elif split== \"dev\":\n",
    "        filename = \"data/ids-cfimdb-dev.csv\"   \n",
    "    data = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for record in csv.DictReader(f, delimiter='\\t'):\n",
    "            sent = record['sentence'].lower().strip()\n",
    "            sent_id = record['id'].lower().strip()\n",
    "            label = int(record['sentiment'].strip())\n",
    "            data.append((sent,label,sent_id))\n",
    "            labels.append(label)\n",
    "    label_distribution = Counter(labels)        \n",
    "    return data, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1707\n",
      "Train Label distribution: Counter({0: 856, 1: 851})\n",
      "Number of dev examples: 245\n",
      "Dev Label distribution: Counter({0: 123, 1: 122})\n"
     ]
    }
   ],
   "source": [
    "cfimdb_train, train_label_distribution = load_data_cfimdb(split=\"train\")\n",
    "cfimdb_dev, dev_label_distribution = load_data_cfimdb(split=\"dev\")\n",
    "\n",
    "print(f\"Number of training examples: {len(cfimdb_train)}\")\n",
    "print(f\"Train Label distribution: {train_label_distribution}\")\n",
    "print(f\"Number of dev examples: {len(cfimdb_dev)}\")\n",
    "print(f\"Dev Label distribution: {dev_label_distribution}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's train the model, without and with finetuning the BERT base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 1429.49 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "max_length = 128\n",
    "learning_rate = 5e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataset = SSTDataset(cfimdb_train, max_length=max_length)\n",
    "val_dataset = SSTDataset(cfimdb_dev, max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTSentimentClassifier(num_classes=2, finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 0.653, Train Accuracy:  0.568, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 54/54 [00:06<00:00,  8.05it/s]\n",
      "Epoch 2, EMA Train Loss: 0.607, Train Accuracy:  0.680, Val Loss:  0.628, Val Accuracy:  0.710: 100%|██████████| 54/54 [00:05<00:00,  9.04it/s]\n",
      "Epoch 3, EMA Train Loss: 0.577, Train Accuracy:  0.719, Val Loss:  0.591, Val Accuracy:  0.743: 100%|██████████| 54/54 [00:05<00:00,  9.04it/s]\n",
      "Epoch 4, EMA Train Loss: 0.579, Train Accuracy:  0.737, Val Loss:  0.564, Val Accuracy:  0.747: 100%|██████████| 54/54 [00:06<00:00,  8.47it/s]\n",
      "Epoch 5, EMA Train Loss: 0.533, Train Accuracy:  0.757, Val Loss:  0.540, Val Accuracy:  0.759: 100%|██████████| 54/54 [00:05<00:00,  9.00it/s]\n",
      "Epoch 6, EMA Train Loss: 0.545, Train Accuracy:  0.772, Val Loss:  0.527, Val Accuracy:  0.739: 100%|██████████| 54/54 [00:06<00:00,  8.95it/s]\n",
      "Epoch 7, EMA Train Loss: 0.498, Train Accuracy:  0.753, Val Loss:  0.515, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.98it/s]\n",
      "Epoch 8, EMA Train Loss: 0.519, Train Accuracy:  0.784, Val Loss:  0.503, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.99it/s]\n",
      "Epoch 9, EMA Train Loss: 0.499, Train Accuracy:  0.791, Val Loss:  0.499, Val Accuracy:  0.780: 100%|██████████| 54/54 [00:06<00:00,  8.95it/s]\n",
      "Epoch 10, EMA Train Loss: 0.492, Train Accuracy:  0.802, Val Loss:  0.483, Val Accuracy:  0.767: 100%|██████████| 54/54 [00:06<00:00,  8.96it/s]\n",
      "Epoch 11, EMA Train Loss: 0.476, Train Accuracy:  0.789, Val Loss:  0.485, Val Accuracy:  0.771: 100%|██████████| 54/54 [00:06<00:00,  8.94it/s]\n",
      "Epoch 12, EMA Train Loss: 0.469, Train Accuracy:  0.800, Val Loss:  0.480, Val Accuracy:  0.767: 100%|██████████| 54/54 [00:06<00:00,  8.93it/s]\n",
      "Epoch 13, EMA Train Loss: 0.435, Train Accuracy:  0.793, Val Loss:  0.481, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.88it/s]\n",
      "Epoch 14, EMA Train Loss: 0.456, Train Accuracy:  0.801, Val Loss:  0.470, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.94it/s]\n",
      "Epoch 15, EMA Train Loss: 0.466, Train Accuracy:  0.798, Val Loss:  0.461, Val Accuracy:  0.771: 100%|██████████| 54/54 [00:06<00:00,  8.94it/s]\n",
      "Epoch 16, EMA Train Loss: 0.452, Train Accuracy:  0.797, Val Loss:  0.465, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.92it/s]\n",
      "Epoch 17, EMA Train Loss: 0.439, Train Accuracy:  0.809, Val Loss:  0.461, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.93it/s]\n",
      "Epoch 18, EMA Train Loss: 0.417, Train Accuracy:  0.798, Val Loss:  0.461, Val Accuracy:  0.771: 100%|██████████| 54/54 [00:06<00:00,  8.79it/s]\n",
      "Epoch 19, EMA Train Loss: 0.428, Train Accuracy:  0.815, Val Loss:  0.449, Val Accuracy:  0.776: 100%|██████████| 54/54 [00:06<00:00,  8.55it/s]\n",
      "Epoch 20, EMA Train Loss: 0.441, Train Accuracy:  0.806, Val Loss:  0.454, Val Accuracy:  0.788: 100%|██████████| 54/54 [00:06<00:00,  8.92it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=20, save_every=50, val_every=1, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4d95e85e4a4a268d4b07b2f0ae1f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.3456544403374088, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch loss</td><td>▇▇▇█▆▅▆▅▅▄▄▃▆▃▄▄▃▃▄▃▂▆▃▃▂▃▂▄▃▂▃▁▅▁▃▃▂▄▂▄</td></tr><tr><td>Moving Avg Loss</td><td>▆█▇▇▆▆▅▅▅▄▄▄▅▄▃▃▃▃▃▂▃▃▃▂▂▃▂▃▂▂▁▁▂▁▂▁▂▁▂▂</td></tr><tr><td>Val Loss</td><td>▁▁████▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch loss</td><td>0.41897</td></tr><tr><td>Moving Avg Loss</td><td>0.44104</td></tr><tr><td>Val Loss</td><td>0.45394</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-wind-5</strong> at: <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/o6mg4u9v' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/o6mg4u9v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240114_171541-o6mg4u9v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The validation accuracy reaches close to 79%. Now let's try finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 1064.91 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "max_length = 128\n",
    "learning_rate = 4e-6\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataset = SSTDataset(cfimdb_train, max_length=max_length)\n",
    "val_dataset = SSTDataset(cfimdb_dev, max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTSentimentClassifier(num_classes=2, finetune=True, dropout_rate=0.2).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/BERT/wandb/run-20240114_171832-3kkxb9s9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/3kkxb9s9' target=\"_blank\">sage-wood-6</a></strong> to <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/3kkxb9s9' target=\"_blank\">https://wandb.ai/tanzids/BERT_Sentiment_Classification/runs/3kkxb9s9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"BERT_Sentiment_Classification\", \n",
    "    config={\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 10,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"Stanford Sentiment Tree\"},)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 0.023, Train Accuracy:  0.995, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 54/54 [00:18<00:00,  2.90it/s]\n",
      "Epoch 2, EMA Train Loss: 0.023, Train Accuracy:  0.995, Val Loss:  0.376, Val Accuracy:  0.906: 100%|██████████| 54/54 [00:18<00:00,  2.91it/s]\n",
      "Epoch 3, EMA Train Loss: 0.013, Train Accuracy:  0.996, Val Loss:  0.363, Val Accuracy:  0.918: 100%|██████████| 54/54 [00:18<00:00,  2.87it/s]\n",
      "Epoch 4, EMA Train Loss: 0.010, Train Accuracy:  0.997, Val Loss:  0.400, Val Accuracy:  0.914: 100%|██████████| 54/54 [00:18<00:00,  2.92it/s]\n",
      "Epoch 5, EMA Train Loss: 0.006, Train Accuracy:  0.998, Val Loss:  0.376, Val Accuracy:  0.918: 100%|██████████| 54/54 [00:18<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=5, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With finetuning, the validation accuracy has reached around 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
