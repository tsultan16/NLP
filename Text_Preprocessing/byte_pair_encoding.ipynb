{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding\n",
    "\n",
    "This is an algorithm for subword tokenization. It has two parts, a `token learner` and a `token segmenter`. The learner generates a vocabulary of subword type and the segmenter converts a test sentence into a sequence of subword tokens using the learned vocabulary.\n",
    "\n",
    "To learn the vocabulary, we perform the following steps:\n",
    "\n",
    "1) Given the input corpus of text (which is a single string), split across all whitespaces to get a set of strings. Then append a special end-of-word character \"_\" to each string. We define each of these strings to be a \"word\".\n",
    "2) Count the frequency of all the unique words. Split each word into a list of its individual characters, which are the tokens.\n",
    "3) Initialize the vocabulary as the set of all unique characters across these words, including the \"_\" character.\n",
    "4) Find the pair of adjacent tokens that appear most frequently (in case of ties, just break in some consistent way). Merge the pair of tokens into a single new token, add this new token to the vocabulary and replace all occuracnces of this pair across all words with this new token.\n",
    "5) Repeat step 4 until the vocabulary reaches some specified size.   \n",
    "\n",
    "In addition to the learned vocabulary of all merged tokens, we also keep track of all the pairs that have been merged. Then we can segment a test corpus by first splitting it into individual characters (replacing all whitespaces with the \"_\" end of word character). Then we replace all occurances of adjacent tokens from the first merged pair with the merged pair token. Then we repeat this for the second pair and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Define some helper functions that will be used by the token learner\n",
    "\"\"\"\n",
    "\n",
    "# splits corpus into words, returns a dictionary of word frequencies\n",
    "def init_vocab_words(corpus, eow_token=\"_\"):\n",
    "    vocab = list(set(list(\"\".join(corpus.split())))) + [eow_token]\n",
    "    # split corpus across whitespaces and make all characters lower case\n",
    "    words = corpus.lower().strip().split()\n",
    "    # get word frequencies\n",
    "    word_freq = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        # store the word key as a string containing tokens separated by whitespace\n",
    "        word_key = \" \".join(list(word+eow_token))\n",
    "        word_freq[word_key] += 1\n",
    "\n",
    "    return vocab, word_freq\n",
    "\n",
    "\n",
    "# find all unique pairs of adjacent tokens and their counts\n",
    "def get_pairs(word_freq):\n",
    "    pairs = collections.Counter()\n",
    "    for word, freq in word_freq.items():\n",
    "        tokens = word.split()\n",
    "        for pair in zip(tokens[:-1], tokens[1:]):\n",
    "            pairs[pair] += freq \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# merge a pair of tokens (token learner)\n",
    "def merge_pair_learner(pair, word_freq):\n",
    "    word_freq_new = collections.defaultdict(int)\n",
    "    pattern = \" \".join(pair)\n",
    "    pattern_joined = \"\".join(pair) \n",
    "    if not pair[1].endswith(\"_\"):\n",
    "        pattern = pattern + \" \"\n",
    "        pattern_joined = pattern_joined + \" \"\n",
    "\n",
    "    for word, freq in word_freq.items():\n",
    "        # merge all occurances of the pair in every word\n",
    "        word_new = re.sub(pattern, pattern_joined, word)\n",
    "        word_freq_new[word_new] = freq\n",
    "    return word_freq_new\n",
    "\n",
    "\"\"\"\n",
    "    BPE token learner \n",
    "\"\"\"\n",
    "def bpe_learn(corpus, k=100):\n",
    "    \n",
    "    # get inital vocab, word and token pair frequencies\n",
    "    vocab, word_freq = init_vocab_words(corpus)\n",
    "    #print(f\"word_freq: {word_freq}\")\n",
    "\n",
    "    # performs mergers to learn the vocabulary\n",
    "    merged_pairs = []\n",
    "    for _ in range(k):\n",
    "        # get counts of all adjacent token pairs\n",
    "        pairs = get_pairs(word_freq)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # get most frequent pair\n",
    "        most_freq_pair = pairs.most_common(1)[0][0]\n",
    "        #print(f\"best pair: {most_freq_pair}\")\n",
    "\n",
    "        # apply merger\n",
    "        word_freq = merge_pair_learner(most_freq_pair, word_freq)\n",
    "        #print(f\"word_freq after merge: {word_freq}\")\n",
    "\n",
    "        # add merged token to vocab\n",
    "        vocab.append(\"\".join(most_freq_pair))\n",
    "        merged_pairs.append(most_freq_pair)\n",
    "\n",
    "    # precompute tokenized words\n",
    "    word_tokens = collections.defaultdict(list)\n",
    "    corpus_words = corpus.lower().strip().split()\n",
    "    for word in corpus_words:\n",
    "        word_tokens[word] = tokenize_word(word, vocab, merged_pairs)   \n",
    "\n",
    "    #print(f\"merged pairs: {merged_pairs}\")    \n",
    "    #print(f\"final vocab: {vocab}\")    \n",
    "    #print(f\"precomputed tokenizations: {word_tokens}\")\n",
    "\n",
    "    return vocab, merged_pairs, word_tokens    \n",
    "    \n",
    "\n",
    "# merge a pair of tokens (token segmenter)\n",
    "def merge_pair_segmenter(pair, corpus_tokens):\n",
    "    pattern = \" \".join(pair)\n",
    "    pattern_joined = \"\".join(pair) \n",
    "    if not pair[1].endswith(\"_\"):\n",
    "        pattern = pattern + \" \"\n",
    "        pattern_joined = pattern_joined + \" \"\n",
    "    # merge all occurances of the pair in every word\n",
    "    corpus_tokens_new = re.sub(pattern, pattern_joined, corpus_tokens)\n",
    "    return corpus_tokens_new\n",
    "\n",
    "\n",
    "# encode a word into a list of BPE tokens\n",
    "def tokenize_word(word, vocab, pairs):\n",
    "    # split word into individual characters separated by white spaces, also insert eow token\n",
    "    word_tokens = \" \".join(list(word)) + \" _\"\n",
    "    # now replace all occurances of pairs with the merged pair\n",
    "    for pair in pairs:\n",
    "        word_tokens =  merge_pair_segmenter(pair, word_tokens)\n",
    "        if len(word_tokens.split()) == 1:\n",
    "            break\n",
    "    \n",
    "    word_tokens = word_tokens.split()\n",
    "    return word_tokens\n",
    "\n",
    "\"\"\" \n",
    "    BPE token segmenter\n",
    "\"\"\"    \n",
    "def bpe_segment(test_corpus, vocab, pairs, precomputed_word_tokens):\n",
    "    # split corpus into words\n",
    "    corpus_words = test_corpus.split()\n",
    "    # now replace all occurances of pairs with the merged pair\n",
    "    #print(f\"Corpus words: \\n{corpus_words}\")\n",
    "    corpus_tokens = []\n",
    "    for word in corpus_words:\n",
    "        if word in precomputed_word_tokens:\n",
    "            word_tokens = precomputed_word_tokens[word]\n",
    "        else:    \n",
    "            word_tokens = tokenize_word(word, vocab, pairs)\n",
    "        #print(f\"{word} ---> {word_tokens}\")\n",
    "        corpus_tokens.extend(word_tokens)    \n",
    "    return corpus_tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our implemntation on a small example corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new \n",
      "['Y', 'e', 's', ',', '_', 't', 'h', 'e', 'y', '_', 's', 'a', 'i', 'd', '_', 'w', 'e', '_', 'ne', 'e', 'd', '_', 't', 'o', '_', 'low', 'er_', 'i', 't', '.', '_']\n"
     ]
    }
   ],
   "source": [
    "example_corpus = \"low \"*5 + \"lowest \"*2 + \"newer \"*6 + \"wider \"*3 + \"new \"*2\n",
    "print(example_corpus)\n",
    "\n",
    "# learn the vocabulary\n",
    "vocab, merged_pairs, word_tokens  = bpe_learn(example_corpus)\n",
    "\n",
    "# encode a test sentence\n",
    "s = \"Yes, they said we need to lower it.\"\n",
    "s_encoded = bpe_segment(s, vocab, merged_pairs, word_tokens)\n",
    "print(s_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's organize the code into a tokenizer class. \n",
    "\n",
    "#### Note: When we first split the training corpus on whitespaces, we will also treat each punctuation character (like comma, period, question mark, etc.) as a separate word. We also will not use lowercase folding, instead we allow both upper and lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE():\n",
    "\n",
    "    def __init__(self, max_vocab_size=100):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.default_init_vocab = list(string.ascii_letters) \n",
    "        self.vocab = None\n",
    "        self.merged_pairs = None\n",
    "        self.subword2idx = None\n",
    "\n",
    "    # splits corpus into words, returns a dictionary of word frequencies\n",
    "    def init_vocab_words(self, corpus, eow_token=\"_\"):\n",
    "        vocab = list(set(self.default_init_vocab + list(\"\".join(corpus.split())))) + [eow_token]\n",
    "        # split corpus across whitespaces and make all characters lower case (we use NLTK word_tokenize fucntion because\n",
    "        # we want to keep punctuations as separate words)\n",
    "        words = word_tokenize(corpus.strip())\n",
    "        print(f\"Number of words in corpus: {len(words)}\")\n",
    "        # get word frequencies\n",
    "        word_freq = collections.defaultdict(int)\n",
    "        for word in words:\n",
    "            # store the word key as a string containing tokens separated by whitespace\n",
    "            word_key = \" \".join(list(word+eow_token))\n",
    "            word_freq[word_key] += 1\n",
    "\n",
    "        return vocab, word_freq\n",
    "\n",
    "    # find all unique pairs of adjacent tokens and their counts\n",
    "    def get_pairs(self, word_freq):\n",
    "        pairs = collections.Counter()\n",
    "        for word, freq in word_freq.items():\n",
    "            tokens = word.split()\n",
    "            for pair in zip(tokens[:-1], tokens[1:]):\n",
    "                pairs[pair] += freq \n",
    "        return pairs\n",
    "\n",
    "    # merge a pair of tokens (token learner)\n",
    "    def merge_pair_learner(self, pair, word_freq, iter):\n",
    "        word_freq_new = collections.defaultdict(int)\n",
    "        pattern = \" \".join(pair)\n",
    "        pattern_joined = \"\".join(pair) \n",
    "        if not pair[1].endswith(\"_\"):\n",
    "            pattern = pattern + \" \" # match whitespace at the end\n",
    "            pattern_joined = pattern_joined + \" \"  \n",
    "        pattern = re.escape(pattern) # special characters need to be escaped   \n",
    "        for word, freq in word_freq.items():\n",
    "            # merge all occurances of the pair in every word\n",
    "            word_new = re.sub(pattern, pattern_joined, word)\n",
    "            word_freq_new[word_new] = freq\n",
    "        return word_freq_new\n",
    "\n",
    "    # learns a BPE subword vocabulary and merge rules from a given training corpus\n",
    "    def learn(self, corpus):\n",
    "        # get inital vocab, word and token pair frequencies\n",
    "        vocab, word_freq = self.init_vocab_words(corpus)\n",
    "        # performs mergers to learn the vocabulary\n",
    "        merged_pairs = []        \n",
    "        pbar = tqdm(total=self.max_vocab_size, desc=\"Building vocab. Num tokens added --> \")\n",
    "        for _ in range(self.max_vocab_size):\n",
    "            # get counts of all adjacent token pairs\n",
    "            pairs = self.get_pairs(word_freq)\n",
    "            if not pairs:\n",
    "                break\n",
    "            # get most frequent pair\n",
    "            most_freq_pair = pairs.most_common(1)[0][0]\n",
    "            # apply merger\n",
    "            word_freq = self.merge_pair_learner(most_freq_pair, word_freq, _)\n",
    "            # add merged token to vocab\n",
    "            vocab.append(\"\".join(most_freq_pair))\n",
    "            merged_pairs.append(most_freq_pair)\n",
    "            pbar.update(1)\n",
    "            \n",
    "        self.vocab = sorted(vocab)\n",
    "        self.merged_pairs = merged_pairs\n",
    "        self.subword2idx = {t:i for i,t in enumerate(self.vocab)}\n",
    "\n",
    "        # precompute tokenized words\n",
    "        word_tokens = collections.defaultdict(list)\n",
    "        corpus_words = set(corpus.lower().strip().split())\n",
    "        pbar = tqdm(total=len(corpus_words), desc=\"Precomputing word tokenizations --> \")\n",
    "        for word in corpus_words:\n",
    "            word_tokens[word] = self.tokenize_word(word)   \n",
    "            pbar.update(1)\n",
    "        self.word_tokens = word_tokens\n",
    "\n",
    "    # merge a pair of tokens (token segmenter)\n",
    "    def merge_pair_segmenter(self, pair, tokens):\n",
    "        pattern = \" \".join(pair)\n",
    "        pattern_joined = \"\".join(pair) \n",
    "        if not pair[1].endswith(\"_\"):\n",
    "            pattern = pattern + \" \"\n",
    "            pattern_joined = pattern_joined + \" \"\n",
    "        pattern = re.escape(pattern)        \n",
    "        # merge all occurances of the pair in every word\n",
    "        tokens_new = re.sub(pattern, pattern_joined, tokens)\n",
    "        return tokens_new\n",
    "\n",
    "    # encode a word into a list of BPE tokens\n",
    "    def tokenize_word(self, word):\n",
    "        # split word into individual characters separated by white spaces, also insert eow token\n",
    "        word_tokens = \" \".join(list(word)) + \" _\"\n",
    "        # now replace all occurances of pairs with the merged pair\n",
    "        for pair in self.merged_pairs:\n",
    "            word_tokens =  self.merge_pair_segmenter(pair, word_tokens)\n",
    "            if len(word_tokens.split()) == 1:\n",
    "                break\n",
    "        \n",
    "        word_tokens = word_tokens.split()\n",
    "        return word_tokens\n",
    "\n",
    "    # tokenize a corpus/sentence\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        # split corpus into words\n",
    "        corpus_words = word_tokenize(sentence.strip())\n",
    "        # now replace all occurances of pairs with the merged pair\n",
    "        corpus_tokens = []\n",
    "        for word in corpus_words:\n",
    "            if word in self.word_tokens:\n",
    "                tokens = self.word_tokens[word]\n",
    "            else:    \n",
    "                tokens = self.tokenize_word(word)\n",
    "            corpus_tokens.extend(tokens)    \n",
    "        return corpus_tokens    \n",
    "    \n",
    "    # convert sentence to list of integer indices of subword tokens\n",
    "    def encode(self, sentence):\n",
    "        # convert sentence to subword tokens\n",
    "        tokens = self.tokenize_sentence(sentence)\n",
    "        # convert subwords tokens to indices\n",
    "        tokens = [self.subword2idx[t] for t in tokens]\n",
    "        return tokens\n",
    "    \n",
    "    # decoding token sequence back to a sentence is easy, just merge tokens and put a space after every token that ends with the \"_\" character\n",
    "    def decode(self, tokens_idx):\n",
    "        tokens = [self.vocab[idx] for idx in tokens_idx]\n",
    "        decoded_sentence = \"\"\n",
    "        for token in tokens:\n",
    "            if token.endswith(\"_\"):\n",
    "                decoded_sentence = decoded_sentence + token.strip(\"_\") + \" \"\n",
    "            else:\n",
    "                decoded_sentence = decoded_sentence + token    \n",
    "        return decoded_sentence.rstrip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new \n",
      "Number of words in corpus: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab. Num tokens added --> :  16%|█▌        | 16/100 [00:00<00:00, 7633.81it/s]\n",
      "Precomputing word tokenizations --> : 100%|██████████| 5/5 [00:00<00:00, 6981.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'e', 's', '_', 't', 'h', 'e', 'y', '_', 's', 'a', 'i', 'd', '_', 'w', 'e', '_', 'ne', 'e', 'd', '_', 't', 'o', '_', 'low', 'er_', 'i', 't', '_']\n",
      "[24, 31, 58, 26, 59, 36, 31, 67, 26, 58, 27, 37, 30, 26, 62, 31, 26, 50, 31, 30, 26, 59, 54, 26, 42, 33, 37, 59, 26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "example_corpus = \"low \"*5 + \"lowest \"*2 + \"newer \"*6 + \"wider \"*3 + \"new \"*2\n",
    "print(example_corpus)\n",
    "\n",
    "bpe_encoder = BPE()\n",
    "bpe_encoder.learn(example_corpus)\n",
    "\n",
    "# encode a test sentence\n",
    "s = \"Yes they said we need to lower it\"\n",
    "s_tokenized= bpe_encoder.tokenize_sentence(s)\n",
    "s_encoded= bpe_encoder.encode(s)\n",
    "print(s_tokenized)\n",
    "print(s_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the lazy fox. But, that's not the end of this story. What happened afterwards?\n",
      "Number of words in corpus: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab. Num tokens added --> :  69%|██████▉   | 69/100 [00:00<00:00, 15481.28it/s]\n",
      "Precomputing word tokenizations --> : 100%|██████████| 18/18 [00:00<00:00, 9023.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'e', 's_', ',_', 'th', 'e', 'y_', 's', 'a', 'i', 'd_', 'w', 'e_', 'n', 'e', 'e', 'd_', 't', 'o', '_', 'l', 'o', 'w', 'er', '_', 'i', 't_', '._']\n",
      "[38, 60, 107, 3, 114, 60, 124, 106, 41, 76, 59, 121, 61, 88, 60, 60, 59, 112, 91, 40, 83, 91, 121, 64, 40, 76, 113, 5]\n",
      "Yes , they said we need to lower it .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "example_corpus = \"The quick brown fox jumped over the lazy fox. But, that's not the end of this story. What happened afterwards?\"\n",
    "print(example_corpus)\n",
    "\n",
    "bpe_encoder = BPE()\n",
    "bpe_encoder.learn(example_corpus)\n",
    "\n",
    "# encode a test sentence\n",
    "s = \"Yes, they said we need to lower it.\"\n",
    "s_tokenized= bpe_encoder.tokenize_sentence(s)\n",
    "s_encoded= bpe_encoder.encode(s)\n",
    "s_decoded= bpe_encoder.decode(s_encoded)\n",
    "print(s_tokenized)\n",
    "print(s_encoded)\n",
    "print(s_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can test on a bigger corpus from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/tanzid/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "# raw text from \"Emma\" by Jane Austen\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus: 196546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab. Num tokens added --> : 100%|██████████| 200/200 [00:01<00:00, 101.75it/s]\n",
      "Precomputing word tokenizations --> : 100%|██████████| 7344/7344 [00:01<00:00, 6738.93it/s]\n"
     ]
    }
   ],
   "source": [
    "bpe_encoder = BPE(max_vocab_size=200)\n",
    "bpe_encoder.learn(\" \".join(emma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y', 'e', 's', 'ss_', ',_', 'the', 'y_', 'sa', 'id_', 'w', 'e_', 'ne', 'ed_', 't', 'a_', 'l', 'ow', 'er_', 'it_', '._']\n",
      "[64, 110, 219, 232, 10, 241, 273, 221, 153, 259, 111, 187, 114, 236, 74, 164, 205, 121, 159, 15]\n",
      "Yesss , they said we need ta lower it .\n"
     ]
    }
   ],
   "source": [
    "# encode a test sentence\n",
    "s = \"Yesss, they said we need ta lower it.\"\n",
    "s_tokenized = bpe_encoder.tokenize_sentence(s)\n",
    "s_encoded = bpe_encoder.encode(s)\n",
    "s_decoded = bpe_encoder.decode(s_encoded)\n",
    "print(s_tokenized)\n",
    "print(s_encoded)\n",
    "print(s_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
