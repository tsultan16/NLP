{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'low low low low low lowest lowest newer newer newer newer newer newer wider wider wider new new '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_corpus = \"low \"*5 + \"lowest \"*2 + \"newer \"*6 + \"wider \"*3 + \"new \"*2\n",
    "example_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding\n",
    "\n",
    "This is an algorithm for subword tokenization. It has two parts, a `token learner` and a `token segmenter`. The learner generates a vocabulary of subword type and the segmenter converts a test sentence into a sequence of subword tokens using the learned vocabulary.\n",
    "\n",
    "To learn the vocabulary, we perform the following steps:\n",
    "\n",
    "1) Given the input corpus of text (which is a single string), split across all whitespaces to get a set of strings. Then append a special end-of-word character \"_\" to each string. We define each of these strings to be a \"word\".\n",
    "2) Count the frequency of all the unique words. Split each word into a list of its individual characters, which are the tokens.\n",
    "3) Initialize the vocabulary as the set of all unique characters across these words, including the \"_\" character.\n",
    "4) Find the pair of adjacent tokens that appear most frequently (in case of ties, just break in some consistent way). Merge the pair of tokens into a single new token, add this new token to the vocabulary and replace all occuracnces of this pair across all words with this new token.\n",
    "5) Repeat step 4 until the vocabulary reaches some specified size.   \n",
    "\n",
    "In addition to the learned vocabulary of all merged tokens, we also keep track of all the pairs that have been merged. Then we can segment a test corpus by first splitting it into individual characters (replacing all whitespaces with the \"_\" end of word character). Then we replace all occurances of adjacent tokens from the first merged pair with the merged pair token. Then we repeat this for the second pair and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Define some helper functions that will be used by the token learner\n",
    "\"\"\"\n",
    "\n",
    "# splits corpus into words, returns a dictionary of word frequencies\n",
    "def init_vocab_words(corpus, eow_token=\"_\"):\n",
    "    vocab = list(set(list(\"\".join(corpus.split())))) + [eow_token]\n",
    "    # split corpus across whitespaces and make all characters lower case\n",
    "    words = corpus.lower().strip().split()\n",
    "    # get word frequencies\n",
    "    word_freq = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        # store the word key as a string containing tokens separated by whitespace\n",
    "        word_key = \" \".join(list(word+eow_token))\n",
    "        word_freq[word_key] += 1\n",
    "\n",
    "    return vocab, word_freq\n",
    "\n",
    "\n",
    "# find all unique pairs of adjacent tokens and their counts\n",
    "def get_pairs(word_freq):\n",
    "    pairs = collections.Counter()\n",
    "    for word, freq in word_freq.items():\n",
    "        tokens = word.split()\n",
    "        for pair in zip(tokens[:-1], tokens[1:]):\n",
    "            pairs[pair] += freq \n",
    "    return pairs\n",
    "\n",
    "\n",
    "# merge a pair of tokens (token learner)\n",
    "def merge_pair_learner(pair, word_freq):\n",
    "    word_freq_new = collections.defaultdict(int)\n",
    "    pattern = \" \".join(pair)\n",
    "    pattern_joined = \"\".join(pair) \n",
    "    if not pair[1].endswith(\"_\"):\n",
    "        pattern = pattern + \" \"\n",
    "        pattern_joined = pattern_joined + \" \"\n",
    "\n",
    "    for word, freq in word_freq.items():\n",
    "        # merge all occurances of the pair in every word\n",
    "        word_new = re.sub(pattern, pattern_joined, word)\n",
    "        word_freq_new[word_new] = freq\n",
    "    return word_freq_new\n",
    "\n",
    "\"\"\"\n",
    "    BPE token learner \n",
    "\"\"\"\n",
    "def bpe_learn(corpus, k=100):\n",
    "    \n",
    "    # get inital vocab, word and token pair frequencies\n",
    "    vocab, word_freq = init_vocab_words(corpus)\n",
    "    #print(f\"word_freq: {word_freq}\")\n",
    "\n",
    "    # performs mergers to learn the vocabulary\n",
    "    merged_pairs = []\n",
    "    for _ in range(k):\n",
    "        # get counts of all adjacent token pairs\n",
    "        pairs = get_pairs(word_freq)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # get most frequent pair\n",
    "        most_freq_pair = pairs.most_common(1)[0][0]\n",
    "        #print(f\"best pair: {most_freq_pair}\")\n",
    "\n",
    "        # apply merger\n",
    "        word_freq = merge_pair_learner(most_freq_pair, word_freq)\n",
    "        #print(f\"word_freq after merge: {word_freq}\")\n",
    "\n",
    "        # add merged token to vocab\n",
    "        vocab.append(\"\".join(most_freq_pair))\n",
    "        merged_pairs.append(most_freq_pair)\n",
    "\n",
    "    # precompute tokenized words\n",
    "    word_tokens = collections.defaultdict(list)\n",
    "    corpus_words = corpus.lower().strip().split()\n",
    "    for word in corpus_words:\n",
    "        word_tokens[word] = tokenize_word(word, vocab, merged_pairs)   \n",
    "\n",
    "    #print(f\"merged pairs: {merged_pairs}\")    \n",
    "    #print(f\"final vocab: {vocab}\")    \n",
    "    #print(f\"precomputed tokenizations: {word_tokens}\")\n",
    "\n",
    "    return vocab, merged_pairs, word_tokens    \n",
    "    \n",
    "\n",
    "# merge a pair of tokens (token segmenter)\n",
    "def merge_pair_segmenter(pair, corpus_tokens):\n",
    "    pattern = \" \".join(pair)\n",
    "    pattern_joined = \"\".join(pair) \n",
    "    if not pair[1].endswith(\"_\"):\n",
    "        pattern = pattern + \" \"\n",
    "        pattern_joined = pattern_joined + \" \"\n",
    "    # merge all occurances of the pair in every word\n",
    "    corpus_tokens_new = re.sub(pattern, pattern_joined, corpus_tokens)\n",
    "    return corpus_tokens_new\n",
    "\n",
    "\n",
    "# encode a word into a list of BPE tokens\n",
    "def tokenize_word(word, vocab, pairs):\n",
    "    # split word into individual characters separated by white spaces, also insert eow token\n",
    "    word_tokens = \" \".join(list(word)) + \" _\"\n",
    "    # now replace all occurances of pairs with the merged pair\n",
    "    for pair in pairs:\n",
    "        word_tokens =  merge_pair_segmenter(pair, word_tokens)\n",
    "        if len(word_tokens.split()) == 1:\n",
    "            break\n",
    "    \n",
    "    word_tokens = word_tokens.split()\n",
    "    return word_tokens\n",
    "\n",
    "\"\"\" \n",
    "    BPE token segmenter\n",
    "\"\"\"    \n",
    "def bpe_segment(test_corpus, vocab, pairs, precomputed_word_tokens):\n",
    "    # split corpus into words\n",
    "    corpus_words = test_corpus.split()\n",
    "    # now replace all occurances of pairs with the merged pair\n",
    "    #print(f\"Corpus words: \\n{corpus_words}\")\n",
    "    for word in corpus_words:\n",
    "        if word in precomputed_word_tokens:\n",
    "            word_tokens = precomputed_word_tokens[word]\n",
    "        else:    \n",
    "            word_tokens = tokenize_word(word, vocab, pairs)\n",
    "        print(f\"{word} ---> {word_tokens}\")\n",
    "\n",
    "    #return corpus_tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merged_pairs, word_tokens  = bpe_learn(example_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y e s , _ t h e y _ s a i d _ w e _ n e e d _ t o _ l o w e r _ i t . _'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Yes, they said we need to lower it.\"\n",
    "\" \".join(list(\"_\".join(s.split())+\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low', 'er_']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_word(\"lower\", vocab, merged_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, ---> ['Y', 'e', 's', ',', '_']\n",
      "they ---> ['t', 'h', 'e', 'y', '_']\n",
      "said ---> ['s', 'a', 'i', 'd', '_']\n",
      "we ---> ['w', 'e', '_']\n",
      "need ---> ['ne', 'e', 'd', '_']\n",
      "to ---> ['t', 'o', '_']\n",
      "lower ---> ['low', 'er_']\n",
      "it. ---> ['i', 't', '.', '_']\n"
     ]
    }
   ],
   "source": [
    "#bpe_segment(s, vocab, merged_pairs)\n",
    "bpe_segment(s, vocab, merged_pairs, word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
