{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Language Model\n",
    "\n",
    "Given a sequence of word embedding vectors $w_1, .., w_N$, we pass the sequence through a `uni-directional RNN` to obtain a sequence of hidden states $h_1,..,h_N$. Then note that each hidden state $h_i$ can be regarded as a contextual repsentation of the words $w_1, .., w_i$. So using this hidden state, we can compute a probability distribution for the next word that follows all the preceding words:\n",
    "\n",
    "$P(w_{i+1} | w_1,...,w_i) = f(h_i)$\n",
    "\n",
    "where $f$ is a function that transforms $h_i$ into the probability distribution. $f$ can be a feedforward network, in the simplest case a linear projection followed by a softmax. Also note that we use a uni-directional RNN (and not bi-directional) because for a language model, we want to predict the next word using only the previous words as context.\n",
    "\n",
    "Optional: The performance of an RNN model can be further improved if we choose the embedding dimensions and the RNN hidden state dimensions to be the same. This allows us to then re-use the embedding matrix to perform the linear projection of the hidden states into the output logits instead of using a separate projection matrix and therefore saves a lot of extra parameters and potentially reduces overfitting. This technique is also called `weight tying`. \n",
    "\n",
    "Previously we looked at simple n-gram language models which are only feasilble for small $n$, i.e. shorter context size. With an RNN, we have access to much larger contexts and therefore we can get better performance (e.g. lower perplexity compared to n-gram LMs).\n",
    "\n",
    "We will train a word-level RNN LM on the collected works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random, math\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the training data\n",
    "with open('shakespeare.txt', 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 40000\n",
      "Number of training sentences: 36000\n",
      "Number of test sentences: 4000\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize the sentences (split on whitespaces) and add start and end sentence tokens, keep punctuations as individual tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>' \n",
    "sentences_tokenized = [[start_token]+word_tokenize(s.lower())+[end_token] for s in lines]\n",
    "print(f\"Num sentences: {len(sentences_tokenized)}\")    \n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "\n",
    "sentences_train = sentences_tokenized[:-num_test]\n",
    "sentences_val = sentences_tokenized[-num_test:]\n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_val)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all sentences\n",
    "corpus_train = [w for s in sentences_train for w in s] \n",
    "corpus_val = [w for s in sentences_val for w in s] \n",
    "\n",
    "# create vocabulary\n",
    "pad_token = \"<PAD>\"\n",
    "vocab = [pad_token] + sorted(list(set([w for s in sentences_tokenized for w in s])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# tokenize the corpus\n",
    "x_train = [word2idx[word] for word in corpus_train]\n",
    "x_val = [word2idx[word] for word in corpus_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shakespeare(Dataset):\n",
    "    def __init__(self, corpus, block_size=16):\n",
    "        self.corpus = corpus\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)-self.block_size-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = torch.tensor(self.corpus[idx:idx+self.block_size], dtype=torch.long)\n",
    "        targets = torch.tensor(self.corpus[idx+1:idx+1+self.block_size], dtype=torch.long)\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the RNN LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=32, num_rnn_layers=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        c = 0.1        \n",
    "        torch.nn.init.uniform_(self.emb.weight, -c, c)\n",
    "\n",
    "        # create rnn layers (we will use bidirectional LSTM so the output hidden states will have dims=2*hidden_dims)\n",
    "        if num_rnn_layers == 1:\n",
    "            self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=num_rnn_layers, batch_first=True)\n",
    "        else:    \n",
    "            self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        # tie the output layer weights with the embedding layer weights\n",
    "        self.output_layer.weight = self.emb.weight\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y=None):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute rnn hidden states\n",
    "        x, _ = self.rnn_layers(x) # shape: (B,L,D)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,vocab_size)\n",
    "\n",
    "        if y==None:\n",
    "            return x\n",
    "\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,vocab_size)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y)\n",
    "        return x, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, vocab, word2idx, temperature=1.0, topk=None, start_token=\"<s>\", end_token=\"</s>\", max_len=30, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        # generate one word at a time\n",
    "        x = torch.full(size=(1,1), fill_value=word2idx[start_token], dtype=torch.long, device=device)\n",
    "        for _ in range(max_len):\n",
    "            logits = self.forward(x) # shape: (1,L,V)\n",
    "            # rescale the logits with the temperature\n",
    "            logits = logits / temperature\n",
    "            if topk is not None:\n",
    "                topk_logits, idx = torch.sort(logits[0,-1,:], descending=True)\n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(topk_logits, dim=-1) # shape: (V,)\n",
    "                next_word_idx = idx[torch.multinomial(p, num_samples=1)]\n",
    "            else:             \n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(logits[:,-1,:], dim=-1) # shape: (V,)\n",
    "                next_word_idx = torch.multinomial(p, num_samples=1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_word_idx.view(1,1)), dim=1)\n",
    "        # convert integer tokens to words\n",
    "        words = x.view(-1).tolist()\n",
    "        words = [vocab[w] for w in words[1:]]\n",
    "        # remove <s> tokens and replace </s> tokens with \"\\n\"\n",
    "        sent = []\n",
    "        for w in words:\n",
    "            if w != start_token:\n",
    "                if w != end_token:\n",
    "                    sent.append(w)\n",
    "                else:\n",
    "                    sent.append(\"\\n\")    \n",
    "\n",
    "        sent= \" \".join(sent) \n",
    "        \n",
    "        self.train()\n",
    "\n",
    "        return sent\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10, val_every=1, save_every=10, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    pp = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}, Val Perplexity: {pp:.1f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = num_correct / num_total        \n",
    "        if epoch%val_every == 0:\n",
    "            # compute validation loss\n",
    "            val_loss, val_acc, pp = validation(model, val_dataloader, device=device)\n",
    "\n",
    "        if (epoch+1) % save_every == 0:\n",
    "            save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    perplexity = math.exp(val_loss)\n",
    "    return val_loss, val_accuracy, perplexity\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'rnntagger_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('rnntagger_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303695\n",
      "31100\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = Shakespeare(x_train, block_size=64)\n",
    "#val_dataset = Shakespeare(x_val, block_size=64)\n",
    "\n",
    "train_dataset = Shakespeare(x_train[3*65536:], block_size=16)\n",
    "val_dataset = Shakespeare(x_val[:16384], block_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint!\n",
      "Total number of parameters in transformer network: 5.803317 M\n",
      "RAM used: 1066.91 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "D = 256\n",
    "num_rnn_layers = 5\n",
    "learning_rate = 1e-3\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = RNNLM(vocab_size, D, num_rnn_layers=num_rnn_layers, dropout_rate=0.5).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.95)\n",
    "model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Neural_LM/wandb/run-20240111_072403-87nhtpr5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/RNN_shakespeare/runs/87nhtpr5' target=\"_blank\">jumping-tree-1</a></strong> to <a href='https://wandb.ai/tanzids/RNN_shakespeare' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/RNN_shakespeare' target=\"_blank\">https://wandb.ai/tanzids/RNN_shakespeare</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/RNN_shakespeare/runs/87nhtpr5' target=\"_blank\">https://wandb.ai/tanzids/RNN_shakespeare/runs/87nhtpr5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"RNN_shakespeare\", \n",
    "    config={\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": B, \n",
    "        \"emb_dim\": D,\n",
    "        \"num_rnn_layers\" : num_rnn_layers,\n",
    "        \"corpus\": \"Shakespeare\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 3.131, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000, Val Perplexity: 0.0: 100%|██████████| 3346/3346 [00:34<00:00, 96.20it/s] \n",
      "Epoch 2, EMA Train Loss: 3.007, Train Accuracy:  0.421, Val Loss:  4.405, Val Accuracy:  0.364, Val Perplexity: 81.8: 100%|██████████| 3346/3346 [00:34<00:00, 97.27it/s] \n",
      "Epoch 3, EMA Train Loss: 2.876, Train Accuracy:  0.431, Val Loss:  4.405, Val Accuracy:  0.364, Val Perplexity: 81.8: 100%|██████████| 3346/3346 [00:34<00:00, 97.50it/s] \n",
      "Epoch 4, EMA Train Loss: 2.762, Train Accuracy:  0.436, Val Loss:  4.405, Val Accuracy:  0.364, Val Perplexity: 81.8: 100%|██████████| 3346/3346 [00:34<00:00, 97.54it/s] \n",
      "Epoch 5, EMA Train Loss: 2.706, Train Accuracy:  0.442, Val Loss:  4.405, Val Accuracy:  0.364, Val Perplexity: 81.8: 100%|██████████| 3346/3346 [00:35<00:00, 93.36it/s]\n",
      "Epoch 6, EMA Train Loss: 2.588, Train Accuracy:  0.447, Val Loss:  4.405, Val Accuracy:  0.364, Val Perplexity: 81.8: 100%|██████████| 3346/3346 [00:35<00:00, 93.18it/s]\n",
      "Epoch 7, EMA Train Loss: 2.569, Train Accuracy:  0.453, Val Loss:  4.611, Val Accuracy:  0.359, Val Perplexity: 100.6: 100%|██████████| 3346/3346 [00:35<00:00, 93.35it/s]\n",
      "Epoch 8, EMA Train Loss: 2.603, Train Accuracy:  0.458, Val Loss:  4.611, Val Accuracy:  0.359, Val Perplexity: 100.6: 100%|██████████| 3346/3346 [00:37<00:00, 90.31it/s]\n",
      "Epoch 9, EMA Train Loss: 2.557, Train Accuracy:  0.461, Val Loss:  4.611, Val Accuracy:  0.359, Val Perplexity: 100.6: 100%|██████████| 3346/3346 [00:36<00:00, 92.03it/s]\n",
      "Epoch 10, EMA Train Loss: 2.511, Train Accuracy:  0.465, Val Loss:  4.611, Val Accuracy:  0.359, Val Perplexity: 100.6: 100%|██████████| 3346/3346 [00:36<00:00, 92.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, EMA Train Loss: 2.469, Train Accuracy:  0.468, Val Loss:  4.611, Val Accuracy:  0.359, Val Perplexity: 100.6: 100%|██████████| 3346/3346 [00:36<00:00, 90.63it/s]\n",
      "Epoch 12, EMA Train Loss: 2.431, Train Accuracy:  0.470, Val Loss:  4.812, Val Accuracy:  0.354, Val Perplexity: 123.0: 100%|██████████| 3346/3346 [00:36<00:00, 91.83it/s]\n",
      "Epoch 13, EMA Train Loss: 2.442, Train Accuracy:  0.473, Val Loss:  4.812, Val Accuracy:  0.354, Val Perplexity: 123.0: 100%|██████████| 3346/3346 [00:36<00:00, 91.52it/s]\n",
      "Epoch 14, EMA Train Loss: 2.512, Train Accuracy:  0.475, Val Loss:  4.812, Val Accuracy:  0.354, Val Perplexity: 123.0: 100%|██████████| 3346/3346 [00:37<00:00, 89.21it/s]\n",
      "Epoch 15, EMA Train Loss: 2.445, Train Accuracy:  0.476, Val Loss:  4.812, Val Accuracy:  0.354, Val Perplexity: 123.0: 100%|██████████| 3346/3346 [00:37<00:00, 88.41it/s]\n",
      "Epoch 16, EMA Train Loss: 2.458, Train Accuracy:  0.479, Val Loss:  4.812, Val Accuracy:  0.354, Val Perplexity: 123.0: 100%|██████████| 3346/3346 [00:35<00:00, 93.21it/s]\n",
      "Epoch 17, EMA Train Loss: 2.463, Train Accuracy:  0.481, Val Loss:  4.879, Val Accuracy:  0.352, Val Perplexity: 131.6: 100%|██████████| 3346/3346 [00:36<00:00, 90.62it/s]\n",
      "Epoch 18, EMA Train Loss: 2.402, Train Accuracy:  0.482, Val Loss:  4.879, Val Accuracy:  0.352, Val Perplexity: 131.6: 100%|██████████| 3346/3346 [00:36<00:00, 90.44it/s]\n",
      "Epoch 19, EMA Train Loss: 2.441, Train Accuracy:  0.484, Val Loss:  4.879, Val Accuracy:  0.352, Val Perplexity: 131.6: 100%|██████████| 3346/3346 [00:36<00:00, 92.93it/s]\n",
      "Epoch 20, EMA Train Loss: 2.366, Train Accuracy:  0.485, Val Loss:  4.879, Val Accuracy:  0.352, Val Perplexity: 131.6: 100%|██████████| 3346/3346 [00:37<00:00, 88.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=20, save_every=10, val_every=5) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5eb33675514ace8e0f2f9de42f7de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch loss</td><td>█▅▄▄▃▃▂▂▃▂▂▃▂▂▂▂▁▂▂▂▂▁▁▂▁▂▁▂▂▂▂▂▂▁▁▂▂▂▂▂</td></tr><tr><td>Moving Avg Loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Val Loss</td><td>▁███████████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch loss</td><td>3.32277</td></tr><tr><td>Moving Avg Loss</td><td>3.38491</td></tr><tr><td>Val Loss</td><td>4.54387</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-tree-1</strong> at: <a href='https://wandb.ai/tanzids/RNN_shakespeare/runs/87nhtpr5' target=\"_blank\">https://wandb.ai/tanzids/RNN_shakespeare/runs/87nhtpr5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240111_072403-87nhtpr5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mark the run as finished\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is some sap to the deputy . \n",
      " \n",
      " escalus : \n",
      " sir , my lord , \n",
      " if your law be so murdering days his land , \n",
      " that i have seen him last , but , as she is , \n",
      " am there as monstrous as you take off our right \n",
      " is pawn 'd his swelling eye , \n",
      " the first mould of life , you have not answer 'd \n",
      " for his own old lord and well-warranted man , \n",
      " drest as good as damask roses ; \n",
      " masks too weak to fear her brother 's ghost , \n",
      " that is not the sweet apollo 's son . \n",
      " \n",
      " florizel : \n",
      " come , sir ; here 's no man the rest , \n",
      " that thou neglect him not , and i do thou hast come by \n",
      " him ; and , or repent to your father , nor as \n",
      " i am no children , i am glad i am a courtier : i \n",
      " have heard her in their silent affairs \n",
      " and with the witness of that hath access too much \n",
      " all foolery like language . but his head to the \n",
      " speech of this reason : if i should be \n",
      " false , that for the other earth is so green , \n",
      " perfume in the end . \n",
      " \n",
      " paulina : \n",
      " ha ! undone ! \n",
      " say a man witch : i am not well pence \n",
      " that angelo were a changeling : he will be a wife for a \n",
      " daughter and most i can not do , and yield the world : \n",
      " though i believe not thou dost not institute \n",
      " that may encounter to you ? \n",
      " \n",
      " shepherd : \n",
      " that 's there ; ere the boy is spotless \n",
      " you do not speak to see at her ? \n",
      " \n",
      " polixenes : \n",
      " yes , not a man . \n",
      " \n",
      " autolycus : \n",
      " i am : by the depart , if i \n",
      " had heard he 's at this place of all my soul : \n",
      " i am by hand here . \n",
      " i 'll reconcile me to attach him , and we do \n",
      " know the thing that was not no less to be : 't is a meddling friar : \n",
      " so such a day that makes her old life \n",
      " hath been to you so , but the case is now , \n",
      " though did my father have no less prevail , \n",
      " but they behold my sister . \n",
      " \n",
      " duke vincentio : \n",
      " pardon on thou diest : \n",
      " the son\n"
     ]
    }
   ],
   "source": [
    "s = model.generate(vocab, word2idx, temperature=0.9, topk=None, device=DEVICE, max_len=500)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
