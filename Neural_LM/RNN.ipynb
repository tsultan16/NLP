{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Language Model\n",
    "\n",
    "Given a sequence of word embedding vectors $w_1, .., w_N$, we pass the sequence through a `uni-directional RNN` to obtain a sequence of hidden states $h_1,..,h_N$. Then note that each hidden state $h_i$ can be regarded as a contextual repsentation of the words $w_1, .., w_i$. So using this hidden state, we can compute a probability distribution for the next word that follows all the preceding words:\n",
    "\n",
    "$P(w_{i+1} | w_1,...,w_i) = f(h_i)$\n",
    "\n",
    "where $f$ is a function that transforms $h_i$ into the probability distribution. $f$ can be a feedforward network, in the simplest case a linear projection followed by a softmax. Also note that we use a uni-directional RNN (and not bi-directional) because for a language model, we want to predict the next word using only the previous words as context.\n",
    "\n",
    "Optional: The performance of an RNN model can be further improved if we choose the embedding dimensions and the RNN hidden state dimensions to be the same. This allows us to then re-use the embedding matrix to perform the linear projection of the hidden states into the output logits instead of using a separate projection matrix and therefore saves a lot of extra parameters and potentially reduces overfitting. This technique is also called `weight tying`. \n",
    "\n",
    "Previously we looked at simple n-gram language models which are only feasilble for small $n$, i.e. shorter context size. With an RNN, we have access to much larger contexts and therefore we can get better performance (e.g. lower perplexity compared to n-gram LMs).\n",
    "\n",
    "We will train a word-level RNN LM on the collected works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 32777\n",
      "Number of training sentences: 29500\n",
      "Number of test sentences: 3277\n"
     ]
    }
   ],
   "source": [
    "# prep the training data\n",
    "with open('shakespeare.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# remove all punctuations (except for apostrophe) and escape characters from the lines, lowercase all characters\n",
    "sentences_clean = []\n",
    "for line in lines:\n",
    "    cleaned = re.sub(r\"[^\\w\\s']\",'',line).strip().lower()\n",
    "    if len(cleaned) > 0:\n",
    "        sentences_clean.append(cleaned)\n",
    "\n",
    "# word_tokenize the sentences (split on whitespaces) and add start and end sentence tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'        \n",
    "sentences_tokenized = [[start_token]+s.split()+[end_token] for s in sentences_clean]\n",
    "print(f\"Num sentences: {len(sentences_tokenized)}\")    \n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_val = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_val.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_val)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest train sentence: 18\n",
      "Longest val sentence: 17\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "pad_token = \"<PAD>\"\n",
    "vocab = [pad_token] + sorted(list(set([w for s in sentences_tokenized for w in s])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# tokenize the sentences \n",
    "x_train = [[word2idx[word] for word in s] for s in sentences_train]\n",
    "x_val = [[word2idx[word] for word in s] for s in sentences_val]\n",
    "\n",
    "max_len_train = max([len(s) for s in x_train])\n",
    "max_len_val = max([len(s) for s in x_val])\n",
    "\n",
    "print(f\"Longest train sentence: {max_len_train}\")\n",
    "print(f\"Longest val sentence: {max_len_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shakespeare(Dataset):\n",
    "    def __init__(self, x):\n",
    "        inputs = [s[:-1] for s in x]\n",
    "        targets = [s[1:] for s in x]\n",
    "        self.inputs = pad_sequence([torch.tensor(x, dtype=torch.long) for x in inputs], batch_first=True, padding_value=0)\n",
    "        self.targets = pad_sequence([torch.tensor(y, dtype=torch.long) for y in targets], batch_first=True, padding_value=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Shakespeare(x_train)\n",
    "val_dataset = Shakespeare(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the RNN LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=32, num_rnn_layers=1, dropout_rate=0.1, padding_idx=-1):\n",
    "        super().__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # embedding layer\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        c = 0.1        \n",
    "        torch.nn.init.uniform_(self.emb.weight, -c, c)\n",
    "\n",
    "        # create rnn layers (we will use bidirectional LSTM so the output hidden states will have dims=2*hidden_dims)\n",
    "        if num_rnn_layers == 1:\n",
    "            self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=num_rnn_layers, batch_first=True)\n",
    "        else:    \n",
    "            self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        # tie the output layer weights with the embedding layer weights\n",
    "        self.output_layer.weight = self.emb.weight\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute rnn hidden states\n",
    "        x, _ = self.rnn_layers(x) # shape: (B,L,D)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,vocab_size)\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,vocab_size)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y, ignore_index=self.padding_idx)\n",
    "\n",
    "        return x, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
