{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Language Model\n",
    "\n",
    "Given a sequence of word embedding vectors $w_1, .., w_N$, we pass the sequence through a `uni-directional RNN` to obtain a sequence of hidden states $h_1,..,h_N$. Then note that each hidden state $h_i$ can be regarded as a contextual repsentation of the words $w_1, .., w_i$. So using this hidden state, we can compute a probability distribution for the next word that follows all the preceding words:\n",
    "\n",
    "$P(w_{i+1} | w_1,...,w_i) = f(h_i)$\n",
    "\n",
    "where $f$ is a function that transforms $h_i$ into the probability distribution. $f$ can be a feedforward network, in the simplest case a linear projection followed by a softmax. Also note that we use a uni-directional RNN (and not bi-directional) because for a language model, we want to predict the next word using only the previous words as context.\n",
    "\n",
    "Optional: The performance of an RNN model can be further improved if we choose the embedding dimensions and the RNN hidden state dimensions to be the same. This allows us to then re-use the embedding matrix to perform the linear projection of the hidden states into the output logits instead of using a separate projection matrix and therefore saves a lot of extra parameters and potentially reduces overfitting. This technique is also called `weight tying`. \n",
    "\n",
    "Previously we looked at simple n-gram language models which are only feasilble for small $n$, i.e. shorter context size. With an RNN, we have access to much larger contexts and therefore we can get better performance (e.g. lower perplexity compared to n-gram LMs).\n",
    "\n",
    "We will train a word-level RNN LM on the collected works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the training data\n",
    "with open('shakespeare.txt', 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 40000\n",
      "Number of training sentences: 36000\n",
      "Number of test sentences: 4000\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize the sentences (split on whitespaces) and add start and end sentence tokens, keep punctuations as individual tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'        \n",
    "sentences_tokenized = [[start_token]+word_tokenize(s.lower())+[end_token] for s in lines]\n",
    "print(f\"Num sentences: {len(sentences_tokenized)}\")    \n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_val = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_val.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_val)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'first', 'citizen', ':', '</s>'],\n",
       " ['<s>',\n",
       "  'before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  ',',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak',\n",
       "  '.',\n",
       "  '</s>'],\n",
       " ['<s>', '</s>'],\n",
       " ['<s>', 'all', ':', '</s>'],\n",
       " ['<s>', 'speak', ',', 'speak', '.', '</s>'],\n",
       " ['<s>', '</s>'],\n",
       " ['<s>', 'first', 'citizen', ':', '</s>'],\n",
       " ['<s>',\n",
       "  'you',\n",
       "  'are',\n",
       "  'all',\n",
       "  'resolved',\n",
       "  'rather',\n",
       "  'to',\n",
       "  'die',\n",
       "  'than',\n",
       "  'to',\n",
       "  'famish',\n",
       "  '?',\n",
       "  '</s>'],\n",
       " ['<s>', '</s>'],\n",
       " ['<s>', 'all', ':', '</s>']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest train sentence: 22\n",
      "Longest val sentence: 20\n"
     ]
    }
   ],
   "source": [
    "# create vocabulary\n",
    "pad_token = \"<PAD>\"\n",
    "vocab = [pad_token] + sorted(list(set([w for s in sentences_tokenized for w in s])))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# tokenize the sentences \n",
    "x_train = [[word2idx[word] for word in s] for s in sentences_train]\n",
    "x_val = [[word2idx[word] for word in s] for s in sentences_val]\n",
    "\n",
    "max_len_train = max([len(s) for s in x_train])\n",
    "max_len_val = max([len(s) for s in x_val])\n",
    "\n",
    "print(f\"Longest train sentence: {max_len_train}\")\n",
    "print(f\"Longest val sentence: {max_len_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shakespeare(Dataset):\n",
    "    def __init__(self, x):\n",
    "        inputs = [s[:-1] for s in x]\n",
    "        targets = [s[1:] for s in x]\n",
    "        self.inputs = pad_sequence([torch.tensor(x, dtype=torch.long) for x in inputs], batch_first=True, padding_value=0)\n",
    "        self.targets = pad_sequence([torch.tensor(y, dtype=torch.long) for y in targets], batch_first=True, padding_value=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Shakespeare(x_train)\n",
    "val_dataset = Shakespeare(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the RNN LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=32, num_rnn_layers=1, dropout_rate=0.1, padding_idx=-1):\n",
    "        super().__init__()\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # embedding layer\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        c = 0.1        \n",
    "        torch.nn.init.uniform_(self.emb.weight, -c, c)\n",
    "\n",
    "        # create rnn layers (we will use bidirectional LSTM so the output hidden states will have dims=2*hidden_dims)\n",
    "        if num_rnn_layers == 1:\n",
    "            self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=num_rnn_layers, batch_first=True)\n",
    "        else:    \n",
    "            self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=embedding_dim, num_layers=num_rnn_layers, batch_first=True, dropout=dropout_rate)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        # tie the output layer weights with the embedding layer weights\n",
    "        self.output_layer.weight = self.emb.weight\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y=None):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute rnn hidden states\n",
    "        x, _ = self.rnn_layers(x) # shape: (B,L,D)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,vocab_size)\n",
    "\n",
    "        if y==None:\n",
    "            return x\n",
    "\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,vocab_size)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y, ignore_index=self.padding_idx)\n",
    "        return x, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, vocab, word2idx, start_token=\"<s>\", end_token=\"</s>\", max_len=30, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        # generate one word at a time\n",
    "        x = torch.full(size=(1,1), fill_value=word2idx[start_token], dtype=torch.long, device=device)\n",
    "        for _ in range(max_len):\n",
    "            logits = self.forward(x) # shape: (1,L,V)\n",
    "            # sample from the distribution for the last word in the sequence\n",
    "            p = F.softmax(logits[0,-1,:]) # shape: (V,)\n",
    "            next_word_idx = torch.multinomial(p, num_samples=1)\n",
    "            if next_word_idx == word2idx[end_token]:\n",
    "                break\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_word_idx.view(1,1)), dim=1)\n",
    "        self.train()\n",
    "        \n",
    "        # convert integer tokens to words\n",
    "        words = x.view(-1).tolist()\n",
    "        words = [vocab[w] for w in words[1:]]\n",
    "        return \" \".join(words)\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            \n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = num_correct / num_total        \n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "\n",
    "        #if epoch % 5 == 0:\n",
    "        #    save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'rnntagger_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('rnntagger_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 0.835445 M\n",
      "RAM used: 1214.17 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "D = 64\n",
    "num_rnn_layers = 1\n",
    "learning_rate = 1e-2\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = RNNLM(vocab_size, D, num_rnn_layers=num_rnn_layers).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 5.116, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 282/282 [00:03<00:00, 80.52it/s]\n",
      "Epoch 2, EMA Train Loss: 4.803, Train Accuracy:  0.184, Val Loss:  5.027, Val Accuracy:  0.220:  72%|███████▏  | 204/282 [00:02<00:00, 94.11it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1029243/1347638028.py:53: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(logits[0,-1,:]) # shape: (V,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'o warwick and my honour like to chance'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(vocab, word2idx, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
