{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Language Model\n",
    "\n",
    "Similar to what we did for the RNN, we are given a sequence of word embedding vectors $w_1, .., w_N$, we pass the sequence through a `Transformer decoder network` (i.e. causal masking is applied to the attention scores) to obtain a sequence of output vectors $h_1,..,h_N$. Then each of these $h_i$ can be regarded as a contextual repsentation of the words $w_1, .., w_i$. So using this output, we can compute a probability distribution for the next word that follows all the preceding words:\n",
    "\n",
    "$P(w_{i+1} | w_1,...,w_i) = f(h_i)$\n",
    "\n",
    "Again, as in the case of the RNN, we will use a simple feed forward network for the transformation $f$, in this case a linear projection followed by a softmax. Instead of using word-level tokenization, we will use sub-word tokenization, in particular Byte-Pair Encoding. We will train the model on the collected work of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random, math\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from BPE import BPE\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the training data\n",
    "with open('shakespeare.txt', 'r') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus: 334797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocab. Num tokens added --> :  22%|██▏       | 2249/10000 [00:38<01:54, 67.70it/s]"
     ]
    }
   ],
   "source": [
    "# now lets train a BPE tokenizer on the Shakespeare corpus\n",
    "tokenizer = BPE(max_vocab_size=10000, sos_token=\"<s>\", eos_token=\"</s>\")\n",
    "tokenizer.learn(lines)\n",
    "tokenizer.precompute_word_tokens(lines)\n",
    "print(f\"\\nLearned vocab: {tokenizer.vocab}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>_', 'Y', 'ea', 'h_', ',_', 'I_', \"'\", 'm_', 'g', 'on_', 'na_', 'inform_', 'them_', 'to', 'day_', '._', '</s>_']]\n",
      "[[94, 2105, 4214, 5226, 72, 1024, 5, 6265, 4970, 6884, 6656, 5645, 9029, 9192, 3759, 81, 92]]\n",
      "[\"<s> Yeah , I 'm gon na inform them today . \\n\"]\n"
     ]
    }
   ],
   "source": [
    "# encode and decode on test sentence to make sure tokenizer is working properly \n",
    "s = [\"Yeah, I'm gonna inform them today.\"]\n",
    "s_tokenized= tokenizer.tokenize_sentences(s)\n",
    "s_encoded= tokenizer.encode(s)\n",
    "s_decoded= tokenizer.decode(s_encoded)\n",
    "print(s_tokenized)\n",
    "print(s_encoded)\n",
    "print(s_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# load trained BPE tokenizer from file\\nwith open('BPE_tokenizer.pkl', 'rb') as f:\\n    tokenizer = pickle.load(f)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the trained tokenizer to file\n",
    "\n",
    "import pickle\n",
    "with open(\"BPE_tokenizer.pkl\", 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "    \n",
    "\"\"\"\n",
    "# load trained BPE tokenizer from file\n",
    "with open('BPE_tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the sentences and create train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subword tokenize the sentences and convert to integer tokens\n",
    "sentences_tokenized = tokenizer.encode(lines)\n",
    "\n",
    "# create train-val splits\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "x_train = sentences_tokenized[:-num_test]\n",
    "x_val   = sentences_tokenized[-num_test:]\n",
    "\n",
    "# concatenate all sentences\n",
    "x_train = [w for s in x_train for w in s] \n",
    "x_val   = [w for s in x_val for w in s] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens train: 316352\n",
      "Num tokens val: 32542\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num tokens train: {len(x_train)}\")\n",
    "print(f\"Num tokens val: {len(x_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shakespeare(Dataset):\n",
    "    def __init__(self, corpus, block_size=16):\n",
    "        self.corpus = corpus\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)-self.block_size-1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = torch.tensor(self.corpus[idx:idx+self.block_size], dtype=torch.long)\n",
    "        targets = torch.tensor(self.corpus[idx+1:idx+1+self.block_size], dtype=torch.long)\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Transformer decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, blocks_size, embedding_dim=16, feedforward_dim=64, num_heads=1, num_layers=1, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = blocks_size\n",
    "        # embedding layers\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_emb = torch.nn.Embedding(blocks_size, embedding_dim)\n",
    "        c = 0.01        \n",
    "        torch.nn.init.uniform_(self.emb.weight, -c, c)\n",
    "        torch.nn.init.uniform_(self.pos_emb.weight, -c, c)\n",
    "\n",
    "        # transformer decoder\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=feedforward_dim, dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        # tie the output layer weights with the embedding layer weights\n",
    "        self.output_layer.weight = self.emb.weight\n",
    "\n",
    "    def create_causal_mask(self, input):\n",
    "        _, L, _ = input.shape\n",
    "        # create an L x L matrix with ones on and below diagonal\n",
    "        mask = torch.tril(torch.ones(size=(L,L), device=input.device))\n",
    "        # create mask in which the positions where there is a zero is filled with -infinity \n",
    "        mask = mask.masked_fill((mask==0), float(\"-inf\"))\n",
    "        return mask\n",
    "\n",
    "    def sinusoidal_positional_encoding(self, input):\n",
    "        _, L, D = input.shape\n",
    "        # static positional encoding (max length set to 1024)\n",
    "        pos_emb = torch.zeros(size=(L, D), device=input.device)\n",
    "        for pos in range(L):\n",
    "            for i in range(0, D, 2):\n",
    "                pos_emb[pos, i] = math.sin(pos / (10000 ** ((2 * i)/D)))\n",
    "                if i+1 < D:\n",
    "                    pos_emb[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1))/D)))\n",
    "        return pos_emb\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y=None):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # add positional embedding\n",
    "        #x = x + self.sinusoidal_positional_encoding(x)# shape: (B,L,D)\n",
    "        x = x + self.pos_emb(torch.arange(x.shape[1], device=x.device)) # shape: (B,L,D)\n",
    "        # pass through transformer decoder layers\n",
    "        mask = self.create_causal_mask(x)\n",
    "        x = self.transformer_decoder(x, x, tgt_mask=mask) # shape: (B,L,D)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,vocab_size)\n",
    "\n",
    "        if y==None:\n",
    "            return x\n",
    "\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,vocab_size)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y)\n",
    "        return x, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, vocab, subword2idx, temperature=1.0, topk=None, start_token=\"<s>\", end_token=\"</s>\", max_len=30, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        # generate one token at a time\n",
    "        x = torch.full(size=(1,1), fill_value=subword2idx[start_token], dtype=torch.long, device=device)\n",
    "        tokens = [x.item()]\n",
    "        for _ in range(max_len):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-self.block_size:]\n",
    "            logits = self.forward(x) # shape: (1,L,V)\n",
    "            # rescale the logits with the temperature\n",
    "            logits = logits / temperature\n",
    "            if topk is not None:\n",
    "                topk_logits, idx = torch.sort(logits[0,-1,:], descending=True)\n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(topk_logits, dim=-1) # shape: (V,)\n",
    "                next_word_idx = idx[torch.multinomial(p, num_samples=1)]\n",
    "            else:             \n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(logits[:,-1,:], dim=-1) # shape: (V,)\n",
    "                next_word_idx = torch.multinomial(p, num_samples=1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_word_idx.view(1,1)), dim=1)\n",
    "            tokens.append(next_word_idx.item())\n",
    "\n",
    "        self.train()\n",
    "        return tokens\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10, val_every=1, save_every=10, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    pp = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}, Val Perplexity: {pp:.1f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = num_correct / num_total        \n",
    "        if epoch%val_every == 0:\n",
    "            # compute validation loss\n",
    "            val_loss, val_acc, pp = validation(model, val_dataloader, device=device)\n",
    "\n",
    "        if (epoch+1) % save_every == 0:\n",
    "            save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    perplexity = math.exp(val_loss)\n",
    "    return val_loss, val_accuracy, perplexity\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'transformer_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('transformer_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 32\n",
    "train_dataset = Shakespeare(x_train, block_size=L)\n",
    "val_dataset = Shakespeare(x_val, block_size=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 0.367285 M\n",
      "RAM used: 1010.53 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "D = 32\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "\n",
    "model = TransformerLM(vocab_size, L, embedding_dim=D, feedforward_dim=4*D, num_heads=num_heads, num_layers=num_layers, dropout_rate=0.1).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/9885 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 4.262, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000, Val Perplexity: 0.0: 100%|██████████| 9885/9885 [01:36<00:00, 102.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=1, save_every=100, val_every=1) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Upon Whiter ced burden gues armed deliverance \n",
      " <s> And f-resipenitence ebb le vitae \n",
      " <s> Ah , Edward experty , black ewtribunes ; That tily and twispeechless er ear . \n",
      " <s> COMINIUS : \n",
      " <s> Why , Hontolmber ; Dockws 't , keys , and supLa's duke child . \n",
      " <s> Grithirty or Clarence . he ne'er pense with our lookof men \n",
      " <s> If I when I 'll not warrant : \n",
      " <s> criservant can do leave Warwick we slain ; \n",
      " <s> Is full on the end with Romeo , I 'll good company \n",
      " <s> And patence \n",
      " <s> You , , here is been eyes , youth , this die , \n",
      " <s> If I cut my unpreserved . Unto the entrails \n",
      " <s> But he , that speak'st they have in this well ! I big is early . \n",
      " <s> \n",
      " <s> \n",
      " <s> Away was thy yourself he , she were in them : Menenius , \n",
      " <s> To brittle living . \n",
      " <s> this a angels impossible him . \n",
      " <s> \n",
      " <s> \n",
      " <s> \n",
      " <s> CLAUDIO : \n",
      " <s> Good friends changed to rough prize and thou good who well from him : \n",
      " <s> But I know to make me peace than doubt the Padua by this wound \n",
      " <s> Sgentleman great ease , we but a lady , , the king , \n",
      " <s> do a sent you . \n",
      " <s> \n",
      " <s> He is your ckle , myself , \n",
      " <s> I do I shall the you , I will have begot him . \n",
      " <s> \n",
      " <s> \n",
      " <s> Good king , and shall have it , I say you are to have not be not not you \n",
      " <s> But , to you must were for some an yourself I great and thy that , \n",
      " <s> so have love thy knowledge \n",
      " <s> thou will say to be her knowledge \n",
      " <s> Your me you never I will think did . \n",
      " <s> \n",
      " <s> DUKE IV : \n",
      " <s> KING RICHARD III : \n",
      " <s> Happlord then can will must beauty of sight far . \n",
      " <s> \n",
      " <s> Margaret , but loving passage . \n",
      " <s> \n",
      " <s> \n",
      " <s> Faith , take your brother resisted words , Mark eleven . \n",
      " <s> \n",
      " <s> BAPTISTA : \n",
      " <s> \n",
      " <s> Let . \n",
      " <s> CATESBY : \n",
      " <s> maliry ! \n",
      " <s> hubow of us than none depart . \n",
      " <s> \n",
      " <s> \n",
      " <s> \n",
      " <s> KING OF IV : \n",
      " <s> Against your house , confession ; \n",
      " <s> And produce YORK : \n",
      " <s> BENVOLIO : \n",
      " <s> Faith , hear lay me it ; look : \n",
      " <s> This is a mutinous ; exitself ? \n",
      " <s> Here , he leave it should ability , , \n",
      " <s> Where , despite , thy lodmasteel ; and presdstlevied , \n",
      " <s> Hments counsel , the youthful young newr uy , \n",
      " <s> Come , their creation nor the bride to morrow , \n",
      " <s> Roman : She PERDITA : \n",
      " <s> To second fair , \n",
      " <s> It incorporate afshield . \n",
      " <s> \n",
      " <s> What , with wrecily heavier beauty with the sight , grant with statutes appear , \n",
      " <s> Your pper with these fitter of many of she can it , \n",
      " <s> Like dog still at a dignity ! heaven ! \n",
      " <s> But thou may be their ship and will have fresh fortunate \n",
      " <s> QUEEN IV : \n",
      " <s> To make a first remorse and fought , \n",
      " <s> BRUTUS : \n",
      " <s> You did at Edward else , our closely , \n",
      " <s> And handent , my wooers is my view he . \n",
      " <s> \n",
      " <s> Not doth half ; for my fair possess own . \n",
      " <s> DUKE VI : \n",
      " <s> GLOUCESTER : \n",
      " <s> Be thee , contrawreck . \n",
      " <s> \n",
      " <s> FLORIZEL : \n",
      " <s> Where give not bad west , that will must since , that so quick . \n",
      " <s> \n",
      " <s> CORIOLANUS : \n",
      " <s> JOHN OF YORK : it . Nay ; but well . \n",
      " <s> \n",
      " <s> She , by justice : \n",
      " <s> \n",
      " <s> Or am murder out on , I I am not not will it rei \n",
      " <s> With give your be no blows , dukedom \n",
      " <s> I 'll had I will 'll begging ? \n",
      " <s> HENRY BOLINGBROKE : I bring for mine ? \n",
      " <s> \n",
      " <s> To she too lling like know with the nurse , obtain him \n",
      " <s> What nothing coninsterstabonea , \n",
      " <s> bestrid , Romeo ? \n",
      " <s> ISABELLA : \n",
      " <s> PETER : \n",
      " <s> subdue in Mab , speed . \n",
      " <s> DERBY : \n",
      " <s> ANGELO : \n",
      " <s> VINCENTIO : \n",
      " <s> A lord ? and disshelter ; she is laabout the such hap , \n",
      " <s> \n",
      " <s> But hands at a cauybeard -- \n",
      " <s> MONTAGUE : \n",
      " <s> So thank you are not here ? I tell you you as the one \n",
      " <s> But good what set that a needy hearts no our such \n",
      " <s> Ay to thy , grieve me , \n",
      " <s> But no gone in his former overchosen . \n",
      " <s> I secret ear , I suppose I can say , I JULIET : \n",
      " <s> Send 'd his wisdom is an process , \n",
      " <s> Till do not be hear his common set it must can have be too . \n",
      " <s> \n",
      " <s> \n",
      " <s> Help , come , Loswell ill-cted the care ! \n",
      " <s> \n",
      " <s> another rheum , the medried with a gaunt ? ' sir : \n",
      " <s> Now , spur ; live . Why thou supper , \n",
      " <s> What , and distrests , \n",
      " <s> That joint not lovely face , achieve did we 'll have let forth . \n",
      " <s> ROMEO : it , the \n",
      " <s> These lives must came a miraIV : \n",
      " <s> AUFIDIUS : reaill-ck-living , my madam , \n",
      " <s> Why , so you you mine , you be so is ay , a face , \n",
      " <s> But not your own not see , now . Then were but he shall with the witted sing in \n",
      " <s> And this 's more , you make him all a air , Romeo ; \n",
      " <s> To am the cheerfully of thou sin to the niece . \n",
      " <s> \n",
      " <s> O he thou been than a man let me , and speak , impose : \n",
      " <s> I to , it , the behalf ? what can warrant . \n",
      " <s> \n",
      " <s> \n",
      " <s> JULIET : \n",
      " <s> may could say . \n",
      " <s> \n",
      " <s> \n",
      " <s> <s> LADY ANNE : \n",
      " <s> Seize him to MENENIUS : \n",
      " <s> doings : it : \n",
      " <s> The power : \n",
      " <s> A father is for fer his Gaunt thus : \n",
      " <s> Unbeseethpassamother \n",
      " <s> Ay , folly , inform 'd him . \n",
      " <s> Our treble woman ! here so Volsce , \n",
      " <s> pikes . \n",
      " <s> sh . Most belly from violon'd show your revel you way with it . \n",
      " <s> \n",
      " <s> But me 's afore rather you art do't . \n",
      " <s> \n",
      " <s> PAULINA : \n",
      " <s> KING RICHARD BOLINGBROKE : \n",
      " <s> If to more ten , try , but do not right . \n",
      " <s> \n",
      " <s> CARLISLE : \n",
      " <s> EXTON : \n",
      " <s> When cheeks . \n",
      " <s> A vice , and that did willingly be aid . \n",
      " <s> \n",
      " <s> \n",
      " <s> First sour Volscians ; \n",
      " <s> Be thought my rend the son from my offence together , 't know his sovereign out \n",
      " <s> And you be town and true other rail him \n",
      " <s> you have confess I 'll have be as I would I send it \n",
      " <s> Your heart I must make my king she \n",
      " <s> And not he know it should I I break so : thy been some \n",
      " <s> to to do you say for any hands \n",
      " <s> We <s> set'st tailor ; \n",
      " <s> I Incigate : my love cruel Rutland 's lesrevive your thrveretwo \n",
      " <s> To be party to not make a wounds ; \n",
      " <s> And thou think should have bold molehill forbid \n",
      " <s> Let 'd tongue \n",
      " <s> Some showing riintelligence is a terrible liners and branch his brother . Come , \n",
      " <s> There was bids me farewell . How fear thee , as I KING RICHARD ELIZABETH : \n",
      " <s> To tied me you is mask . We have Romeo shall be told that midnight \n",
      " <s> The good cross 'd with the rough , \n",
      " <s> He he Blchers shalt doth be hear all \n",
      " <s> Yea , and most cat ; my die to France : : \n",
      " <s> A appetite I have my tears -- \n",
      " <s> First CAPULET : \n",
      " <s> Your brother youthful same have not thy soul . Please my Pereaneedless t'st left them \n",
      " <s> DUKE fall \n",
      " <s> JULIET : \n",
      " <s> Do do burn the midnight strength with the enets a soest weeds , \n",
      " <s> His skAppacles , make this devise , \n",
      " <s> My ears it when for some report Gloucester , \n",
      " <s> Lest your profit is a what sight , the kers not by all your captifully . \n",
      " <s> \n",
      " <s> longer Gloucester is it bless the supreme , one know your belly long \n",
      " <s> Nay 's you are welcome ? Will ; cram you is strength , to a earth , \n",
      " <s> popular case that not can for the speech : lay me \n",
      " <s> before the sea in the lord , his English a \n",
      " <s> When you you have you we are not you mean was thy thy affection unto her ; \n",
      " <s> I I seest much into him you is Warwick , \n",
      " <s> for 't were friend , the nothing ginger enough . \n",
      " <s> \n",
      " <s> \n",
      " <s> RATCLIFF : \n",
      " <s> Thou offence . \n",
      " <s> besneedy , hope friends , reverend , the matter , you not present . \n",
      " <s> \n",
      " <s> \n",
      " <s> There no hath love , sit , trumpet , To-morrow poor lamb nympowns and this , \n",
      " <s> So profane here from his obedient wink ; \n",
      " <s> Consui; who stops the lord , whose ller late and commoostentation , \n",
      " <s> So looks braloof . \n",
      " <s> My ood thy dukedom , Re-monument made say , \n",
      " <s> What must makes to dear lord , well so Thursday , \n",
      " <s> Lest esteem , plotted , so consumed that can BUCKINGHAM : good , \n",
      " <s> And I patient . Second nothing : \n",
      " <s> LARTIUS : \n",
      " <s> The Duke of York , honour , -- \n",
      " <s> \n",
      " <s> LADY BENVOLIO : \n",
      " <s> Of my liege , like up recompense . \n",
      " <s> \n",
      " <s> \n",
      " <s> \n",
      " <s> ROMEO : \n",
      " <s> Stay , country , thus fs . \n",
      " <s> \n",
      " <s> First why those as prithee , sith thou hast go : \n",
      " <s> There is resign serve you circumstances \n",
      " <s> Than that\n"
     ]
    }
   ],
   "source": [
    "tokens = model.generate(tokenizer.vocab, tokenizer.subword2idx, temperature=1.0, topk=None, device=DEVICE, max_len=2000)\n",
    "decoded = tokenizer.decode([tokens])\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
