{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Passage Retrieval (DPR)\n",
    "\n",
    "We saw how how to use the TFIDF representation for passages to perform retreival. One major problem with this kind of sparse vector representation is that if the query words don't exactly match any words from the relavant passages, then the retreival system will not be able to find those passages (because of zero cosine similarity between the query and passage vectors). To solve this problem, we can use some form of dense vector representation that can generalize better and can match a query to relevant documents even when there may not be exact matching words from the query in the passage. \n",
    "\n",
    "In the DPR model, we have `bi-encoders`, i.e. two separate BERT networks, a `query encoder` and a `pasage encoder`, which learn to map queries and passages respectively into a dense vector space in which the similarity between a query vector and it's corresponding relevant passage(s) is maximized. We use the output for the `[CLS]` token from each encoder as the dense vector representation. \n",
    "\n",
    "The bi-encoders are jointly trained using a supervised classification task where each input instance is a tuple $(q_i, p_i^{+}, p_{i,1}^{-}, ...,p_{i,n}^{-})$ where $q_i$ is a query, $p_i^{+}$ is a rlevant/positive passage and each of the $n$ $p_{i,j}^{-}$ are irrelevant/negative documents. Then we use the query encoder to compute the dense vector representation for the query $E_{Q}(q)$ and use the passage encoder for all the passages $E_P(p)$. Then we compute similarity scores between the query vector and each passage vector: $sim(q_i, p)$ for $p \\in \\{p_i^{+}, p_{i,1}^{-}, ...,p_{i,n}^{-}\\}$. We can interpret these similarity scores as unnormalized logits for $(n+1)$ different class labels. With this interpretaion, we can define $sim(q_i, p_i^{+})$ as the logit for the \"correct\\ground truth class\" and then simply use the `softmax cross-entropy/negative log-likelihood loss` function:\n",
    "\n",
    "$L(q_i, p_i^{+}, p_{i,1}^{-}, ...,p_{i,n}^{-}) = -\\log \\frac{exp(sim(q_i, p_i^{+}))}{exp(sim(q_i, p_i^{+})) + \\sum_{j=1}^n exp(sim(q_i, p_{i,j}^{-}))}$\n",
    "\n",
    "Note that $[exp(sim(q_i, p_i^{+}), exp(sim(q_i, p_{i,1}^{-}),..., exp(sim(q_i, p_{i,n}^{-})]$ represents a probability distrbution and minimizing the loss function pushes $exp(sim(q_i, p_i^{+}))$ towards 1 and pushes the $exp(sim(q_i, p_{i,j}^{-}))$ towards zero, which allows us to achieve the dense vector space in which a query vector is maximally similar to the positive passage vector and dis-similar to the negative passages. We also use the simple `dot product` as our similarity metric.\n",
    "\n",
    "For the SQuAD dataset, we already have given question, context passage pairs. Now we need to somehow choose negative passages for each pair. For training efficieny, we can use a simple trick. Given that we have a minibatch of $B$ such (question, context passage) pairs, then for each pair, we can simply just assign the passages from the other $B-1$ pairs as the negatives. Then we can compute the pair-wise dot product between every question-passgae pair with a single matrix multiplication. So given a matrix $Q$ of shape $(B,d)$ containing the batch of query vectors (where $d$ is the hidden dimensions of the encoded vectors) and a matrix $P$ of the same shape containing the batch of passages, we can compute the matrix $QP^T$ whose $(i,j)th$ entry given us the dot product between the ith question and the jth passage. So the $ith$ diagonal entry in this matrix is the dot product between $ith$ question and its corresponding positive passage and all other elements from that row are dot products with the negative passages. Then by taking the softmax of each row of this matrix, we can compute the total loss for the batch by just summing up the negative log of the terms along the diagonal. In addition to training efficiency, the other huge advantage of this technique is that the dataset will be shuffled before each epoch so that each question-positive passage pair will always get a different sample set of negative passages and therefore we effectively get a very large set of negatives per pair.\n",
    "\n",
    "We wil use two DistilBERT models for our bi-encoders (two BERTs probably won't fit on my GPU and DistilBERT is half the size and performs almost as well as BERT anyway).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define our Bi-encoder model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBiEncoder(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.query_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.passage_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        for param in self.query_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.passage_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, query_idx, query_attn_mask, passage_idx, passage_attn_mask):\n",
    "        # compute BERT encodings, extract the `[CLS]` encoding (first element of the sequence), apply dropout        \n",
    "        passage_output = self.passage_encoder(passage_idx, attention_mask=passage_attn_mask)\n",
    "        passage_enc = self.dropout(passage_output.last_hidden_state[:,0]) # shape: (batch_size, hidden_size)\n",
    "        query_output = self.query_encoder(query_idx, attention_mask=query_attn_mask)\n",
    "        query_enc = self.dropout(query_output.last_hidden_state[:, 0]) # shape: (batch_size, hidden_size)\n",
    "        # compute similarity score matrix\n",
    "        scores = torch.mm(query_enc, passage_enc.transpose(0, 1)) # shape: (batch_size, batch_size)\n",
    "        # compute cross-entropy loss\n",
    "        loss = F.cross_entropy(scores, torch.arange(len(scores)).to(scores.device))\n",
    "        return scores, loss\n",
    "    \n",
    "    @ torch.no_grad()\n",
    "    def encode_queries(self, query_idx, query_attn_mask):\n",
    "        self.eval()\n",
    "        query_output = self.query_encoder(query_idx, attention_mask=query_attn_mask)\n",
    "        query_enc = self.dropout(query_output.last_hidden_state[:, 0])\n",
    "        return query_enc\n",
    "\n",
    "    @ torch.no_grad()\n",
    "    def encode_passages(self, passage_idx, passage_attn_mask):\n",
    "        self.eval()\n",
    "        passage_output = self.passage_encoder(passage_idx, attention_mask=passage_attn_mask)\n",
    "        passage_enc = self.dropout(passage_output.last_hidden_state[:,0]) # shape: (batch_size, hidden_size)\n",
    "        return passage_enc\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, accumulation_steps=1, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = batch\n",
    "            # move batch to device\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = query_idx.to(device), query_attn_mask.to(device), passage_idx.to(device), passage_attn_mask.to(device)\n",
    "            # forward pass\n",
    "            scores, loss = model(query_idx, query_attn_mask, passage_idx, passage_attn_mask )\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # apply gradient step \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                # optimizer step\n",
    "                optimizer.step()\n",
    "                # reset gradients\n",
    "                optimizer.zero_grad()\n",
    "   \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = query_idx.shape\n",
    "            y_pred = scores.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            targets = torch.arange(B).to(device) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "\n",
    "            if val_every is not None:\n",
    "                if i%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                    pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        # run optimizer step for remainder batches\n",
    "        if len(train_dataloader) % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = batch\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = query_idx.to(device), query_attn_mask.to(device), passage_idx.to(device), passage_attn_mask.to(device)\n",
    "            scores, loss = model(query_idx, query_attn_mask, passage_idx, passage_attn_mask)\n",
    "            B, _ = query_idx.shape\n",
    "            y_pred = scores.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            targets = torch.arange(B).to(device) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'dpr_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('dpr_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load up the SQuAD v1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 19035\n",
      "Number of questions: 86821\n",
      "Number of passages: 1204\n",
      "Number of questions: 5928\n"
     ]
    }
   ],
   "source": [
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file) \n",
    "\n",
    "def get_passages(squad, num_titles=None):\n",
    "    if num_titles is None:\n",
    "        num_titles = len(squad['data'])\n",
    "    # for each title, get passages and all corresponding questions from SQuAD train set\n",
    "    passages = []\n",
    "    questions = []\n",
    "    num_questions = 0\n",
    "    j = 0\n",
    "    for i in range(num_titles):\n",
    "        #print(f\"Title# {i}: {squad['data'][i]['title']}, Number of passages: {len(squad['data'][i]['paragraphs'])}\")\n",
    "        for p in squad['data'][i]['paragraphs']:\n",
    "            passages.append(p['context'])\n",
    "            for q in p['qas']:\n",
    "                if not q['is_impossible']:\n",
    "                    questions.append((q,j))    \n",
    "                    num_questions += 1\n",
    "            j += 1\n",
    "    print(f\"Number of passages: {len(passages)}\")\n",
    "    print(f\"Number of questions: {num_questions}\")\n",
    "    return passages, questions\n",
    "\n",
    "passages_train, questions_train = get_passages(squad_train)\n",
    "passages_val, questions_val = get_passages(squad_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this because huggingface tokenizer is not thread safe when using it inside __getitem__ instead of dataloader collatefunction\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  #\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, passages, questions, block_size=128):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the question and context passage\n",
    "        q = self.questions[idx][0]\n",
    "        passage_idx = self.questions[idx][1]\n",
    "        question = q['question']\n",
    "        passage = self.passages[passage_idx]\n",
    "        # tokenize the context passage\n",
    "        passage_encoding = self.tokenizer.encode_plus(passage, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        passage_idx = passage_encoding['input_ids']\n",
    "        # tokenize the question\n",
    "        question_encoding = self.tokenizer.encode_plus(question, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        question_idx = question_encoding['input_ids']\n",
    "\n",
    "        # get answer span start and end character positions, for multiple answers, we will only use the first answer\n",
    "        first_answer_idx = 0\n",
    "        answer_start_char = q['answers'][first_answer_idx]['answer_start']\n",
    "        answer_end_char = answer_start_char + len(q['answers'][first_answer_idx]['text'])\n",
    "        # convert char positions to token positions\n",
    "        answer_start_token = passage_encoding.char_to_token(answer_start_char)\n",
    "        answer_end_token = passage_encoding.char_to_token(answer_end_char-1)\n",
    "\n",
    "        # select a window size so that the passage sequence will be no longer than the block size\n",
    "        window_size_tokens = self.block_size - 2 # 2 special tokens ([CLS], [SEP])\n",
    "        # now create a window around the answer span, pick the window start position randomly\n",
    "        window_start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "        window_start_max = min(answer_start_token, max(0,len(passage_idx) - window_size_tokens)) # we want to make the window as large as possible, but not go over the end of the context\n",
    "        window_start = random.randint(window_start_min, window_start_max)\n",
    "        window_end = window_start + window_size_tokens        \n",
    "        # select window of passage tokens\n",
    "        passage_window_tokens = passage_idx[window_start:window_end]\n",
    "\n",
    "        # create padded query and passage sequences\n",
    "        question_idx = [self.tokenizer.cls_token_id] + question_idx + [self.tokenizer.sep_token_id]\n",
    "        question_idx = question_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(question_idx))\n",
    "        passage_idx = [self.tokenizer.cls_token_id] + passage_window_tokens + [self.tokenizer.sep_token_id]\n",
    "        passage_idx = passage_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(passage_idx))\n",
    "\n",
    "        # make sure the passage sequence and query sequences are not longer than max_length\n",
    "        if len(question_idx) > self.block_size or len(passage_idx) > self.block_size:\n",
    "            raise Exception(f\"Passage sequence length {len(passage_idx)} or question sequence length {len(question_idx)} is longer than max_length {self.block_size}!\")\n",
    "        \n",
    "        # create attention masks\n",
    "        question_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in question_idx]\n",
    "        passage_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in passage_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        question_idx = torch.tensor(question_idx)\n",
    "        question_attn_mask = torch.tensor(question_attn_mask)\n",
    "        passage_idx = torch.tensor(passage_idx)\n",
    "        passage_attn_mask = torch.tensor(passage_attn_mask)\n",
    "\n",
    "        return question_idx, question_attn_mask, passage_idx, passage_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 192\n",
    "train_dataset = SquadDataset(passages_train, questions_train, block_size=block_size)\n",
    "val_dataset = SquadDataset(passages_val, questions_val, block_size=block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint!\n",
      "Total number of parameters in transformer network: 132.72576 M\n",
      "RAM used: 3363.17 MB\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTBiEncoder().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27978e46bd84d03ad77565489233aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112907111116026, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Information_Retrieval_Extraction/wandb/run-20240119_014726-7a9r052w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/DPR/runs/7a9r052w' target=\"_blank\">fancy-bird-3</a></strong> to <a href='https://wandb.ai/tanzids/DPR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/DPR' target=\"_blank\">https://wandb.ai/tanzids/DPR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/DPR/runs/7a9r052w' target=\"_blank\">https://wandb.ai/tanzids/DPR/runs/7a9r052w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"DPR\", \n",
    "    config={\n",
    "        \"bi-encoder model\": \"MobileBERT\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"SQuAD v1\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5427 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.019, Train Accuracy:  0.991, Val Loss:  0.105, Val Accuracy:  0.966:   1%|          | 56/5427 [00:50<26:20,  3.40it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.026, Train Accuracy:  0.991, Val Loss:  0.105, Val Accuracy:  0.966:   1%|▏         | 69/5427 [00:54<25:57,  3.44it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.063, Train Accuracy:  0.987, Val Loss:  0.105, Val Accuracy:  0.966:  18%|█▊        | 1000/5427 [05:28<22:02,  3.35it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.018, Train Accuracy:  0.988, Val Loss:  0.085, Val Accuracy:  0.969:  37%|███▋      | 2000/5427 [10:57<16:44,  3.41it/s]   Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.038, Train Accuracy:  0.987, Val Loss:  0.111, Val Accuracy:  0.965:  55%|█████▌    | 3000/5427 [16:26<12:05,  3.34it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.060, Train Accuracy:  0.987, Val Loss:  0.110, Val Accuracy:  0.962:  74%|███████▎  | 4000/5427 [21:56<07:11,  3.31it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.041, Train Accuracy:  0.988, Val Loss:  0.098, Val Accuracy:  0.967:  92%|█████████▏| 5000/5427 [27:26<02:07,  3.36it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 1, EMA Train Loss: 0.063, Train Accuracy:  0.988, Val Loss:  0.102, Val Accuracy:  0.966: 100%|██████████| 5427/5427 [30:06<00:00,  3.00it/s]  \n",
      "Epochs:   0%|          | 0/5427 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.008, Train Accuracy:  0.997, Val Loss:  0.102, Val Accuracy:  0.964:   1%|          | 59/5427 [00:51<26:28,  3.38it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.011, Train Accuracy:  0.991, Val Loss:  0.102, Val Accuracy:  0.964:   8%|▊         | 434/5427 [02:42<24:37,  3.38it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.038, Train Accuracy:  0.990, Val Loss:  0.102, Val Accuracy:  0.964:  18%|█▊        | 1000/5427 [05:29<21:54,  3.37it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.017, Train Accuracy:  0.990, Val Loss:  0.101, Val Accuracy:  0.965:  37%|███▋      | 2000/5427 [10:58<16:49,  3.40it/s]   Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.011, Train Accuracy:  0.990, Val Loss:  0.105, Val Accuracy:  0.964:  55%|█████▌    | 3000/5427 [16:28<12:01,  3.37it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.018, Train Accuracy:  0.990, Val Loss:  0.106, Val Accuracy:  0.966:  74%|███████▎  | 4000/5427 [21:58<06:57,  3.42it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.010, Train Accuracy:  0.990, Val Loss:  0.088, Val Accuracy:  0.968:  92%|█████████▏| 5000/5427 [27:27<02:06,  3.37it/s]  Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch 2, EMA Train Loss: 0.029, Train Accuracy:  0.989, Val Loss:  0.104, Val Accuracy:  0.966: 100%|██████████| 5427/5427 [30:07<00:00,  3.00it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=2, save_every=2, val_every=1000, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "#save_model_checkpoint(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dcc899f90c40e6b70eddc0e025c74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the model has been trained, let's precompute the encodings for all passages across the training and validation set, then evaluate the top-k retreival accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "passages = passages_train + passages_val\n",
    "questions = questions_train + questions_val\n",
    "\n",
    "b_size = 196-2\n",
    "\n",
    "# first we create fixed sized blocks from the passages, we will only use the validation set for this\n",
    "passage_blocks = []\n",
    "passage_blocks_enc = []\n",
    "passage_blocks_index = {}\n",
    "j = 0\n",
    "for k, p in enumerate(passages):\n",
    "    #print(p)\n",
    "    p_enc = tokenizer.encode_plus(p, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)['input_ids']\n",
    "    # split p_enc into blocks of size b_size\n",
    "    for i in range(0, len(p_enc), b_size):\n",
    "        p_enc_block = [tokenizer.cls_token_id] + p_enc[i:i+b_size] + [tokenizer.sep_token_id]  \n",
    "        # apply padding to the end of the block\n",
    "        p_enc_block = p_enc_block + [tokenizer.pad_token_id]*(b_size-len(p_enc_block))  \n",
    "        passage_blocks_enc.append(p_enc_block)\n",
    "        passage_blocks.append(tokenizer.decode(p_enc[i:i+b_size]))\n",
    "        passage_blocks_index[j] = k\n",
    "        j += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(p_enc_block):\n",
    "    # create attention masks\n",
    "    attn_mask  = [1 if idx != tokenizer.pad_token_id else 0 for idx in p_enc_block]\n",
    "    return attn_mask\n",
    "\n",
    "def encode_query(query):\n",
    "    query_enc = tokenizer.encode_plus(query, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)['input_ids']\n",
    "    # apply padding to the end of the block\n",
    "    query_enc = [tokenizer.cls_token_id] + query_enc + [tokenizer.sep_token_id] + [tokenizer.pad_token_id]*(b_size-len(query_enc)-2)  \n",
    "    return query_enc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Question#: 100%|██████████| 24502/24502 [09:30<00:00, 42.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# pre-compute the dense encodings for all passages\n",
    "passage_blocks_vectors = []\n",
    "pbar = tqdm(passage_blocks_enc, desc=\"Question#\")\n",
    "for p in pbar:\n",
    "    passage_blocks_enc_tensor = torch.tensor(p).unsqueeze(0).to(DEVICE)\n",
    "    attn_mask = torch.tensor(create_attention_mask(p)).unsqueeze(0).to(DEVICE)\n",
    "    passage_vector = model.encode_passages(passage_blocks_enc_tensor, attn_mask)\n",
    "    passage_blocks_vectors.append(passage_vector.squeeze(0))\n",
    "\n",
    "# now stack up the passage vectors into a matrix\n",
    "passage_blocks_vectors = torch.stack(passage_blocks_vectors)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive_passages(query, k=5):\n",
    "    # encode the query\n",
    "    q_enc = encode_query(query)\n",
    "    # compute the query vector\n",
    "    q_enc_tensor = torch.tensor(q_enc).unsqueeze(0).to(DEVICE)\n",
    "    attn_mask = torch.tensor(create_attention_mask(q_enc)).unsqueeze(0).to(DEVICE)\n",
    "    q_vector = model.encode_queries(q_enc_tensor, attn_mask)\n",
    "    # compute the similarity scores\n",
    "    scores = torch.mm(passage_blocks_vectors, q_vector.view(-1,1))\n",
    "    # get the top k scores\n",
    "    topk_scores, topk_indices = torch.topk(scores.view(-1), k=k)\n",
    "    # get topk passages\n",
    "    topk_passages = [passage_blocks[i] for i in topk_indices]\n",
    "    main_passages_idx = [passage_blocks_index[i.item()] for i in topk_indices]\n",
    "    return topk_scores, topk_passages, main_passages_idx    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Beyoncé's biggest musical influence?\n",
      "Gold Passage Idx# 38: Beyoncé names Michael Jackson as her major musical influence. Aged five, Beyoncé attended her first ever concert where Jackson performed and she claims to have realised her purpose. When she presented him with a tribute award at the World Music Awards in 2006, Beyoncé said, \"if it wasn't for Michael Jackson, I would never ever have performed.\" She admires Diana Ross as an \"all-around entertainer\" and Whitney Houston, who she said \"inspired me to get up there and do what she did.\" She credits Mariah Carey's singing and her song \"Vision of Love\" as influencing her to begin practicing vocal runs as a child. Her other musical influences include Aaliyah, Prince, Lauryn Hill, Sade Adu, Donna Summer, Mary J. Blige, Janet Jackson, Anita Baker and Rachelle Ferrell.\n",
      "Answer: Michael Jackson\n",
      "Gold passage found in topk passages!\n",
      "\n",
      "Topk passages:\n",
      "\n",
      "Passage#0: beyonce has stated that she is personally inspired by us first lady michelle obama, saying \" she proves you can do it all \" and she has described oprah winfrey as \" the definition of inspiration and a strong woman \". she has also discussed how jay z is a continuing inspiration to her, both with what she describes as his lyrical genius and in the obstacles he has overcome in his life. beyonce has expressed admiration for the artist jean - michel basquiat, posting in a letter \" what i find in the work of jean - michel basquiat, i search for in every day in music... he is lyrical and raw \". in february 2013, beyonce said that madonna inspired her to take control of her own career. she commented : \" i think about madonna and how she took all of the great things she achieved and started the label and developed other artists. but there are not enough of those women. \".\n",
      "Main Passage Index: 40\n",
      "Score: 40.371826171875\n",
      "\n",
      "Passage#1: beyonce names michael jackson as her major musical influence. aged five, beyonce attended her first ever concert where jackson performed and she claims to have realised her purpose. when she presented him with a tribute award at the world music awards in 2006, beyonce said, \" if it wasn't for michael jackson, i would never ever have performed. \" she admires diana ross as an \" all - around entertainer \" and whitney houston, who she said \" inspired me to get up there and do what she did. \" she credits mariah carey's singing and her song \" vision of love \" as influencing her to begin practicing vocal runs as a child. her other musical influences include aaliyah, prince, lauryn hill, sade adu, donna summer, mary j. blige, janet jackson, anita baker and rachelle ferrell.\n",
      "Main Passage Index: 38\n",
      "Score: 37.86869812011719\n",
      "\n",
      "Passage#2: beyonce's work has influenced numerous artists including adele, ariana grande, lady gaga, bridgit mendler, rihanna, kelly rowland, sam smith, meghan trainor, nicole scherzinger, rita ora, zendaya, cheryl cole, jojo, alexis jordan, jessica sanchez, and azealia banks. american indie rock band white rabbits also cited her an inspiration for their third album milk famous ( 2012 ), friend gwyneth paltrow studied beyonce at her live concerts while learning to become a musical performer for the 2010 film country strong. nicki minaj has stated that seeing beyonce's pepsi commercial influenced her decision to appear in the company's 2012 global campaign.\n",
      "Main Passage Index: 51\n",
      "Score: 36.46063995361328\n",
      "\n",
      "Passage#3: a self - described \" modern - day feminist \", beyonce creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. on stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with destiny's child, making her one of the best - selling music artists of all time. she has won 20 grammy awards and is the most nominated woman in the award's history. the recording industry association of america recognized her as the top certified artist in america during the 2000s decade. in 2009, billboard named her the top radio songs artist of the decade, the top female artist of the 2000s and their artist of the millennium in 2011. time listed her among the 100 most influential people in the world in 2013\n",
      "Main Passage Index: 2\n",
      "Score: 36.277740478515625\n",
      "\n",
      "Passage#4: beyonce giselle knowles - carter ( / biːˈjɒnseɪ / bee - yon - say ) ( born september 4, 1981 ) is an american singer, songwriter, record producer and actress. born and raised in houston, texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of r & b girl - group destiny's child. managed by her father, mathew knowles, the group became one of the world's best - selling girl groups of all time. their hiatus saw the release of beyonce's debut album, dangerously in love ( 2003 ), which established her as a solo artist worldwide, earned five grammy awards and featured the billboard hot 100 number - one singles \" crazy in love \" and \" baby boy \".\n",
      "Main Passage Index: 0\n",
      "Score: 36.049041748046875\n"
     ]
    }
   ],
   "source": [
    "# test on some randomly selected questions\n",
    "question_idx = random.randint(0, len(questions)-1)\n",
    "query = questions[question_idx][0]['question']\n",
    "gold_passage_idx = questions[question_idx][1]\n",
    "\n",
    "passage = passages[gold_passage_idx]\n",
    "print(f\"Question: {query}\")\n",
    "print(f\"Gold Passage Idx# {gold_passage_idx}: {passage}\")\n",
    "print(f\"Answer: {questions[question_idx][0]['answers'][0]['text']}\")\n",
    "\n",
    "# get topk passages\n",
    "topk_scores, topk_passages, main_passages_idx = retreive_passages(query, k=5)\n",
    "if(gold_passage_idx in main_passages_idx):\n",
    "    print(\"Gold passage found in topk passages!\")\n",
    "\n",
    "print(f\"\\nTopk passages:\")\n",
    "for i in range(len(topk_passages)):\n",
    "    print(f\"\\nPassage#{i}: {topk_passages[i]}\")\n",
    "    print(f\"Main Passage Index: {main_passages_idx[i]}\")\n",
    "    print(f\"Score: {topk_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve passages for every question from validation set and compute topk accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_accuracy(k=5, num_questions=1000): \n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    idxs = random.sample(range(0, len(questions)), num_questions)\n",
    "    pbar = tqdm(idxs, desc=\"Question#\")\n",
    "    for question_idx in pbar:\n",
    "        query = questions[question_idx][0]['question']\n",
    "        gold_passage_idx = questions[question_idx][1]\n",
    "        # get topk passages\n",
    "        topk_scores, topk_passages, main_passages_idx = retreive_passages(query, k)\n",
    "        if(gold_passage_idx in main_passages_idx):\n",
    "            num_correct += 1\n",
    "        num_total += 1    \n",
    "        pbar.set_description(f\"Top-{k} Accuracy: {num_correct/num_total:.3f}\")  \n",
    "\n",
    "    print(f\"Top-{k} accuracy = {num_correct/num_total:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Top-10 Accuracy: 0.800: 100%|██████████| 10000/10000 [06:19<00:00, 26.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-10 accuracy = 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compute_topk_accuracy(k=10, num_questions=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the top-10 retreival accuracy of this DPR model is comparable to BM25. Also note that since we've broken the passges up into smaller fixed size blocks, this might give DPR a slight advantage. Also since we're working with a small number of passages, DPR retreival is faster than BM25, but this isn't typically true at larger scales.\n",
    "\n",
    "Now, let's try passage retrieval with our own made up queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topk passages:\n",
      "\n",
      "Passage#0: in the late 1960s, the term heavy metal was used interchangeably with hard rock, but gradually began to be used to describe music played with even more volume and intensity. while hard rock maintained a bluesy rock and roll identity, including some swing in the back beat and riffs that tended to outline chord progressions in their hooks, heavy metal's riffs often functioned as stand - alone melodies and had no swing in them. heavy metal took on \" darker \" characteristics after black sabbath's breakthrough at the beginning of the 1970s. in the 1980s it developed a number of subgenres, often termed extreme metal, some of which were influenced by hardcore punk, and which further differentiated the two styles. despite this differentiation, hard rock and heavy metal have existed side by side, with bands frequently standing on the boundary of, or crossing between, the genres.\n",
      "Main Passage 6115: In the late 1960s, the term heavy metal was used interchangeably with hard rock, but gradually began to be used to describe music played with even more volume and intensity. While hard rock maintained a bluesy rock and roll identity, including some swing in the back beat and riffs that tended to outline chord progressions in their hooks, heavy metal's riffs often functioned as stand-alone melodies and had no swing in them. Heavy metal took on \"darker\" characteristics after Black Sabbath's breakthrough at the beginning of the 1970s. In the 1980s it developed a number of subgenres, often termed extreme metal, some of which were influenced by hardcore punk, and which further differentiated the two styles. Despite this differentiation, hard rock and heavy metal have existed side by side, with bands frequently standing on the boundary of, or crossing between, the genres.\n",
      "Score: 39.02433776855469\n",
      "\n",
      "Passage#1: the punk generation, began to gain international attention from 1976, culminating in the release of their multi - platinum albums let there be rock ( 1977 ) and highway to hell ( 1979 ). also influenced by a punk ethos were heavy metal bands like motorhead, while judas priest abandoned the remaining elements of the blues in their music, further differentiating the hard rock and heavy metal styles and helping to create the new wave of british heavy metal which was pursued by bands like iron maiden, saxon and venom.\n",
      "Main Passage 6096: From outside the United Kingdom and the United States, the Canadian trio Rush released three distinctively hard rock albums in 1974–75 (Rush, Fly by Night and Caress of Steel) before moving toward a more progressive sound with the 1976 album 2112. The Irish band Thin Lizzy, which had formed in the late 1960s, made their most substantial commercial breakthrough in 1976 with the hard rock album Jailbreak and their worldwide hit \"The Boys Are Back in Town\", which reached number 8 in the UK and number 12 in the US. Their style, consisting of two duelling guitarists often playing leads in harmony, proved itself to be a large influence on later bands. They reached their commercial, and arguably their artistic peak with Black Rose: A Rock Legend (1979). The arrival of Scorpions from Germany marked the geographical expansion of the subgenre. Australian-formed AC/DC, with a stripped back, riff heavy and abrasive style that also appealed to the punk generation, began to gain international attention from 1976, culminating in the release of their multi-platinum albums Let There Be Rock (1977) and Highway to Hell (1979). Also influenced by a punk ethos were heavy metal bands like Motörhead, while Judas Priest abandoned the remaining elements of the blues in their music, further differentiating the hard rock and heavy metal styles and helping to create the New Wave of British Heavy Metal which was pursued by bands like Iron Maiden, Saxon and Venom.\n",
      "Score: 38.09634780883789\n",
      "\n",
      "Passage#2: by the end of the decade a distinct genre of hard rock was emerging with bands like led zeppelin, who mixed the music of early rock bands with a more hard - edged form of blues rock and acid rock on their first two albums led zeppelin ( 1969 ) and led zeppelin ii ( 1969 ), and deep purple, who began as a progressive rock group but achieved their commercial breakthrough with their fourth and distinctively heavier album, in rock ( 1970 ). also significant was black sabbath's paranoid ( 1970 ), which combined guitar riffs with dissonance and more explicit references to the occult and elements of gothic horror. all three of these bands have been seen as pivotal in the development of heavy metal, but where metal further accentuated the intensity of the music, with bands like judas priest following sabbath's lead into territory that was often \" darker and more menacing \", hard rock tended to continue to remain the more exuberant, good - time music.\n",
      "Main Passage 6116: By the end of the decade a distinct genre of hard rock was emerging with bands like Led Zeppelin, who mixed the music of early rock bands with a more hard-edged form of blues rock and acid rock on their first two albums Led Zeppelin (1969) and Led Zeppelin II (1969), and Deep Purple, who began as a progressive rock group but achieved their commercial breakthrough with their fourth and distinctively heavier album, In Rock (1970). Also significant was Black Sabbath's Paranoid (1970), which combined guitar riffs with dissonance and more explicit references to the occult and elements of Gothic horror. All three of these bands have been seen as pivotal in the development of heavy metal, but where metal further accentuated the intensity of the music, with bands like Judas Priest following Sabbath's lead into territory that was often \"darker and more menacing\", hard rock tended to continue to remain the more exuberant, good-time music.\n",
      "Score: 38.00166320800781\n",
      "\n",
      "Passage#3: hard rock developed into a major form of popular music in the 1970s, with bands such as led zeppelin, the who, deep purple, aerosmith, ac / dc and van halen. during the 1980s, some hard rock bands moved away from their hard rock roots and more towards pop rock, while others began to return to a hard rock sound. established bands made a comeback in the mid - 1980s and it reached a commercial peak in the 1980s, with glam metal bands like bon jovi and def leppard and the rawer sounds of guns n'roses, which followed up with great success in the later part of that decade. hard rock began losing popularity with the commercial success of grunge and later britpop in the 1990s.\n",
      "Main Passage 6099: Hard rock developed into a major form of popular music in the 1970s, with bands such as Led Zeppelin, The Who, Deep Purple, Aerosmith, AC/DC and Van Halen. During the 1980s, some hard rock bands moved away from their hard rock roots and more towards pop rock, while others began to return to a hard rock sound. Established bands made a comeback in the mid-1980s and it reached a commercial peak in the 1980s, with glam metal bands like Bon Jovi and Def Leppard and the rawer sounds of Guns N' Roses, which followed up with great success in the later part of that decade. Hard rock began losing popularity with the commercial success of grunge and later Britpop in the 1990s.\n",
      "Score: 37.30837631225586\n",
      "\n",
      "Passage#4: pop rock, while others, including rush with moving pictures ( 1981 ), began to return to a hard rock sound. the creation of thrash metal, which mixed heavy metal with elements of hardcore punk from about 1982, particularly by metallica, anthrax, megadeth and slayer, helped to create extreme metal and further remove the style from hard rock, although a number of these bands or their members would continue to record some songs closer to a hard rock sound. kiss moved away from their hard rock roots toward pop metal : firstly removing their makeup in 1983 for their lick it up album, and then adopting the visual and sound of glam metal for their 1984 release, animalize, both of which marked a return to commercial success. pat benatar was one of the first women to achieve commercial success in hard rock, with three successive top 5 albums between 1980 and 1982.\n",
      "Main Passage 6102: The opening years of the 1980s saw a number of changes in personnel and direction of established hard rock acts, including the deaths of Bon Scott, the lead singer of AC/DC, and John Bonham, drummer with Led Zeppelin. Whereas Zeppelin broke up almost immediately afterwards, AC/DC pressed on, recording the album Back in Black (1980) with their new lead singer, Brian Johnson. It became the fifth-highest-selling album of all time in the US and the second-highest-selling album in the world. Black Sabbath had split with original singer Ozzy Osbourne in 1979 and replaced him with Ronnie James Dio, formerly of Rainbow, giving the band a new sound and a period of creativity and popularity beginning with Heaven and Hell (1980). Osbourne embarked on a solo career with Blizzard of Ozz (1980), featuring American guitarist Randy Rhoads. Some bands, such as Queen, moved away from their hard rock roots and more towards pop rock, while others, including Rush with Moving Pictures (1981), began to return to a hard rock sound. The creation of thrash metal, which mixed heavy metal with elements of hardcore punk from about 1982, particularly by Metallica, Anthrax, Megadeth and Slayer, helped to create extreme metal and further remove the style from hard rock, although a number of these bands or their members would continue to record some songs closer to a hard rock sound. Kiss moved away from their hard rock roots toward pop metal: firstly removing their makeup in 1983 for their Lick It Up album, and then adopting the visual and sound of glam metal for their 1984 release, Animalize, both of which marked a return to commercial success. Pat Benatar was one of the first women to achieve commercial success in hard rock, with three successive Top 5 albums between 1980 and 1982.\n",
      "Score: 34.214839935302734\n",
      "\n",
      "Passage#5: of the sleaze metal movement in sweden, including vains of jenna, hardcore superstar and crashdiet.\n",
      "Main Passage 6104: The term \"retro-metal\" has been applied to such bands as Texas based The Sword, California's High on Fire, Sweden's Witchcraft and Australia's Wolfmother. Wolfmother's self-titled 2005 debut album combined elements of the sounds of Deep Purple and Led Zeppelin. Fellow Australians Airbourne's début album Runnin' Wild (2007) followed in the hard riffing tradition of AC/DC. England's The Darkness' Permission to Land (2003), described as an \"eerily realistic simulation of '80s metal and '70s glam\", topped the UK charts, going quintuple platinum. The follow-up, One Way Ticket to Hell... and Back (2005), reached number 11, before the band broke up in 2006. Los Angeles band Steel Panther managed to gain a following by sending up 80s glam metal. A more serious attempt to revive glam metal was made by bands of the sleaze metal movement in Sweden, including Vains of Jenna, Hardcore Superstar and Crashdïet.\n",
      "Score: 33.39532470703125\n",
      "\n",
      "Passage#6: often categorised with the new wave of british heavy metal, in 1981 def leppard released their second album high'n'dry, mixing glam - rock with heavy metal, and helping to define the sound of hard rock for the decade. the follow - up pyromania ( 1983 ), reached number two on the american charts and the singles \" photograph \", \" rock of ages \" and \" foolin'\", helped by the emergence of mtv, all reached the top 40. it was widely emulated, particularly by the emerging californian glam metal scene. this was followed by us acts like motley crue, with their albums too fast for love ( 1981 ) and shout at the devil ( 1983 ) and, as the style grew, the arrival of bands such as ratt, white lion, twisted sister and quiet riot. quiet riot's album metal health ( 1983 ) was the first glam metal album, and\n",
      "Main Passage 6107: Often categorised with the New Wave of British Heavy Metal, in 1981 Def Leppard released their second album High 'n' Dry, mixing glam-rock with heavy metal, and helping to define the sound of hard rock for the decade. The follow-up Pyromania (1983), reached number two on the American charts and the singles \"Photograph\", \"Rock of Ages\" and \"Foolin'\", helped by the emergence of MTV, all reached the Top 40. It was widely emulated, particularly by the emerging Californian glam metal scene. This was followed by US acts like Mötley Crüe, with their albums Too Fast for Love (1981) and Shout at the Devil (1983) and, as the style grew, the arrival of bands such as Ratt, White Lion, Twisted Sister and Quiet Riot. Quiet Riot's album Metal Health (1983) was the first glam metal album, and arguably the first heavy metal album of any kind, to reach number one in the Billboard music charts and helped open the doors for mainstream success by subsequent bands.\n",
      "Score: 33.29393005371094\n",
      "\n",
      "Passage#7: at the turn of the 21st century, a post - punk revival developed in british and american alternative and indie rock, which soon started appearing in other countries, as well. the earliest sign of a revival was the emergence of various underground bands in the mid -'90s. however, the first commercially successful bands – the strokes, franz ferdinand, interpol, neils children and editors – surfaced in the late 1990s to early 2000s, as did several dance - oriented bands such as the rapture, radio 4 and lcd soundsystem. additionally, some darker post - punk bands began to appear in the indie music scene in the 2010s, including cold cave, she wants revenge, eagulls, the soft moon, she past away and light asylum, who were also affiliated with the darkwave revival, as well as a place to bury strangers, who combined early post - punk and shoegaze. these bands tend to draw a fanbase who are a combination of the\n",
      "Main Passage 14862: At the turn of the 21st century, a post-punk revival developed in British and American alternative and indie rock, which soon started appearing in other countries, as well. The earliest sign of a revival was the emergence of various underground bands in the mid-'90s. However, the first commercially successful bands – the Strokes, Franz Ferdinand, Interpol, Neils Children and Editors – surfaced in the late 1990s to early 2000s, as did several dance-oriented bands such as the Rapture, Radio 4 and LCD Soundsystem. Additionally, some darker post-punk bands began to appear in the indie music scene in the 2010s, including Cold Cave, She Wants Revenge, Eagulls, the Soft Moon, She Past Away and Light Asylum, who were also affiliated with the darkwave revival, as well as A Place to Bury Strangers, who combined early post-punk and shoegaze. These bands tend to draw a fanbase who are a combination of the indie music subculture, older post-punk fans and the current goth subculture. In the 2010s, Savages played a music reminiscent of early British post-punk bands of the late '70s.\n",
      "Score: 33.11216735839844\n",
      "\n",
      "Passage#8: with the rise of disco in the us and punk rock in the uk, hard rock's mainstream dominance was rivalled toward the later part of the decade. disco appealed to a more diverse group of people and punk seemed to take over the rebellious role that hard rock once held. early punk bands like the ramones explicitly rebelled against the drum solos and extended guitar solos that characterised stadium rock, with almost all of their songs clocking in around two minutes with no guitar solos. however, new rock acts continued to emerge and record sales remained high into the 1980s. 1977 saw the debut and rise to stardom of foreigner, who went on to release several platinum albums through to the mid - 1980s. midwestern groups like kansas, reo speedwagon and styx helped further cement heavy rock in the midwest as a form of stadium rock. in 1978, van halen emerged from the los angeles music scene with a sound based around the skills of lead guitarist eddie van halen\n",
      "Main Passage 6117: With the rise of disco in the US and punk rock in the UK, hard rock's mainstream dominance was rivalled toward the later part of the decade. Disco appealed to a more diverse group of people and punk seemed to take over the rebellious role that hard rock once held. Early punk bands like The Ramones explicitly rebelled against the drum solos and extended guitar solos that characterised stadium rock, with almost all of their songs clocking in around two minutes with no guitar solos. However, new rock acts continued to emerge and record sales remained high into the 1980s. 1977 saw the début and rise to stardom of Foreigner, who went on to release several platinum albums through to the mid-1980s. Midwestern groups like Kansas, REO Speedwagon and Styx helped further cement heavy rock in the Midwest as a form of stadium rock. In 1978, Van Halen emerged from the Los Angeles music scene with a sound based around the skills of lead guitarist Eddie Van Halen. He popularised a guitar-playing technique of two-handed hammer-ons and pull-offs called tapping, showcased on the song \"Eruption\" from the album Van Halen, which was highly influential in re-establishing hard rock as a popular genre after the punk and disco explosion, while also redefining and elevating the role of electric guitar.\n",
      "Score: 32.858375549316406\n",
      "\n",
      "Passage#9: the term \" retro - metal \" has been applied to such bands as texas based the sword, california's high on fire, sweden's witchcraft and australia's wolfmother. wolfmother's self - titled 2005 debut album combined elements of the sounds of deep purple and led zeppelin. fellow australians airbourne's debut album runnin'wild ( 2007 ) followed in the hard riffing tradition of ac / dc. england's the darkness'permission to land ( 2003 ), described as an \" eerily realistic simulation of'80s metal and'70s glam \", topped the uk charts, going quintuple platinum. the follow - up, one way ticket to hell... and back ( 2005 ), reached number 11, before the band broke up in 2006. los angeles band steel panther managed to gain a following by sending up 80s glam metal. a more serious attempt to revive glam metal was made by bands\n",
      "Main Passage 6104: The term \"retro-metal\" has been applied to such bands as Texas based The Sword, California's High on Fire, Sweden's Witchcraft and Australia's Wolfmother. Wolfmother's self-titled 2005 debut album combined elements of the sounds of Deep Purple and Led Zeppelin. Fellow Australians Airbourne's début album Runnin' Wild (2007) followed in the hard riffing tradition of AC/DC. England's The Darkness' Permission to Land (2003), described as an \"eerily realistic simulation of '80s metal and '70s glam\", topped the UK charts, going quintuple platinum. The follow-up, One Way Ticket to Hell... and Back (2005), reached number 11, before the band broke up in 2006. Los Angeles band Steel Panther managed to gain a following by sending up 80s glam metal. A more serious attempt to revive glam metal was made by bands of the sleaze metal movement in Sweden, including Vains of Jenna, Hardcore Superstar and Crashdïet.\n",
      "Score: 32.672393798828125\n",
      "\n",
      "Passage#10: post - punk was an eclectic genre which resulted in a wide variety of musical innovations and helped merge white and black musical styles. out of the post - punk milieu came the beginnings of various subsequent genres, including new wave, dance - rock, new pop, industrial music, synthpop, post - hardcore, neo - psychedelia alternative rock and house music. bands such as joy division, siouxsie and the banshees, bauhaus and the cure played in a darker, more morose style of post - punk that lead to the development of the gothic rock genre.\n",
      "Main Passage 14861: Post-punk was an eclectic genre which resulted in a wide variety of musical innovations and helped merge white and black musical styles. Out of the post-punk milieu came the beginnings of various subsequent genres, including new wave, dance-rock, New Pop, industrial music, synthpop, post-hardcore, neo-psychedelia alternative rock and house music. Bands such as Joy Division, Siouxsie and the Banshees, Bauhaus and the Cure played in a darker, more morose style of post-punk that lead to the development of the gothic rock genre.\n",
      "Score: 32.6144905090332\n",
      "\n",
      "Passage#11: although foo fighters continued to be one of the most successful rock acts, with albums like in your honor ( 2005 ) reaching number two in the us and uk, many of the first wave of post - grunge bands began to fade in popularity. acts like creed, staind, puddle of mudd and nickelback took the genre into the 2000s with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs. they were followed in this vein by new acts including shinedown and seether. acts with more conventional hard rock sounds included andrew w. k., beautiful creatures and buckcherry, whose breakthrough album 15 ( 2006 ) went platinum and spawned the single \" sorry \" ( 2007 ), which made the top 10 of the billboard 100. these were joined by bands with hard rock leanings that emerged in the mid - 2000s from the garage rock or post punk revival, including black rebel\n",
      "Main Passage 6109: Although Foo Fighters continued to be one of the most successful rock acts, with albums like In Your Honor (2005) reaching number two in the US and UK, many of the first wave of post-grunge bands began to fade in popularity. Acts like Creed, Staind, Puddle of Mudd and Nickelback took the genre into the 2000s with considerable commercial success, abandoning most of the angst and anger of the original movement for more conventional anthems, narratives and romantic songs. They were followed in this vein by new acts including Shinedown and Seether. Acts with more conventional hard rock sounds included Andrew W.K., Beautiful Creatures and Buckcherry, whose breakthrough album 15 (2006) went platinum and spawned the single \"Sorry\" (2007), which made the Top 10 of the Billboard 100. These were joined by bands with hard rock leanings that emerged in the mid-2000s from the garage rock or post punk revival, including Black Rebel Motorcycle Club and Kings of Leon, and Queens of the Stone Age from the US, Three Days Grace from Canada, Jet from Australia and The Datsuns from New Zealand. In 2009 Them Crooked Vultures, a supergroup that brought together Foo Fighters' Dave Grohl, Queens of the Stone Age's Josh Homme and Led Zeppelin bass player John Paul Jones attracted attention as a live act and released a self-titled debut album that reached the top 20 in the US and UK and the top ten in several other countries.\n",
      "Score: 32.323123931884766\n",
      "\n",
      "Passage#12: 10 singles on the uk singles chart. metallica's load ( 1996 ) and reload ( 1997 ) each sold in excess of 4 million copies in the us and saw the band develop a more melodic and blues rock sound. as the initial impetus of grunge bands faltered in the middle years of the decade, post - grunge bands emerged. they emulated the attitudes and music of grunge, particularly thick, distorted guitars, but with a more radio - friendly commercially oriented sound that drew more directly on traditional hard rock. among the most successful acts were the foo fighters, candlebox, live, collective soul, australia's silverchair and england's bush, who all cemented post - grunge as one of the most commercially viable subgenres by the late 1990s. similarly, some post - britpop bands that followed in the wake of oasis, including feeder and stereophonics, adopted a hard rock or \" pop -\n",
      "Main Passage 6098: Some established acts continued to enjoy commercial success, such as Aerosmith, with their number one multi-platinum albums: Get a Grip (1993), which produced four Top 40 singles and became the band's best-selling album worldwide (going on to sell over 10 million copies), and Nine Lives (1997). In 1998, Aerosmith released the number one hit \"I Don't Want to Miss a Thing\", which remains the only single by a hard rock band to debut at number one. AC/DC produced the double platinum Ballbreaker (1995). Bon Jovi appealed to their hard rock audience with songs such as \"Keep the Faith\" (1992), but also achieved success in adult contemporary radio, with the Top 10 ballads \"Bed of Roses\" (1993) and \"Always\" (1994). Bon Jovi's 1995 album These Days was a bigger hit in Europe than it was in the United States, spawning four Top 10 singles on the UK Singles Chart. Metallica's Load (1996) and ReLoad (1997) each sold in excess of 4 million copies in the US and saw the band develop a more melodic and blues rock sound. As the initial impetus of grunge bands faltered in the middle years of the decade, post-grunge bands emerged. They emulated the attitudes and music of grunge, particularly thick, distorted guitars, but with a more radio-friendly commercially oriented sound that drew more directly on traditional hard rock. Among the most successful acts were the Foo Fighters, Candlebox, Live, Collective Soul, Australia's Silverchair and England's Bush, who all cemented post-grunge as one of the most commercially viable subgenres by the late 1990s. Similarly, some post-Britpop bands that followed in the wake of Oasis, including Feeder and Stereophonics, adopted a hard rock or \"pop-metal\" sound.\n",
      "Score: 32.101318359375\n",
      "\n",
      "Passage#13: from outside the united kingdom and the united states, the canadian trio rush released three distinctively hard rock albums in 1974 – 75 ( rush, fly by night and caress of steel ) before moving toward a more progressive sound with the 1976 album 2112. the irish band thin lizzy, which had formed in the late 1960s, made their most substantial commercial breakthrough in 1976 with the hard rock album jailbreak and their worldwide hit \" the boys are back in town \", which reached number 8 in the uk and number 12 in the us. their style, consisting of two duelling guitarists often playing leads in harmony, proved itself to be a large influence on later bands. they reached their commercial, and arguably their artistic peak with black rose : a rock legend ( 1979 ). the arrival of scorpions from germany marked the geographical expansion of the subgenre. australian - formed ac / dc, with a stripped back, riff heavy and abrasive style that also appealed to\n",
      "Main Passage 6096: From outside the United Kingdom and the United States, the Canadian trio Rush released three distinctively hard rock albums in 1974–75 (Rush, Fly by Night and Caress of Steel) before moving toward a more progressive sound with the 1976 album 2112. The Irish band Thin Lizzy, which had formed in the late 1960s, made their most substantial commercial breakthrough in 1976 with the hard rock album Jailbreak and their worldwide hit \"The Boys Are Back in Town\", which reached number 8 in the UK and number 12 in the US. Their style, consisting of two duelling guitarists often playing leads in harmony, proved itself to be a large influence on later bands. They reached their commercial, and arguably their artistic peak with Black Rose: A Rock Legend (1979). The arrival of Scorpions from Germany marked the geographical expansion of the subgenre. Australian-formed AC/DC, with a stripped back, riff heavy and abrasive style that also appealed to the punk generation, began to gain international attention from 1976, culminating in the release of their multi-platinum albums Let There Be Rock (1977) and Highway to Hell (1979). Also influenced by a punk ethos were heavy metal bands like Motörhead, while Judas Priest abandoned the remaining elements of the blues in their music, further differentiating the hard rock and heavy metal styles and helping to create the New Wave of British Heavy Metal which was pursued by bands like Iron Maiden, Saxon and Venom.\n",
      "Score: 31.980806350708008\n",
      "\n",
      "Passage#14: indie music subculture, older post - punk fans and the current goth subculture. in the 2010s, savages played a music reminiscent of early british post - punk bands of the late'70s.\n",
      "Main Passage 14862: At the turn of the 21st century, a post-punk revival developed in British and American alternative and indie rock, which soon started appearing in other countries, as well. The earliest sign of a revival was the emergence of various underground bands in the mid-'90s. However, the first commercially successful bands – the Strokes, Franz Ferdinand, Interpol, Neils Children and Editors – surfaced in the late 1990s to early 2000s, as did several dance-oriented bands such as the Rapture, Radio 4 and LCD Soundsystem. Additionally, some darker post-punk bands began to appear in the indie music scene in the 2010s, including Cold Cave, She Wants Revenge, Eagulls, the Soft Moon, She Past Away and Light Asylum, who were also affiliated with the darkwave revival, as well as A Place to Bury Strangers, who combined early post-punk and shoegaze. These bands tend to draw a fanbase who are a combination of the indie music subculture, older post-punk fans and the current goth subculture. In the 2010s, Savages played a music reminiscent of early British post-punk bands of the late '70s.\n",
      "Score: 31.553478240966797\n",
      "\n",
      "Passage#15: since the mid - 2000s, the mainstreaming of bands like wilco and feist have pushed indie rock into the adult contemporary conversation. in the early 2010s, indie musicians like imagine dragons, mumford & sons, of monsters & men, the lumineers and ed sheeran also had indie songs that crossed over to the adult contemporary charts.\n",
      "Main Passage 3158: Since the mid-2000s, the mainstreaming of bands like Wilco and Feist have pushed indie rock into the adult contemporary conversation. In the early 2010s, indie musicians like Imagine Dragons, Mumford & Sons, Of Monsters & Men, The Lumineers and Ed Sheeran also had indie songs that crossed over to the adult contemporary charts.\n",
      "Score: 31.397092819213867\n",
      "\n",
      "Passage#16: in germany, groups such as einsturzende neubauten developed a unique style of industrial music, utilizing avant - garde noise, homemade instruments and found objects. members of that group would later go on to collaborate with members of the birthday party. in brazil, the post - punk scene grew after the generation of brasilia rock with bands such as legiao urbana, capital inicial and plebe rude and then the opening of the music club madame sata in sao paulo, with acts like cabine c, titas, patife band, fellini and mercenarias, as documented on compilations like the sexual life of the savages and the nao wave / nao sao paulo series, released in the uk, germany and brazil, respectively. [ citation needed ]\n",
      "Main Passage 14858: In Germany, groups such as Einstürzende Neubauten developed a unique style of industrial music, utilizing avant-garde noise, homemade instruments and found objects. Members of that group would later go on to collaborate with members of the Birthday Party. In Brazil, the post-punk scene grew after the generation of Brasilia rock with bands such as Legião Urbana, Capital Inicial and Plebe Rude and then the opening of the music club Madame Satã in São Paulo, with acts like Cabine C, Titãs, Patife Band, Fellini and Mercenárias, as documented on compilations like The Sexual Life of the Savages and the Não Wave/Não São Paulo series, released in the UK, Germany and Brazil, respectively.[citation needed]\n",
      "Score: 31.166336059570312\n",
      "\n",
      "Passage#17: while these few hard rock bands managed to maintain success and popularity in the early part of the decade, alternative forms of hard rock achieved mainstream success in the form of grunge in the us and britpop in the uk. this was particularly evident after the success of nirvana's nevermind ( 1991 ), which combined elements of hardcore punk and heavy metal into a \" dirty \" sound that made use of heavy guitar distortion, fuzz and feedback, along with darker lyrical themes than their \" hair band \" predecessors. although most grunge bands had a sound that sharply contrasted mainstream hard rock, several, including pearl jam, alice in chains, mother love bone and soundgarden, were more strongly influenced by 1970s and 1980s rock and metal, while stone temple pilots managed to turn alternative rock into a form of stadium rock. however, all grunge bands shunned the macho, anthemic and fashion - focused aesthetics particularly associated with glam metal. in\n",
      "Main Passage 6108: While these few hard rock bands managed to maintain success and popularity in the early part of the decade, alternative forms of hard rock achieved mainstream success in the form of grunge in the US and Britpop in the UK. This was particularly evident after the success of Nirvana's Nevermind (1991), which combined elements of hardcore punk and heavy metal into a \"dirty\" sound that made use of heavy guitar distortion, fuzz and feedback, along with darker lyrical themes than their \"hair band\" predecessors. Although most grunge bands had a sound that sharply contrasted mainstream hard rock, several, including Pearl Jam, Alice in Chains, Mother Love Bone and Soundgarden, were more strongly influenced by 1970s and 1980s rock and metal, while Stone Temple Pilots managed to turn alternative rock into a form of stadium rock. However, all grunge bands shunned the macho, anthemic and fashion-focused aesthetics particularly associated with glam metal. In the UK, Oasis were unusual among the Britpop bands of the mid-1990s in incorporating a hard rock sound.\n",
      "Score: 30.978389739990234\n",
      "\n",
      "Passage#18: arguably the first heavy metal album of any kind, to reach number one in the billboard music charts and helped open the doors for mainstream success by subsequent bands.\n",
      "Main Passage 6107: Often categorised with the New Wave of British Heavy Metal, in 1981 Def Leppard released their second album High 'n' Dry, mixing glam-rock with heavy metal, and helping to define the sound of hard rock for the decade. The follow-up Pyromania (1983), reached number two on the American charts and the singles \"Photograph\", \"Rock of Ages\" and \"Foolin'\", helped by the emergence of MTV, all reached the Top 40. It was widely emulated, particularly by the emerging Californian glam metal scene. This was followed by US acts like Mötley Crüe, with their albums Too Fast for Love (1981) and Shout at the Devil (1983) and, as the style grew, the arrival of bands such as Ratt, White Lion, Twisted Sister and Quiet Riot. Quiet Riot's album Metal Health (1983) was the first glam metal album, and arguably the first heavy metal album of any kind, to reach number one in the Billboard music charts and helped open the doors for mainstream success by subsequent bands.\n",
      "Score: 30.95464515686035\n",
      "\n",
      "Passage#19: as of 2008 [ update ], germany is the fourth largest music market in the world and has exerted a strong influence on dance and rock music, and pioneered trance music. artists such as herbert gronemeyer, scorpions, rammstein, nena, dieter bohlen, tokio hotel and modern talking have enjoyed international fame. german musicians and, particularly, the pioneering bands tangerine dream and kraftwerk have also contributed to the development of electronic music. germany hosts many large rock music festivals annually. the rock am ring festival is the largest music festival in germany, and among the largest in the world. german artists also make up a large percentage of industrial music acts, which is called neue deutsche harte. germany hosts some of the largest goth scenes and festivals in the entire world, with events like wave - gothic - treffen and m'era luna festival easily attracting up to 30, 000 people. amongst germany's famous artists there are various\n",
      "Main Passage 8938: As of 2008[update], Germany is the fourth largest music market in the world and has exerted a strong influence on Dance and Rock music, and pioneered trance music. Artists such as Herbert Grönemeyer, Scorpions, Rammstein, Nena, Dieter Bohlen, Tokio Hotel and Modern Talking have enjoyed international fame. German musicians and, particularly, the pioneering bands Tangerine Dream and Kraftwerk have also contributed to the development of electronic music. Germany hosts many large rock music festivals annually. The Rock am Ring festival is the largest music festival in Germany, and among the largest in the world. German artists also make up a large percentage of Industrial music acts, which is called Neue Deutsche Härte. Germany hosts some of the largest Goth scenes and festivals in the entire world, with events like Wave-Gothic-Treffen and M'era Luna Festival easily attracting up to 30,000 people. Amongst Germany's famous artists there are various Dutch entertainers, such as Johannes Heesters.\n",
      "Score: 30.65599822998047\n"
     ]
    }
   ],
   "source": [
    "query = \"Which decade did the heavy metal genre emerge?\"\n",
    "topk_scores, topk_passages, main_passages_idx = retreive_passages(query, k=20)\n",
    "\n",
    "print(f\"\\nTopk passages:\")\n",
    "for i in range(len(topk_passages)):\n",
    "    print(f\"\\nPassage#{i}: {topk_passages[i]}\")\n",
    "    print(f\"Main Passage {main_passages_idx[i]}: {passages[main_passages_idx[i]]}\")\n",
    "    print(f\"Score: {topk_scores[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
