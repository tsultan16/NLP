{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Passage Retrieval (DPR)\n",
    "\n",
    "We saw how how to use the TFIDF representation for passages to perform retreival. One major problem with this kind of sparse vector representation is that if the query words don't exactly match any words from the relavant passages, then the retreival system will not be able to find those passages (because of zero cosine similarity between the query and passage vectors). \n",
    "\n",
    "In DPR, we instead have `bi-encoders`, i.e. two separate BERT networks, a `query encoder` and a `pasage encoder`, which learn to map queries and passages respectively into a dense vector space in which the similarity between a query vector and it's corresponding relevant passage(s) is maximized. We use the output for the `[CLS]` token from each encoder as the dense vector representation. \n",
    "\n",
    "The bi-encoders are jointly trained using a supervised classification task where each input instance is a tuple $(q_i, p_i^{+}, p_{i,1}^{-}, ...,p_{i,n}^{-})$ where $q_i$ is a query, $p_i^{+}$ is a rlevant/positive passage and each of the $n$ $p_{i,j}^{-}$ are irrelevant/negative documents. Then we use the query encoder to compute the dense vector representation for the query $E_{Q}(q)$ and use the passage encoder for all the passages $E_P(p)$. Then we compute similarity scores between the query vector and each passage vector: $sim(q_i, p)$ for $p \\in \\{p_i^{+}, p_{i,1}^{-}, ...,p_{i,n}^{-}\\}$. We can interpret these similarity scores as unnormalized logits for $(n+1)$ different class labels. With this interpretaion, we can define $sim(q_i, p_i^{+})$ as the logit for the \"correct\\ground truth class\" and then simply use the `softmax cross-entropy/negative log-likelihood loss` function:\n",
    "\n",
    "$L(q_i, p_i^{+}, p_{i,1}^{-}, ...,p_{i,n}^{-}) = -\\log \\frac{exp(sim(q_i, p_i^{+}))}{exp(sim(q_i, p_i^{+})) + \\sum_{j=1}^n exp(sim(q_i, p_{i,j}^{-}))}$\n",
    "\n",
    "Note that $[exp(sim(q_i, p_i^{+}), exp(sim(q_i, p_{i,1}^{-}),..., exp(sim(q_i, p_{i,n}^{-})]$ represents a probability distrbution and minimizing the loss function pushes $exp(sim(q_i, p_i^{+}))$ towards 1 and pushes the $exp(sim(q_i, p_{i,j}^{-}))$ towards zero, which allows us to achieve the dense vector space in which a query vector is maximally similar to the positive passage vector and dis-similar to the negative passages. We also use the simple `dot product` as our similarity metric.\n",
    "\n",
    "For the SQuAD dataset, we already have given question, context passage pairs. Now we need to somehow choose negative passages for each pair. For training efficieny, we can use a simple trick. Given that we have a minibatch of $B$ such (question, context passage) pairs, then for each pair, we can simply just assign the passages from the other $B-1$ pairs as the negatives. Then we can compute the pair-wise dot product between every question-passgae pair with a single matrix multiplication. So given a matrix $Q$ of shape $(B,d)$ containing the batch of query vectors (where $d$ is the hidden dimensions of the encoded vectors) and a matrix $P$ of the same shape containing the batch of passages, we can compute the matrix $QP^T$ whose $(i,j)th$ entry given us the dot product between the ith question and the jth passage. So the $ith$ diagonal entry in this matrix is the dot product between $ith$ question and its corresponding positive passage and all other elements from that row are dot products with the negative passages. Then by taking the softmax of each row of this matrix, we can compute the total loss for the batch by just summing up the negative log of the terms along the diagonal. In addition to training efficiency, the other huge advantage of this technique is that the dataset will be shuffled before each epoch so that each question-positive passage pair will always get a different sample set of negative passages and therefore we effectively get a very large set of negatives per pair.\n",
    "\n",
    "We wil use two MobileBERT models for our bi-encoders (two BERTs probably won't fit on my GPU and MobileBERT is less than half the size and performs just as well as BERT anyway).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, MobileBertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define our Bi-encoder model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBiEncoder(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.query_encoder = MobileBertModel.from_pretrained('google/mobilebert-uncased')\n",
    "        self.passage_encoder = MobileBertModel.from_pretrained('google/mobilebert-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        for param in self.query_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        for param in self.passage_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, query_idx, query_attn_mask, passage_idx, passage_attn_mask):\n",
    "        # compute BERT encodings\n",
    "        query_output = self.query_encoder(query_idx, attention_mask=query_attn_mask)\n",
    "        passage_output = self.passage_encoder(passage_idx, attention_mask=passage_attn_mask)\n",
    "        # extract the `[CLS]` encoding (first element of the sequence), apply dropout\n",
    "        query_enc = self.dropout(query_output.last_hidden_state[:, 0]) # shape: (batch_size, hidden_size)\n",
    "        passage_enc = self.dropout(passage_output.last_hidden_state[:,0]) # shape: (batch_size, hidden_size)\n",
    "        # compute similarity score matrix\n",
    "        scores = torch.mm(query_enc, passage_enc.transpose(0, 1)) # shape: (batch_size, batch_size)\n",
    "        # take row-wise softmax\n",
    "        scores = F.softmax(scores, dim=1) # shape: (batch_size, batch_size)\n",
    "        # compute negtive log likelihood loss\n",
    "        loss = -torch.log(scores.diag()).mean()\n",
    "    \n",
    "        return scores, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = batch\n",
    "            # move batch to device\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = query_idx.to(device), query_attn_mask.to(device), passage_idx.to(device), passage_attn_mask.to(device)\n",
    "            # forward pass\n",
    "            scores, loss = model(query_idx, query_attn_mask, passage_idx, passage_attn_mask )\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = query_idx.shape\n",
    "            y_pred = scores.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            targets = torch.arange(B).to(device) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = batch\n",
    "            query_idx, query_attn_mask, passage_idx, passage_attn_mask = query_idx.to(device), query_attn_mask.to(device), passage_idx.to(device), passage_attn_mask.to(device)\n",
    "            scores, loss = model(query_idx, query_attn_mask, passage_idx, passage_attn_mask )\n",
    "            B, _ = query_idx.shape\n",
    "            y_pred = scores.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            targets = torch.arange(B).to(device) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'dpr_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('dpr_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 1190.82 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load up the SQuAD v1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 19035\n",
      "Number of questions: 86821\n",
      "Number of passages: 1204\n",
      "Number of questions: 5928\n"
     ]
    }
   ],
   "source": [
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file) \n",
    "\n",
    "def get_passages(squad, num_titles=None):\n",
    "    if num_titles is None:\n",
    "        num_titles = len(squad['data'])\n",
    "    # for each title, get passages and all corresponding questions from SQuAD train set\n",
    "    passages = []\n",
    "    questions = []\n",
    "    num_questions = 0\n",
    "    j = 0\n",
    "    for i in range(num_titles):\n",
    "        #print(f\"Title# {i}: {squad['data'][i]['title']}, Number of passages: {len(squad['data'][i]['paragraphs'])}\")\n",
    "        for p in squad['data'][i]['paragraphs']:\n",
    "            passages.append(p['context'])\n",
    "            for q in p['qas']:\n",
    "                if not q['is_impossible']:\n",
    "                    questions.append((q,j))    \n",
    "                    num_questions += 1\n",
    "            j += 1\n",
    "    print(f\"Number of passages: {len(passages)}\")\n",
    "    print(f\"Number of questions: {num_questions}\")\n",
    "    return passages, questions\n",
    "\n",
    "passages_train, questions_train = get_passages(squad_train)\n",
    "passages_val, questions_val = get_passages(squad_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this because huggingface tokenizer is not thread safe when using it inside __getitem__ instead of dataloader collatefunction\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  #\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, passages, questions, block_size=128):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the question and context passage\n",
    "        q = self.questions[idx][0]\n",
    "        passage_idx = self.questions[idx][1]\n",
    "        question = q['question']\n",
    "        passage = self.passages[passage_idx]\n",
    "        # tokenize the context passage\n",
    "        passage_encoding = self.tokenizer.encode_plus(passage, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        passage_idx = passage_encoding['input_ids']\n",
    "        # tokenize the question\n",
    "        question_encoding = self.tokenizer.encode_plus(question, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        question_idx = question_encoding['input_ids']\n",
    "\n",
    "        # get answer span start and end character positions, for multiple answers, we will only use the first answer\n",
    "        first_answer_idx = 0\n",
    "        answer_start_char = q['answers'][first_answer_idx]['answer_start']\n",
    "        answer_end_char = answer_start_char + len(q['answers'][first_answer_idx]['text'])\n",
    "        # convert char positions to token positions\n",
    "        answer_start_token = passage_encoding.char_to_token(answer_start_char)\n",
    "        answer_end_token = passage_encoding.char_to_token(answer_end_char-1)\n",
    "\n",
    "        # select a window size so that the passage sequence will be no longer than the block size\n",
    "        window_size_tokens = self.block_size - 2 # 2 special tokens ([CLS], [SEP])\n",
    "        # now create a window around the answer span, pick the window start position randomly\n",
    "        window_start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "        window_start_max = min(answer_start_token, max(0,len(passage_idx) - window_size_tokens)) # we want to make the window as large as possible, but not go over the end of the context\n",
    "        window_start = random.randint(window_start_min, window_start_max)\n",
    "        window_end = window_start + window_size_tokens        \n",
    "        # select window of passage tokens\n",
    "        passage_window_tokens = passage_idx[window_start:window_end]\n",
    "\n",
    "        # create padded query and passage sequences\n",
    "        question_idx = [self.tokenizer.cls_token_id] + question_idx + [self.tokenizer.sep_token_id]\n",
    "        question_idx = question_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(question_idx))\n",
    "        passage_idx = [self.tokenizer.cls_token_id] + passage_window_tokens + [self.tokenizer.sep_token_id]\n",
    "        passage_idx = passage_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(passage_idx))\n",
    "\n",
    "        # make sure the passage sequence and query sequences are not longer than max_length\n",
    "        if len(question_idx) > self.block_size or len(passage_idx) > self.block_size:\n",
    "            raise Exception(f\"Passage sequence length {len(passage_idx)} or question sequence length {len(question_idx)} is longer than max_length {self.block_size}!\")\n",
    "        \n",
    "        # create attention masks\n",
    "        question_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in question_idx]\n",
    "        passage_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in passage_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        question_idx = torch.tensor(question_idx)\n",
    "        question_attn_mask = torch.tensor(question_attn_mask)\n",
    "        passage_idx = torch.tensor(passage_idx)\n",
    "        passage_attn_mask = torch.tensor(passage_attn_mask)\n",
    "\n",
    "        return question_idx, question_attn_mask, passage_idx, passage_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 192\n",
    "train_dataset = SquadDataset(passages_train, questions_train, block_size=block_size)\n",
    "val_dataset = SquadDataset(passages_val, questions_val, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
