{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024 COMP90042 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "#### **BERT Classifier Model**: \n",
    "\n",
    "In this notebook, we implement our main classifier model to perform multi-class classification of encoded $(claim, concatenated\\_relevant\\_evidences)$ pairs into one of the 4 claim classes: `[SUPPORTS, REFUTED, NOT_ENOUGH_INFO, DISPUTED]` by performing softmax classification on the `[CLS]` embedding. \n",
    "\n",
    "Our classifier model consists of a classificaion head (linear layer) attached on top of our pre-trained custom BERT model initialized from pre-training checkpoint.\n",
    "\n",
    "*** **PLEASE NOTE**: We import helper functions that we implemented for pre-processing/cleaning our data from the python script called `utils.py`. Our custom BERT model implementation is contained in the python script called `min_bert_multi.py` and pre-trained weights are loaded from a checkpoint file. Our custom WordPiece Tokenizer implementation is contained is the python script `wordpiece_tokenizer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# install required packages\n",
    "!pip install unidecode\n",
    "!python -m nltk.downloader stopwords\n",
    "!pip install wandb\n",
    "\n",
    "from utils import *\n",
    "from wordpiece_tokenizer import *\n",
    "from min_bert_multi import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import wandb\n",
    "import psutil\n",
    "import random\n",
    "\n",
    "#wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.DataSet Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Claims Dataset with Knowledge Source and Clean the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "\n",
      "Number of evidence passages after cleaning: 1206800\n",
      "Number of training instances after cleaning: 1228\n",
      "Number of validation instances after cleaning: 154\n"
     ]
    }
   ],
   "source": [
    "# load dataset and prepare corpus\n",
    "knowledge_source, train_data, val_data = load_dataset()      \n",
    "print(f\"Number of evidence passages: {len(knowledge_source)}\")\n",
    "print(f\"Number of training instances: {len(train_data)}\")  \n",
    "print(f\"Number of validation instances: {len(val_data)}\")\n",
    "\n",
    "# clean all senteneces in the dataset (this involves converting from unicode to asc-ii, removing URLS, removing repeating non-alphanumeric characters, etc. Just a bunch of thing that are most likely will not be useful for claim classification task)\n",
    "cleaner = SentenceCleaner()\n",
    "knowledge_source, train_data, val_data = cleaner.clean_dataset(knowledge_source, train_data, val_data)\n",
    "print(f\"\\nNumber of evidence passages after cleaning: {len(knowledge_source)}\")\n",
    "print(f\"Number of training instances after cleaning: {len(train_data)}\")  \n",
    "print(f\"Number of validation instances after cleaning: {len(val_data)}\")\n",
    "\n",
    "# dictionary for mapping integer to document id \n",
    "int2docID = {i:evidence_id for i,evidence_id in enumerate(list(knowledge_source.keys()))}\n",
    "\n",
    "claim_ids = [claim_id for claim_id in train_data.keys()]\n",
    "\n",
    "# load trained wordpiece tokenizer from file\n",
    "with open('tokenizer_worpiece_20000_aug.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Classification Task Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Dataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, tokenizer, block_size):\n",
    "        self.claims_data = claims_data          \n",
    "        self.document_store = document_store    # document store\n",
    "        self.tokenizer = tokenizer    # wordpiece tokenizer\n",
    "        self.block_size = block_size  # truncation/max length of sentences\n",
    "        self.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "        self.negative_label = 0\n",
    "        self.positive_label = 1\n",
    "        self.claim_label2int = {'SUPPORTS':0, 'REFUTES':1, 'NOT_ENOUGH_INFO':2, 'DISPUTED':3}\n",
    "        self.document_ids = list(document_store.keys())\n",
    "        self.claim_ids = list(claims_data.keys())        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.claim_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim text\n",
    "        claim_id = self.claim_ids[idx]\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        # get concatenated evidence texts\n",
    "        evidence_ids = self.claims_data[claim_id]['evidences']\n",
    "        evidence_texts = [self.document_store[evidence_id] for evidence_id in evidence_ids] \n",
    "        evidence_text = \" \".join(evidence_texts)\n",
    "        # get claim class\n",
    "        claim_label = self.claim_label2int[self.claims_data[claim_id]['claim_label']]\n",
    "\n",
    "        # encode both sentence using the tokenizer\n",
    "        s1_idx = self.tokenizer.encode([claim_text])[0]\n",
    "        s2_idx = self.tokenizer.encode([evidence_text])[0]\n",
    "\n",
    "        # check if combined length is within block_size-2\n",
    "        if len(s1_idx) + len(s2_idx) + 3 > self.block_size:\n",
    "            # calculate the space available for each sentence\n",
    "            available_space = (self.block_size - 3) // 2\n",
    "            if len(s1_idx) < available_space:\n",
    "                # if s1 is shorter than available space, allocate the remaining space to s2\n",
    "                available_space_s2 = self.block_size - 3 - len(s1_idx)\n",
    "                if len(s2_idx) > available_space_s2:\n",
    "                    # if s2 is longer than the available space, crop it\n",
    "                    start = random.randint(0, len(s2_idx) - available_space_s2)\n",
    "                    s2_idx = s2_idx[start:start+available_space_s2]\n",
    "            elif len(s2_idx) < available_space:\n",
    "                # if s2 is shorter than available space, allocate the remaining space to s1\n",
    "                available_space_s1 = self.block_size - 3 - len(s2_idx)\n",
    "                if len(s1_idx) > available_space_s1:\n",
    "                    # if s1 is longer than the available space, crop it\n",
    "                    start = random.randint(0, len(s1_idx) - available_space_s1)\n",
    "                    s1_idx = s1_idx[start:start+available_space_s1]\n",
    "            else:\n",
    "                # if both sentences are longer than available space, crop both\n",
    "                start = random.randint(0, len(s1_idx) - available_space)\n",
    "                s1_idx = s1_idx[start:start+available_space]\n",
    "                start = random.randint(0, len(s2_idx) - available_space)\n",
    "                s2_idx = s2_idx[start:start+available_space]\n",
    "\n",
    "        # combine the two sentences with a separator token into single sequence        \n",
    "        s = torch.cat([torch.tensor([self.tokenizer.cls_token_id()]), torch.tensor(s1_idx, dtype=torch.long), torch.tensor([self.tokenizer.sep_token_id()]), torch.tensor(s2_idx, dtype=torch.long), torch.tensor([self.tokenizer.sep_token_id()])])\n",
    "        # apply padding\n",
    "        pad_len = max(0,self.block_size-len(s))\n",
    "        s = torch.cat([s, torch.full((pad_len,), self.tokenizer.pad_token_id())])\n",
    "        attention_mask = torch.cat([torch.ones(self.block_size-pad_len), torch.zeros(pad_len)])\n",
    "        claim_label = torch.tensor(claim_label)\n",
    "\n",
    "        # create segment ids\n",
    "        segment_ids = torch.zeros(self.block_size)\n",
    "        sep_idx = (s == self.tokenizer.sep_token_id()).nonzero(as_tuple=False)\n",
    "        segment_ids[sep_idx[0]+1:] = 1\n",
    "        segment_ids = segment_ids.long()\n",
    "\n",
    "        return s, attention_mask, segment_ids, claim_label\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.sent_pairs = self.create_pairs()\n",
    "\n",
    "    # function for encoding a custom out of corpus sentence\n",
    "    def encode_custom(self, sent_1, sent_2):\n",
    "        # encode both sentence using the tokenizer\n",
    "        s1_idx = self.tokenizer.encode([sent_1])[0]\n",
    "        s2_idx = self.tokenizer.encode([sent_2])[0]\n",
    "\n",
    "        # check if combined length is within block_size-2\n",
    "        if len(s1_idx) + len(s2_idx) + 3 > self.block_size:\n",
    "            # calculate the space available for each sentence\n",
    "            available_space = (self.block_size - 3) // 2\n",
    "            if len(s1_idx) < available_space:\n",
    "                # if s1 is shorter than available space, allocate the remaining space to s2\n",
    "                available_space_s2 = self.block_size - 3 - len(s1_idx)\n",
    "                if len(s2_idx) > available_space_s2:\n",
    "                    # if s2 is longer than the available space, crop it\n",
    "                    start = random.randint(0, len(s2_idx) - available_space_s2)\n",
    "                    s2_idx = s2_idx[start:start+available_space_s2]\n",
    "            elif len(s2_idx) < available_space:\n",
    "                # if s2 is shorter than available space, allocate the remaining space to s1\n",
    "                available_space_s1 = self.block_size - 3 - len(s2_idx)\n",
    "                if len(s1_idx) > available_space_s1:\n",
    "                    # if s1 is longer than the available space, crop it\n",
    "                    start = random.randint(0, len(s1_idx) - available_space_s1)\n",
    "                    s1_idx = s1_idx[start:start+available_space_s1]\n",
    "            else:\n",
    "                # if both sentences are longer than available space, crop both\n",
    "                start = random.randint(0, len(s1_idx) - available_space)\n",
    "                s1_idx = s1_idx[start:start+available_space]\n",
    "                start = random.randint(0, len(s2_idx) - available_space)\n",
    "                s2_idx = s2_idx[start:start+available_space]\n",
    "\n",
    "\n",
    "        # combine the two sentences with a separator token into single sequence        \n",
    "        s = torch.cat([torch.tensor([self.tokenizer.cls_token_id()]), torch.tensor(s1_idx, dtype=torch.long), torch.tensor([self.tokenizer.sep_token_id()]), torch.tensor(s2_idx, dtype=torch.long), torch.tensor([self.tokenizer.sep_token_id()])])\n",
    "        # apply padding\n",
    "        pad_len = max(0,self.block_size-len(s))\n",
    "        s = torch.cat([s, torch.full((pad_len,), self.tokenizer.pad_token_id())])\n",
    "        attention_mask = torch.cat([torch.ones(self.block_size-pad_len), torch.zeros(pad_len)])\n",
    "\n",
    "        # create segment ids\n",
    "        segment_ids = torch.zeros(self.block_size)\n",
    "        sep_idx = (s == self.tokenizer.sep_token_id()).nonzero(as_tuple=False)\n",
    "        segment_ids[sep_idx[0]+1:] = 1\n",
    "        segment_ids = segment_ids.long()\n",
    "\n",
    "        return s, attention_mask, segment_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Classifier Model\n",
    "\n",
    "This is just a classifier head on top of the custom BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = bert_pretrained\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head which is a single linear layer\n",
    "        self.classifier_head = torch.nn.Sequential(torch.nn.Linear(bert_pretrained.embedding_dim, 4))\n",
    "\n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, token_type_idx, targets=None):\n",
    "        # compute BERT encodings, extract the pooler output (which is just the [CLS] embedding fed through a feedforward network or just the [CLS] embedding), apply dropout        \n",
    "        #MLM_logits, entailment_logits, claim_class_logits = self.bert_encoder(input_idx, input_attn_mask, segment_idx=token_type_idx) # shape: (batch_size, hidden_size)\n",
    "        bert_cls_encoding = self.bert_encoder(input_idx, input_attn_mask, segment_idx=token_type_idx, return_cls=True) # shape: (batch_size, hidden_size)\n",
    "        bert_cls_encoding = self.dropout(bert_cls_encoding) # shape: (batch_size, hidden_size)\n",
    "        # compute output logits\n",
    "        logits = self.classifier_head(bert_cls_encoding) # shape: (batch_size, 2)        \n",
    "        \n",
    "        # compute cross-entropy loss on the entailment logits\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, accumulation_steps=1, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            # move batch to device\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # apply gradient step \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                # optimizer step\n",
    "                optimizer.step()\n",
    "                # reset gradients\n",
    "                optimizer.zero_grad()\n",
    "   \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total     \n",
    "               \n",
    "            if val_every is not None:\n",
    "                if (i+1)%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\":loss.item(), \"Moving Avg Loss\":avg_loss, \"Train Accuracy\":train_acc, \"Val Loss\": val_loss, \"Val Accuracy\":val_acc}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        # run optimizer step for remainder batches\n",
    "        if len(train_dataloader) % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_classifier_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_classifier_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'classifier_checkpoint.pth')\n",
    "    print(f\"Saved classifier model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_classifier_model_checkpoint(model, optimizer=None, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('classifier_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded cross-encoder model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Training and Validation Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training batches: 62\n",
      "Total number of validation batches: 8\n",
      "1228 154\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "block_size = 128\n",
    "batch_size = 20\n",
    "\n",
    "train_dataset = Classifier_Dataset(train_data, knowledge_source, tokenizer, block_size=block_size)\n",
    "val_dataset = Classifier_Dataset(val_data, knowledge_source, tokenizer, block_size=block_size)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)  # set pin_memory for faster pre-fetching\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)  # set pin_memory for faster pre-fetching \n",
    "print(f\"Total number of training batches: {len(train_dataloader)}\")\n",
    "print(f\"Total number of validation batches: {len(val_dataloader)}\")\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the Classifier BERT model, load pre-trained custom BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained BERT model checkpoint at epoch 40 with loss 1.8120101885718962\n",
      "Device: cuda\n",
      "Total number of parameters in transformer network: 45.032273 M\n",
      "RAM used: 5411.65 MB\n"
     ]
    }
   ],
   "source": [
    "# BERT model hyperparameters\n",
    "embedding_dim = 512\n",
    "head_size = embedding_dim\n",
    "num_heads = 16\n",
    "num_layers = 8\n",
    "dropout_rate = 0.2\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "bert = BERTModel(vocab_size=tokenizer.vocab_size(), block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_layers=num_layers, pad_token_id=tokenizer.pad_token_id(), device=device)\n",
    "bert = bert.to(device)\n",
    "\n",
    "# load pretrained BERT model\n",
    "bert, _, _ =  load_bert_model_checkpoint(bert, name=\"BERT_multitask_checkpoint_entaiment_claims_long_600_epochs\", device=device, strict=False)\n",
    "\n",
    "# create Classifier model\n",
    "model = BERTClassifier(bert)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-8)\n",
    "\n",
    "# load pretrained Classifier model\n",
    "#model = load_classifier_model_checkpoint(model, filename='classifier_checkpoint_pretrain_basic.pth')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/UniMelb/Semester1_2024/COMP90042/Project/wandb/run-20240525_060513-zv1i03gc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM/runs/zv1i03gc' target=\"_blank\">olive-wood-125</a></strong> to <a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Pretrain%20MLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM/runs/zv1i03gc' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Pretrain%20MLM/runs/zv1i03gc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "run = wandb.init(\n",
    "    project=\"BERT Pretrain MLM\", \n",
    "    config={\n",
    "        \"model\": \"Cross Encoder\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size, \n",
    "        \"corpus\": \"Climate Claims\"},)   \n",
    "\n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/62 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 1.166, Train Accuracy:  0.384, Val Loss:  1.240, Val Accuracy:  0.448: 100%|██████████| 62/62 [00:15<00:00,  4.08it/s]\n",
      "Epoch 2, EMA Train Loss: 0.944, Train Accuracy:  0.577, Val Loss:  1.124, Val Accuracy:  0.487: 100%|██████████| 62/62 [00:15<00:00,  4.00it/s]\n",
      "Epoch 3, EMA Train Loss: 0.793, Train Accuracy:  0.658, Val Loss:  1.122, Val Accuracy:  0.565: 100%|██████████| 62/62 [00:15<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, scheduler=scheduler, device=device, num_epochs=3, accumulation_steps=20, val_every=60, save_every=None, log_metrics=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model checkpoint to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classifier model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "#save_classifier_model_checkpoint(model, optimizer, filename='classifier_checkpoint_pretrain_basic.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our main validation metric for evaluating the classifier is the classification accuracy on the Validation Claims. We will run the following 3 different **experiments**:\n",
    "\n",
    "(1) Evaluate the classification accuracy by classifying the input sequence containing a claim paired with a concatenation of all it's gold evidence passages: `[[CLS], claim, [SEP], concatenated gold evidences]`\n",
    "\n",
    "(2) Evaluate the classification accuracy by classifying the input sequence containing a claim paired with a concatenation of the top-5 BM25 retreived evidence passages: `[[CLS], claim, [SEP], concatenated top-5 BM25 evidences]`\n",
    "\n",
    "(3) Evaluate the classification accuracy by classifying the input sequence containing a claim paired with a concatenation of the top-5 monoBERT re-ranked retreived evidence passages: `[[CLS], claim, [SEP], concatenated top-5 monoBERT evidences]` \n",
    "\n",
    "We will loaded the pre-computed BM25 and monoBERT ranked document ids from pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_int2label = {0:'SUPPORTS', 1:'REFUTES', 2:'NOT_ENOUGH_INFO', 3:'DISPUTED'}\n",
    "\n",
    "# define a helper function for predicting clasim classes for a given claim and evidence list\n",
    "def predict_claim_class(val_claim_id, evidence_ids, model, device='cpu'):\n",
    "    # get the claim sentence \n",
    "    claim_sentence = val_data[val_claim_id]['claim_text']\n",
    "    # get the evidence sentences\n",
    "    evidence_texts = [knowledge_source[evidence_id] for evidence_id in evidence_ids] \n",
    "    evidence_text = \" \".join(evidence_texts)\n",
    "    # encode the claim and evidence sentences\n",
    "    input_idx, input_attn_mask, token_type_idx = val_dataset.encode_custom(claim_sentence, evidence_text)\n",
    "    # classify the pair using the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(input_idx.unsqueeze(0).to(device), input_attn_mask.unsqueeze(0).to(device), token_type_idx.unsqueeze(0).to(device))\n",
    "\n",
    "    y_pred = logits.argmax(dim=-1).view(1) # shape (1,)\n",
    "    # convert claim label from int to string\n",
    "    claim_class = claim_int2label[y_pred.item()]\n",
    "    return claim_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the helper function to make sure it work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim ID: claim-1292\n",
      "Claim Text: Any reasonable person can recognize both positives and negatives among the policy proposals of both Tories and Labour.\n",
      "Claim Label: NOT_ENOUGH_INFO\n",
      "Predicted Claim Class: NOT_ENOUGH_INFO\n"
     ]
    }
   ],
   "source": [
    "# pick a random claim from the validation set\n",
    "val_claim_id = random.choice(list(val_data.keys()))\n",
    "# get the evidence ids for the claim\n",
    "evidence_ids = val_data[val_claim_id]['evidences']\n",
    "# get the claim label\n",
    "claim_label = val_data[val_claim_id]['claim_label']\n",
    "# predict the claim class\n",
    "predicted_claim_class = predict_claim_class(val_claim_id, evidence_ids, model, device=device)\n",
    "# get the claim text\n",
    "claim_text = val_data[val_claim_id]['claim_text']\n",
    "# get the evidence texts\n",
    "evidence_texts = [knowledge_source[evidence_id] for evidence_id in evidence_ids]\n",
    "evidence_text = \" \".join(evidence_texts)\n",
    "print(f\"Claim ID: {val_claim_id}\")\n",
    "print(f\"Claim Text: {claim_text}\")\n",
    "print(f\"Claim Label: {claim_label}\")\n",
    "print(f\"Predicted Claim Class: {predicted_claim_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 1**: Evaluate the classification accuracy by classifying the input sequence containing a claim paired with a concatenation of all it's gold evidence passages: `[[CLS], claim, [SEP], concatenated gold evidences]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 154/154 [00:05<00:00, 28.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 1 --> Validation Accuracy = 0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# predict claim classes for all claims in the validation set with gold evidences and compute accuracy\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "pbar = tqdm(val_data.keys(), desc=\"Validation\")\n",
    "for val_claim_id in pbar:\n",
    "    gold_evidence_ids = val_data[val_claim_id]['evidences']\n",
    "    claim_label = val_data[val_claim_id]['claim_label']\n",
    "    predicted_claim_class = predict_claim_class(val_claim_id, gold_evidence_ids, model, device=device)\n",
    "    num_correct += (predicted_claim_class == claim_label)\n",
    "    num_total += 1\n",
    "\n",
    "accuracy = num_correct / num_total\n",
    "\n",
    "print(f\"\\nExperiment 1 --> Validation Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 2**: Evaluate the classification accuracy by classifying the input sequence containing a claim paired with a concatenation of the top-5 BM25 retreived evidence passages: `[[CLS], claim, [SEP], concatenated top-5 BM25 evidences]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed topk BM25 and reranked documents from pickle file\n",
    "with open(\"topk_reranker.pkl\", \"rb\") as file:\n",
    "    topk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 154/154 [00:03<00:00, 42.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 2 --> Validation Accuracy = 0.422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get the top-5 BM25 evidences for each validation claim\n",
    "top5_ev_ids_bm25 = {claim_id: ev_dict[\"bm25\"][0][:5] for claim_id, ev_dict in topk.items()}\n",
    "top5_ev_scores_bm25 = {claim_id: ev_dict[\"bm25\"][1][:5] for claim_id, ev_dict in topk.items()}\n",
    "\n",
    "# predict claim classes for all claims in the validation set with top-5 BM25 evidences and compute accuracy\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "pbar = tqdm(val_data.keys(), desc=\"Validation\")\n",
    "for val_claim_id in pbar:\n",
    "    bm25_evidence_ids = top5_ev_ids_bm25[val_claim_id]\n",
    "    claim_label = val_data[val_claim_id]['claim_label']\n",
    "    predicted_claim_class = predict_claim_class(val_claim_id, bm25_evidence_ids, model, device=device)\n",
    "    num_correct += (predicted_claim_class == claim_label)\n",
    "    num_total += 1\n",
    "\n",
    "accuracy = num_correct / num_total\n",
    "\n",
    "print(f\"\\nExperiment 2 --> Validation Accuracy = {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Experiment 3**: Evaluate the classification accuracy by classifying the input sequence containing a claim paired with a concatenation of the top-5 monoBERT re-ranked retreived evidence passages: `[[CLS], claim, [SEP], concatenated top-5 monoBERT evidences]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 154/154 [00:06<00:00, 23.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiment 3 --> Validation Accuracy = 0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get the top-5 monoBERT evidences for each validation claim\n",
    "top5_ev_ids_monoBERT = {claim_id: ev_dict[\"ce\"][0][:5] for claim_id, ev_dict in topk.items()}\n",
    "top5_ev_scores_monoBERT = {claim_id: ev_dict[\"ce\"][1][:5] for claim_id, ev_dict in topk.items()}\n",
    "\n",
    "# predict claim classes for all claims in the validation set with top-5 BM25 evidences and compute accuracy\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "pbar = tqdm(val_data.keys(), desc=\"Validation\")\n",
    "for val_claim_id in pbar:\n",
    "    monoBERT_evidence_ids = top5_ev_ids_monoBERT[val_claim_id]\n",
    "    claim_label = val_data[val_claim_id]['claim_label']\n",
    "    predicted_claim_class = predict_claim_class(val_claim_id, monoBERT_evidence_ids, model, device=device)\n",
    "    num_correct += (predicted_claim_class == claim_label)\n",
    "    num_total += 1\n",
    "\n",
    "accuracy = num_correct / num_total\n",
    "\n",
    "print(f\"\\nExperiment 3 --> Validation Accuracy = {accuracy:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
