{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024 COMP90042 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "#### **Document Retreival using monoBERT Model**: \n",
    "\n",
    "In this notebook, we finetune our custom pre-trained custom BERT model to perform binary classification of $(claim, evidence)$ pairs into relevant and non-relevant classes. We then use this trained re-ranker model to rerank top-k documents retreived by our BM25 model.\n",
    "\n",
    "*** **PLEASE NOTE**: The BM25 model implementation is contained in a separate python script called: `BM25.py`. We are importing the `BM25` class from this script. We also import helper function that we implemented for pre-processing/cleaning our data from the python script called `utils.py`. Our custom BERT model implementation is contained in the python script called `min_bert_multi.py` and pre-trained weights are loaded from a checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# install required packages\n",
    "!pip install unidecode\n",
    "!python -m nltk.downloader stopwords\n",
    "!pip install wandb\n",
    "\n",
    "from utils import *\n",
    "from min_bert_multi import *\n",
    "from BM25 import BM25_retriever, eval\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "import wandb\n",
    "import psutil\n",
    "import random\n",
    "\n",
    "#wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.DataSet Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Claims Dataset with Knowledge Source and Clean the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "\n",
      "Number of evidence passages after cleaning: 1206800\n",
      "Number of training instances after cleaning: 1228\n",
      "Number of validation instances after cleaning: 154\n"
     ]
    }
   ],
   "source": [
    "# load dataset and prepare corpus\n",
    "knowledge_source, train_data, val_data = load_dataset()      \n",
    "print(f\"Number of evidence passages: {len(knowledge_source)}\")\n",
    "print(f\"Number of training instances: {len(train_data)}\")  \n",
    "print(f\"Number of validation instances: {len(val_data)}\")\n",
    "\n",
    "# clean all senteneces in the dataset (this involves converting from unicode to asc-ii, removing URLS, removing repeating non-alphanumeric characters, removing latex equation blocks, discarding evidence passages that contain code or references to mathematical equations, etc. Just a bunch of thing that are most likely will not be useful for claim classification task)\n",
    "cleaner = SentenceCleaner()\n",
    "knowledge_source, train_data, val_data = cleaner.clean_dataset(knowledge_source, train_data, val_data)\n",
    "print(f\"\\nNumber of evidence passages after cleaning: {len(knowledge_source)}\")\n",
    "print(f\"Number of training instances after cleaning: {len(train_data)}\")  \n",
    "print(f\"Number of validation instances after cleaning: {len(val_data)}\")\n",
    "\n",
    "# dictionary for mapping integer to document id \n",
    "int2docID = {i:evidence_id for i,evidence_id in enumerate(list(knowledge_source.keys()))}\n",
    "\n",
    "claim_ids = [claim_id for claim_id in train_data.keys()]\n",
    "\n",
    "# load trained wordpiece tokenizer from file\n",
    "with open('tokenizer_worpiece_20000_aug.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# load hard negatives from file if available\n",
    "#with open(\"hard_negatives_2.pkl\", \"rb\") as file:\n",
    "#    hard_negatives = pickle.load(file)  \n",
    "hard_negatives = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the Re-Ranking Task Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderDataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, tokenizer, hard_negatives=None, num_negative=2, block_size=128):\n",
    "        self.claims_data = claims_data\n",
    "        self.document_store = document_store\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.num_negative = num_negative\n",
    "        self.block_size = block_size\n",
    "        self.negative_label = 0\n",
    "        self.positive_label = 1\n",
    "        self.document_ids = list(document_store.keys())\n",
    "        self.claim_pairs = self.create_pairs()        \n",
    "        \n",
    "    def create_pairs(self):\n",
    "        claim_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_pairs.append((claim_id, evidence_id, self.positive_label))  \n",
    "                # for each positive evidence, sample `num_negative` \"negative\" evidences randomly from document store\n",
    "                negative_ids = random.sample(self.document_ids, self.num_negative)\n",
    "                for negative_id in negative_ids:\n",
    "                    claim_pairs.append((claim_id, negative_id, self.negative_label))\n",
    "                # also sample a hard negative\n",
    "                if self.hard_negatives is not None:\n",
    "                    hard_negative_id = random.choice(self.hard_negatives[claim_id])\n",
    "                    claim_pairs.append((claim_id, hard_negative_id, self.negative_label))   \n",
    "        # shuffle the pairs \n",
    "        random.shuffle(claim_pairs)                \n",
    "        return claim_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim id and evidence id\n",
    "        claim_id, evidence_id, target_label = self.claim_pairs[idx]\n",
    "        # get the claim and evidence text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        evidence_text = self.document_store[evidence_id]\n",
    "\n",
    "        # encode and create tensors\n",
    "        input_idx, input_attn_mask, token_type_idx = self.tokenize_and_encode(claim_text, evidence_text)\n",
    "        target_label = torch.tensor(target_label)\n",
    "        return input_idx, input_attn_mask, token_type_idx, target_label\n",
    "\n",
    "    def tokenize_and_encode(self, claim_text, evidence_text):\n",
    "        # tokenize the claim and evidence text  \n",
    "        claim_idx = self.tokenizer.encode([claim_text])[0]\n",
    "        evidence_idx = self.tokenizer.encode([evidence_text])[0]\n",
    "\n",
    "        # select a random window from the evidence passage if it won't fit in block size\n",
    "        max_evidence_size = self.block_size - len(claim_idx) - 3\n",
    "        if len(evidence_idx) > max_evidence_size:\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(evidence_idx)-max_evidence_size))\n",
    "            # select the window\n",
    "            evidence_idx = evidence_idx[start_pos:start_pos+max_evidence_size]\n",
    " \n",
    "        # concatenate the claim and evidence, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id()] + claim_idx + [self.tokenizer.sep_token_id()] + evidence_idx + [self.tokenizer.sep_token_id()]\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id()] * (self.block_size - len(input_idx))    \n",
    "\n",
    "        # create segment ids\n",
    "        claim_len = len(claim_idx) + 2\n",
    "        evidence_len = len(evidence_idx) + 1\n",
    "        token_type_idx = [0] * claim_len + [1] * evidence_len + [0] * (self.block_size - claim_len - evidence_len)\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.block_size}!\")\n",
    "    \n",
    "        # create attention masks\n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id() else 0 for idx in input_idx]\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask)\n",
    "        token_type_idx = torch.tensor(token_type_idx) \n",
    "\n",
    "        return input_idx, input_attn_mask, token_type_idx\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.claim_pairs = self.create_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Re-Ranking BERT Model\n",
    "\n",
    "This is just a classifier head on top of the custom BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTReranker(torch.nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = bert_pretrained\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head which is a single linear layer\n",
    "        self.classifier_head = torch.nn.Sequential(torch.nn.Linear(bert_pretrained.embedding_dim, 2))\n",
    "\n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, token_type_idx, targets=None):\n",
    "        # compute BERT encodings, extract the pooler output (which is just the [CLS] embedding fed through a feedforward network or just the [CLS] embedding), apply dropout        \n",
    "        #MLM_logits, entailment_logits, claim_class_logits = self.bert_encoder(input_idx, input_attn_mask, segment_idx=token_type_idx) # shape: (batch_size, hidden_size)\n",
    "        bert_cls_encoding = self.bert_encoder(input_idx, input_attn_mask, segment_idx=token_type_idx, return_cls=True) # shape: (batch_size, hidden_size)\n",
    "        bert_cls_encoding = self.dropout(bert_cls_encoding) # shape: (batch_size, hidden_size)\n",
    "        # compute output logits\n",
    "        logits = self.classifier_head(bert_cls_encoding) # shape: (batch_size, 2)  \n",
    "        \n",
    "        # compute cross-entropy loss on the entailment logits\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, accumulation_steps=1, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    train_precision = 0\n",
    "    train_recall = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_precision = 0\n",
    "    val_recall = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        num_pos = 0\n",
    "        num_true_pos = 0\n",
    "        num_pred_pos = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            # move batch to device\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # apply gradient step \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                # optimizer step\n",
    "                optimizer.step()\n",
    "                # reset gradients\n",
    "                optimizer.zero_grad()\n",
    "   \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total     \n",
    "\n",
    "            # compute recall for positive class\n",
    "            num_pos += targets.eq(1).sum().item()\n",
    "            num_true_pos += (y_pred.eq(1) & targets.eq(1)).sum().item()\n",
    "            train_recall = num_true_pos / num_pos\n",
    "\n",
    "            # compute precision\n",
    "            num_pred_pos += y_pred.eq(1).sum().item()\n",
    "            train_precision = num_true_pos / max(1,num_pred_pos)\n",
    "               \n",
    "            if val_every is not None:\n",
    "                if (i+1)%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc, val_precision, val_recall = validation(model, val_dataloader, device=device)\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Train Precision: {train_precision: .3f}, Train Recall: {train_recall: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}, Val Precision: {val_precision: .3f}, Val Recall: {val_recall: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\":loss.item(), \"Moving Avg Loss\":avg_loss, \"Train Accuracy\":train_acc, \"Train Recall\":train_recall, \"Val Loss\": val_loss, \"Val Accuracy\":val_acc, \"Val Recall\":val_precision, \"Val Recall\":val_recall}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        # run optimizer step for remainder batches\n",
    "        if len(train_dataloader) % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_ce_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        num_pos = 0\n",
    "        num_true_pos = 0\n",
    "        num_pred_pos = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "\n",
    "            # compute recall for positive class\n",
    "            num_pos += targets.eq(1).sum().item()\n",
    "            num_true_pos += (y_pred.eq(1) & targets.eq(1)).sum().item()\n",
    "            val_losses[i] = loss.item()\n",
    "\n",
    "            # compute precision\n",
    "            num_pred_pos += y_pred.eq(1).sum().item()\n",
    "\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    val_precision = num_true_pos / max(1,num_pred_pos)\n",
    "    val_recall = num_true_pos / num_pos\n",
    "\n",
    "    return val_loss, val_accuracy, val_precision, val_recall\n",
    "\n",
    "def save_ce_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'cross_enc_checkpoint.pth')\n",
    "    print(f\"Saved cross-encoder model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_ce_model_checkpoint(model, optimizer=None, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('cross_enc_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded cross-encoder model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Training and Validation Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training batches: 1804, Total number of validation batches: 200\n",
      "Total number of training instances: 57708, Total number of validation instances: 6383\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "# create dataset\n",
    "train_dataset = CrossEncoderDataset(train_data, knowledge_source, tokenizer, hard_negatives=hard_negatives, block_size=block_size, num_negative=12)\n",
    "val_dataset = CrossEncoderDataset(val_data, knowledge_source, tokenizer, block_size=block_size, num_negative=12)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)  # set pin_memory for faster pre-fetching\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=2)   \n",
    "print(f\"Total number of training batches: {len(train_dataloader)}, Total number of validation batches: {len(val_dataloader)}\")\n",
    "print(f\"Total number of training instances: {len(train_dataset)}, Total number of validation instances: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the RE-Ranking BERT model, load pre-trained custom BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained BERT model checkpoint at epoch 40 with loss 1.8120101885718962\n",
      "Device: cuda\n",
      "Total number of parameters in transformer network: 45.031247 M\n",
      "RAM used: 3429.38 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanzid/miniconda3/envs/torch_clone_2/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# BERT model hyperparameters\n",
    "embedding_dim = 512\n",
    "head_size = embedding_dim\n",
    "num_heads = 16\n",
    "num_layers = 8\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "# instantiate custom BERT model\n",
    "bert = BERTModel(vocab_size=tokenizer.vocab_size(), block_size=block_size, embedding_dim=embedding_dim, head_size=head_size, num_heads=num_heads, num_layers=num_layers, pad_token_id=tokenizer.pad_token_id(), device=device)\n",
    "bert = bert.to(device)\n",
    "\n",
    "# load pretrained custom BERT model from checkpoint\n",
    "bert, _, _ =  load_bert_model_checkpoint(bert, name=\"BERT_multitask_checkpoint_entaiment_claims_long_600_epochs\", device=device, strict=False)\n",
    "\n",
    "# instantiate Re-ranker model and optimizer\n",
    "model = BERTReranker(bert)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "model = model.to(device)\n",
    "\n",
    "# instantiate learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-8)\n",
    "\n",
    "# load monoBERT cross encoder model from checkpoint\n",
    "#model = load_ce_model_checkpoint(model, filename='cross_enc_checkpoint_4_epochs_12_negatives.pth')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/UniMelb/Semester1_2024/COMP90042/Project/wandb/run-20240525_162858-djyt56ll</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM/runs/djyt56ll' target=\"_blank\">magic-silence-143</a></strong> to <a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Pretrain%20MLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/BERT%20Pretrain%20MLM/runs/djyt56ll' target=\"_blank\">https://wandb.ai/tanzids/BERT%20Pretrain%20MLM/runs/djyt56ll</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "run = wandb.init(\n",
    "    project=\"BERT Pretrain MLM\", \n",
    "    config={\n",
    "        \"model\": \"Cross Encoder\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"batch_size\": batch_size, \n",
    "        \"corpus\": \"Climate Claims\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune for 4 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1804 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 0.055, Train Accuracy:  0.968, Train Precision:  0.763, Train Recall:  0.808, Val Loss:  0.187, Val Accuracy:  0.954, Val Precision:  0.939, Val Recall:  0.436: 100%|██████████| 1804/1804 [05:00<00:00,  6.00it/s]  \n",
      "Epoch 2, EMA Train Loss: 0.050, Train Accuracy:  0.983, Train Precision:  0.867, Train Recall:  0.902, Val Loss:  0.148, Val Accuracy:  0.959, Val Precision:  0.883, Val Recall:  0.540: 100%|██████████| 1804/1804 [05:16<00:00,  5.70it/s]  \n",
      "Epoch 3, EMA Train Loss: 0.012, Train Accuracy:  0.989, Train Precision:  0.920, Train Recall:  0.934, Val Loss:  0.225, Val Accuracy:  0.954, Val Precision:  0.933, Val Recall:  0.428: 100%|██████████| 1804/1804 [05:22<00:00,  5.59it/s]  \n",
      "Epoch 4, EMA Train Loss: 0.006, Train Accuracy:  0.995, Train Precision:  0.961, Train Recall:  0.970, Val Loss:  0.254, Val Accuracy:  0.956, Val Precision:  0.944, Val Recall:  0.448: 100%|██████████| 1804/1804 [05:25<00:00,  5.54it/s]  \n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, scheduler=scheduler, device=device, num_epochs=4, accumulation_steps=20, val_every=1300, save_every=None, log_metrics=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cross-encoder model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "save_ce_model_checkpoint(model, optimizer, filename='cross_enc_checkpoint_4_epochs_12_negatives.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Re-ranking with the trained model**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions for computing document relevancy scores usong trained Re-ranker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_ids = list(knowledge_source.keys())   \n",
    "\n",
    "# compute score for a given claim and evidence pair\n",
    "def compute_score(claim_text, evidence_text, model, device='cuda'):\n",
    "    model.eval()\n",
    "    input_idx, input_attn_mask, token_type_idx = val_dataset.tokenize_and_encode(claim_text, evidence_text)\n",
    "    input_idx = input_idx.unsqueeze(0)\n",
    "    input_attn_mask = input_attn_mask.unsqueeze(0)\n",
    "    token_type_idx = token_type_idx.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        scores, _ = model(input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device))\n",
    "    \n",
    "    # apply sigmoid to get probability\n",
    "    score = torch.sigmoid(scores[0,1]).item()\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_score_batch(claim_text, evidence_ids, model, device='cuda', batch_size=16):\n",
    "    model.eval()\n",
    "    input_idx_batch, input_attn_mask_batch, token_type_idx_batch = [], [], []\n",
    "    scores = torch.zeros(len(evidence_ids))\n",
    "    for i in range(0, len(evidence_ids), batch_size):\n",
    "        input_idx_batch, input_attn_mask_batch, token_type_idx_batch = [], [], []\n",
    "        for evidence_id in evidence_ids[i:i+batch_size]:\n",
    "            input_idx, input_attn_mask, token_type_idx = val_dataset.tokenize_and_encode(claim_text, knowledge_source[evidence_id])\n",
    "            input_idx_batch.append(input_idx)\n",
    "            input_attn_mask_batch.append(input_attn_mask)\n",
    "            token_type_idx_batch.append(token_type_idx)\n",
    "        input_idx_batch = torch.stack(input_idx_batch)\n",
    "        input_attn_mask_batch = torch.stack(input_attn_mask_batch)\n",
    "        token_type_idx_batch = torch.stack(token_type_idx_batch)    \n",
    "        with torch.no_grad():\n",
    "            scores_batch, _ = model(input_idx_batch.to(device), input_attn_mask_batch.to(device), token_type_idx_batch.to(device))\n",
    "        scores[i:i+batch_size] = torch.sigmoid(scores_batch[:,1]) \n",
    "    scores = scores.cpu().tolist()\n",
    "    return scores\n",
    "\n",
    "\n",
    "# find top-k passages for given claim and compare with gold evidences\n",
    "def topk_passages(data, claim_id, topk=100, device='cuda', batch_size=16):\n",
    "    claim_text = data[claim_id][\"claim_text\"]\n",
    "    model.eval()\n",
    "    scores = torch.zeros(len(passage_ids))\n",
    "    for i in tqdm(range(0, len(passage_ids), batch_size)):\n",
    "        input_idx_batch, input_attn_mask_batch, token_type_idx_batch = [], [], []\n",
    "        for evidence in passage_ids[i:i+batch_size]:\n",
    "            evidence_text = knowledge_source[evidence]\n",
    "            input_idx, input_attn_mask, token_type_idx = val_dataset.tokenize_and_encode(claim_text, evidence_text)\n",
    "            input_idx_batch.append(input_idx)\n",
    "            input_attn_mask_batch.append(input_attn_mask)\n",
    "            token_type_idx_batch.append(token_type_idx)\n",
    "        input_idx_batch = torch.stack(input_idx_batch)\n",
    "        input_attn_mask_batch = torch.stack(input_attn_mask_batch)\n",
    "        token_type_idx_batch = torch.stack(token_type_idx_batch)    \n",
    "        with torch.no_grad():\n",
    "            scores_batch, _ = model(input_idx_batch.to(device), input_attn_mask_batch.to(device), token_type_idx_batch.to(device))\n",
    "        scores[i:i+batch_size] = scores_batch[:,1] \n",
    "\n",
    "    # sort the scores \n",
    "    sorted_scores, sorted_indices = torch.sort(scores, descending=True)    \n",
    "    # sort the passage ids\n",
    "    ranked_passage_ids = [passage_ids[i] for i in sorted_indices]\n",
    "    # get top-k passages\n",
    "    topk_passage_ids = ranked_passage_ids[:topk]\n",
    "    topk_scores = sorted_scores[:topk]\n",
    "\n",
    "    # get gold evidences\n",
    "    gold_evidence_list = data[claim_id][\"evidences\"]\n",
    "    print(f\"Claim: {claim_text}\")\n",
    "    print(f\"\\nGold evidence:\")\n",
    "    for evidence_id in gold_evidence_list:\n",
    "        print(f\"{evidence_id} --> {knowledge_source[evidence_id]}\")  \n",
    "\n",
    "    print(f\"\\nPredicted evidence:\")\n",
    "    for i, evidence_id in enumerate(topk_passage_ids):\n",
    "        print(f\"{evidence_id} --> {knowledge_source[evidence_id]} --> {topk_scores[i]}\")\n",
    "\n",
    "    # evaluation (precision, recall, F1)\n",
    "    intersection = set(topk_passage_ids).intersection(gold_evidence_list)\n",
    "    print(f\"\\nMatching evidence passages: {intersection}\")\n",
    "    precision = len(intersection) / len(topk_passage_ids)\n",
    "    recall = len(intersection) / len(gold_evidence_list)\n",
    "    f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "    print(f\"\\nPrecision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\n",
    "    return topk_passage_ids, topk_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some tests for validation claims to compare yhe relevenacy scores assigned to gold documents vs non-gold documents selected randomly from knowledge scource. Observe that the gold documents will get much higher scores. This will indicate the re-ranking model is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim-2060 --> Global warming is causing more hurricanes and stronger hurricanes.\n",
      "\n",
      "Gold evidences and scores:\n",
      "evidence-553897 --> The maximum rainfall and wind speed from hurricanes and typhoons are likely increasing. --> 0.9698548316955566\n",
      "evidence-8304 --> As the Earth's climate warms, we are seeing many changes: stronger, more destructive hurricanes; heavier rainfall; more disastrous flooding; more areas of the world experiencing severe drought; and more heat waves.\" --> 0.7158042788505554\n",
      "\n",
      "Random evidences and scores:\n",
      "evidence-938605 --> On 23 September 2014, Al-Turki was killed during a series of a U.S.-led anti-Khorasan Group coalition airstrikes over Syria. --> 0.00016451627016067505\n",
      "evidence-951564 --> He had written about his journeys through Afghanistan, once at 19 and again, as described in the book, An Unexpected Light : Travels in Afghanistan, for which he received the Thomas Cook Travel Book Award in 2000 and the ALA Notable Books for Adults in 2002. --> 9.351714106742293e-05\n",
      "evidence-1085451 --> Breed of the Sea is a 1926 American silent film directed by and starring Ralph Ince. --> 0.00019542223890312016\n",
      "evidence-854330 --> But it will be remembered. --> 0.00022755567624699324\n",
      "evidence-866293 --> Owned by Gray Television, WCTV has studios on Halstead Boulevard in Tallahassee, Florida (along I-10). --> 0.00010148669389309362\n",
      "evidence-504082 --> This was different from the feudalism of the West. --> 0.0003355874214321375\n",
      "evidence-295897 --> It began on March 19 at Caruaru and ended on December 10 at Brasilia. --> 0.0002057459350908175\n",
      "evidence-894412 --> Admiral of the Fleet Sir Provo William Parry Wallis, GCB (12 April 1791  13 February 1892) was a Royal Navy officer. --> 7.427160016959533e-05\n"
     ]
    }
   ],
   "source": [
    "# pick a random claim from the validation set\n",
    "claim_id = random.choice(list(val_data.keys())) \n",
    "claim = val_data[claim_id]\n",
    "claim_text = claim[\"claim_text\"]\n",
    "print(f\"{claim_id} --> {claim_text}\")\n",
    "\n",
    "# get gold evidence passages\n",
    "gold_evidence_list = claim[\"evidences\"]\n",
    "# get an twice as many random passages from knowledge source\n",
    "random_passages = random.sample(passage_ids, len(gold_evidence_list)*4)\n",
    "\n",
    "# now score the claim against the gold evidence passages and random passages\n",
    "scores_gold = [compute_score(claim_text, knowledge_source[evidence_id], model) for evidence_id in gold_evidence_list]\n",
    "scores_random = [compute_score(claim_text, knowledge_source[evidence_id], model) for evidence_id in random_passages]\n",
    "\n",
    "print(\"\\nGold evidences and scores:\")\n",
    "for i, evidence_id in enumerate(gold_evidence_list):\n",
    "    print(f\"{evidence_id} --> {knowledge_source[evidence_id]} --> {scores_gold[i]}\")\n",
    "\n",
    "print(\"\\nRandom evidences and scores:\")\n",
    "for i, evidence_id in enumerate(random_passages):\n",
    "    print(f\"{evidence_id} --> {knowledge_source[evidence_id]} --> {scores_random[i]}\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the trained BM25 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained BM25 model\n",
    "with open(\"bm25_b=0.3_k=0.5.pkl\", \"rb\") as file:\n",
    "    bm25 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run some tests comparing BM25 retreived document rankings before and after Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate retreival results for a given claim text\n",
    "def claim_retreive_eval(claim, topk=5000):\n",
    "    print(f\"Claim: {claim['claim_text']}\")\n",
    "    claim_text = claim['claim_text']\n",
    "\n",
    "    # get top k relevant evidence passages and their scores\n",
    "    topk_ev_indices, topk_scores = bm25.retrieve_docs(query=claim_text, topk=topk)\n",
    "    # convert indices to document ids\n",
    "    topk_ev_ids = [int2docID[i] for i in topk_ev_indices]\n",
    "    # get the gold evidence ids\n",
    "    gold_ev_ids = claim['evidences']\n",
    "    # compute precision, recall, and F1\n",
    "    intersection = set(topk_ev_ids).intersection(gold_ev_ids)\n",
    "    precision = len(intersection) / len(topk_ev_ids)\n",
    "    recall = len(intersection) / len(gold_ev_ids)\n",
    "    f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "\n",
    "    print(f\"\\nGold evidence passages: {gold_ev_ids}\")\n",
    "    for ev in gold_ev_ids:\n",
    "        print(f\"{ev} --> {knowledge_source[ev]}\")\n",
    "\n",
    "    print(f\"\\nMatching evidence passages: {intersection}\")\n",
    "\n",
    "    # get cross-encoder scores for retireved passages\n",
    "    ce_scores = compute_score_batch(claim_text, topk_ev_ids, model) #[compute_score(claim_text, knowledge_source[evidence_id], model) for evidence_id in topk_ev_ids]\n",
    "    # sort the passages by cross-encoder scores\n",
    "    sorted_indices = sorted(range(len(ce_scores)), key=lambda i: ce_scores[i], reverse=True)\n",
    "    sorted_ce_scores = [ce_scores[i] for i in sorted_indices] \n",
    "    topk_ev_ids_ce = [topk_ev_ids[i] for i in sorted_indices]\n",
    "\n",
    "    print(f\"Top 10 BM-25 Passages:\")\n",
    "    for i, ev in enumerate(topk_ev_ids[:15]):\n",
    "        print(f\"{ev} --> {knowledge_source[ev]} --> {topk_scores[i]}\")\n",
    "\n",
    "    print(f\"Top 10 Cross-Encoder Passages:\")\n",
    "    for i, ev in enumerate(topk_ev_ids_ce[:15]):\n",
    "        print(f\"{ev} --> {knowledge_source[ev]} --> {sorted_ce_scores[i]}\")    \n",
    "\n",
    "\n",
    "    # get BM25 and cross-encoder rankings of matching evidence passages\n",
    "    bm25_rankings = [topk_ev_ids.index(ev) for ev in intersection]\n",
    "    ce_rankings = [topk_ev_ids_ce.index(ev) for ev in intersection]\n",
    "\n",
    "    print(f\"\\nBM25 and Cross-Encoder rankings of matching evidence passages:\")\n",
    "    for i, ev in enumerate(intersection):\n",
    "        print(f\"{ev} --> BM25 Rank: {bm25_rankings[i]}, CE Rank: {ce_rankings[i]}\")\n",
    "    \n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim id --> claim-1219\n",
      "Claim: Global warming is driving major melting on the surface of Greenland's glaciers and is speeding up their travel into the sea.\"\n",
      "\n",
      "Gold evidence passages: ['evidence-88825', 'evidence-1127398', 'evidence-1115033']\n",
      "evidence-88825 --> Surface temperature increases are greatest in the Arctic, which has contributed to the retreat of glaciers, permafrost, and sea ice.\n",
      "evidence-1127398 --> This acceleration is due mostly to human-caused global warming, which is driving thermal expansion of seawater and the melting of land-based ice sheets and glaciers.\n",
      "evidence-1115033 --> Global warming could lead to an increase in freshwater in the northern oceans, by melting glaciers in Greenland, and by increasing precipitation, especially through Siberian rivers.\n",
      "\n",
      "Matching evidence passages: {'evidence-88825', 'evidence-1115033', 'evidence-1127398'}\n",
      "Top 10 BM-25 Passages:\n",
      "evidence-797505 --> Recent global warming has caused mountain glaciers and the ice sheets in Greenland and Antarctica to melt and global sea level to rise. --> 1.6292135083650428\n",
      "evidence-1088478 --> The effect of global warming is already being registered in melting glaciers, melting mountain ice caps, and rising sea levels. --> 1.3641056477855984\n",
      "evidence-118577 --> The effects of global warming in the Arctic include rising temperatures, loss of sea ice, and melting of the Greenland ice sheet. --> 1.3573214125324649\n",
      "evidence-1115033 --> Global warming could lead to an increase in freshwater in the northern oceans, by melting glaciers in Greenland, and by increasing precipitation, especially through Siberian rivers. --> 1.279938470577655\n",
      "evidence-894521 --> Global warming also has an enormous impact with respect to melting glaciers and ice sheets. --> 1.2775086653663614\n",
      "evidence-59240 --> The glaciers of Greenland are also contributing to a rise in the global sea level faster than was previously believed. --> 1.2557740944252542\n",
      "evidence-747315 --> Total warming in Greenland was . --> 1.236829801953143\n",
      "evidence-975870 --> Higher global temperatures melt glaciers such as the one in Greenland, which flow into the oceans, adding to the amount of seawater. --> 1.217982732106667\n",
      "evidence-1127398 --> This acceleration is due mostly to human-caused global warming, which is driving thermal expansion of seawater and the melting of land-based ice sheets and glaciers. --> 1.2123301662009527\n",
      "evidence-171206 --> Warming temperatures lead to the melting of glaciers and ice sheets. --> 1.2118102168148663\n",
      "evidence-344321 --> The decline in sea ice does have a notable potential to significantly speed up global warming and the climate changes. --> 1.1831503126677372\n",
      "evidence-808139 --> If the entire of ice were to melt, global sea levels would rise . --> 1.1246248066361082\n",
      "evidence-1192872 --> Under the influence of global warming, melt at the base of the ice sheet increases. --> 1.1245775217447818\n",
      "evidence-240414 --> Ongoing effects include rising sea levels due to thermal expansion and melting of glaciers and ice sheets, and warming of the ocean surface, leading to increased temperature stratification. --> 1.1123455712582144\n",
      "evidence-1040612 --> The Greenland ice sheet occupies about 82% of the surface of Greenland, and if melted would cause sea levels to rise by 7.2 metres. --> 1.0879121178117581\n",
      "Top 10 Cross-Encoder Passages:\n",
      "evidence-1127398 --> This acceleration is due mostly to human-caused global warming, which is driving thermal expansion of seawater and the melting of land-based ice sheets and glaciers. --> 0.9092593789100647\n",
      "evidence-399273 --> For instance, Mercer published a study in 1978 predicting that anthropogenic carbon dioxide warming and its potential effects on climate in the 21st century could cause a sea level rise of around 5 metres (16 ft) from melting of the West Antarctic ice-sheet alone. --> 0.907028317451477\n",
      "evidence-1108323 --> These can be defined as geologically brief (<200,000 year) events characterized by rapid global warming, major changes in the environment, and massive carbon addition. --> 0.8852205872535706\n",
      "evidence-311416 --> Globally, these effects are estimated to have led to a slight cooling, dominated by an increase in surface albedo. --> 0.8807904124259949\n",
      "evidence-716825 --> The surprising effect of this is that the global warming potential of CO is three times that of CO 2. --> 0.8805460333824158\n",
      "evidence-88825 --> Surface temperature increases are greatest in the Arctic, which has contributed to the retreat of glaciers, permafrost, and sea ice. --> 0.8698478937149048\n",
      "evidence-213585 --> Between 1993 and 2018, thermal expansion of the oceans contributed 42% to sea level rise; the melting of temperate glaciers, 21%; Greenland, 15%; and Antarctica, 8%. --> 0.8618009686470032\n",
      "evidence-817308 --> The effects of global warming in the Arctic, or climate change in the Arctic include rising air and water temperatures, loss of sea ice, and melting of the Greenland ice sheet with a related cold temperature anomaly, observed since the 1970s. --> 0.8583608865737915\n",
      "evidence-240134 --> The Arctic is affected by current global warming, leading to Arctic sea ice shrinkage, diminished ice in the Greenland ice sheet, and Arctic methane release as the permafrost thaws. --> 0.8559008240699768\n",
      "evidence-123917 --> The contribution of the Greenland ice sheet on sea level over the next couple of centuries can be very high due to a self-reinforcing cycle (a so-called positive feedback). --> 0.8367307186126709\n",
      "evidence-289731 --> \"Methane release from melting permafrost could trigger dangerous global warming\". --> 0.7884409427642822\n",
      "evidence-118577 --> The effects of global warming in the Arctic include rising temperatures, loss of sea ice, and melting of the Greenland ice sheet. --> 0.7838746309280396\n",
      "evidence-510852 --> Human-caused increases in greenhouse gases are responsible for most of the observed global average surface warming of roughly 0.8 degC (1.5 degF) over the past 140 years. --> 0.7767347693443298\n",
      "evidence-973645 --> The global average and combined land and ocean surface temperature, show a warming of 0.85  degC, in the period 1880 to 2012, based on multiple independently produced datasets. --> 0.7511827349662781\n",
      "evidence-382966 --> Pine Island Glacier (PIG) is a large ice stream, and the fastest melting glacier in Antarctica, responsible for about 25% of Antarctica's ice loss. --> 0.7308281064033508\n",
      "\n",
      "BM25 and Cross-Encoder rankings of matching evidence passages:\n",
      "evidence-88825 --> BM25 Rank: 78, CE Rank: 5\n",
      "evidence-1115033 --> BM25 Rank: 3, CE Rank: 123\n",
      "evidence-1127398 --> BM25 Rank: 8, CE Rank: 0\n",
      "Precision: 0.003, Recall: 1.0, F1: 0.005982053838484547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.003, 1.0, 0.005982053838484547)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick a random claim from validation set\n",
    "id = random.choice(list(val_data.keys()))   # random.choice(list(train_data.keys()))\n",
    "claim = val_data[id]                        # train_data[id] \n",
    "print(f\"Claim id --> {id}\")\n",
    "# retreive with BM25 and then perform re-ranking\n",
    "claim_retreive_eval(claim, topk=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Retreival on Validation Claims and Evaluate Performance by computing the following metrics at specified range of k-values: Mean Average Precision (mAP), Average Recall, Average F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average precision, recall, f1 scores and mean average precision (mAP) at every k value\n",
    "def reranking_eval(claims_dataset, bm25, cross_encoder, k_values=[10], mixed=False):\n",
    "    k_values = sorted(k_values)\n",
    "    if mixed:\n",
    "        num_vars = 3\n",
    "    else:\n",
    "        num_vars = 2    \n",
    "    precision = np.zeros(shape=(len(k_values), num_vars))\n",
    "    recall = np.zeros(shape=(len(k_values), num_vars))\n",
    "    f1 = np.zeros(shape=(len(k_values), num_vars))\n",
    "    ap_values = np.zeros((len(k_values), num_vars))\n",
    "    topk = {}\n",
    "\n",
    "    for claim_id, claim in tqdm(claims_dataset.items(), total=len(claims_dataset)):\n",
    "        query = claim[\"claim_text\"]\n",
    "        gold_evidence_list = claim[\"evidences\"]\n",
    "        # retrieve bm25 topk documents\n",
    "        topk_ev_indices, topk_scores_bm25 = bm25.retrieve_docs(query, topk=max(k_values))\n",
    "        if topk_ev_indices == []:\n",
    "            continue\n",
    "        # convert document indices to document id strings\n",
    "        topk_ev_ids_bm25 = [int2docID[i] for i in topk_ev_indices]\n",
    "\n",
    "        # compute cross-encoder scores for topk documents\n",
    "        ce_scores = compute_score_batch(query, topk_ev_ids_bm25, cross_encoder)\n",
    "        # sort the scores\n",
    "        sorted_indices = sorted(range(len(ce_scores)), key=lambda i: ce_scores[i], reverse=True)\n",
    "        sorted_ce_scores = [ce_scores[i] for i in sorted_indices] \n",
    "        # compute cross-encoder re-ranking\n",
    "        topk_ev_ids_ce = [topk_ev_ids_bm25[i] for i in sorted_indices]\n",
    "        # save topk ranked docs with scores\n",
    "        topk[claim_id] = {\"bm25\": (topk_ev_ids_bm25, topk_scores_bm25), \"ce\": (topk_ev_ids_ce, sorted_ce_scores)}\n",
    "\n",
    "        for i, k in enumerate(k_values):\n",
    "            # (precision, recall, F1) for bm25\n",
    "            intersection = set(topk_ev_ids_bm25[:k]).intersection(gold_evidence_list)\n",
    "            p = len(intersection) / len(topk_ev_ids_bm25[:k])\n",
    "            r = len(intersection) / len(gold_evidence_list)\n",
    "            precision[i,0] += p\n",
    "            recall[i,0] += r\n",
    "            f1[i,0] += (2*p*r/(p + r)) if (p+r) > 0 else 0\n",
    "\n",
    "            # Compute Average Precision (AP) for bm25\n",
    "            precisions_bm25 = []\n",
    "            for j in range(k):\n",
    "                intersection = set(topk_ev_ids_bm25[:j+1]).intersection(gold_evidence_list)\n",
    "                if intersection:\n",
    "                    precisions_bm25.append(len(intersection)/(j+1))\n",
    "            ap_values[i, 0] += sum(precisions_bm25) / len(precisions_bm25) if precisions_bm25 else 0\n",
    "\n",
    "            # (precision, recall, F1) for cross-encoder\n",
    "            intersection = set(topk_ev_ids_ce[:k]).intersection(gold_evidence_list)\n",
    "            p = len(intersection) / len(topk_ev_ids_ce[:k])\n",
    "            r = len(intersection) / len(gold_evidence_list)\n",
    "            precision[i,1] += p\n",
    "            recall[i,1] += r\n",
    "            f1[i,1] += (2*p*r/(p + r)) if (p+r) > 0 else 0\n",
    "\n",
    "            # Compute Average Precision (AP) for cross-encoder\n",
    "            precisions_ce = []\n",
    "            for j in range(k):\n",
    "                intersection = set(topk_ev_ids_ce[:j+1]).intersection(gold_evidence_list)\n",
    "                if intersection:\n",
    "                    precisions_ce.append(len(intersection)/(j+1))\n",
    "            ap_values[i, 1] += sum(precisions_ce) / len(precisions_ce) if precisions_ce else 0\n",
    "\n",
    "            if mixed:\n",
    "                # (precision, recall, F1) for 1/2 bm25 + 1/2 cross-encoder\n",
    "                #topk_ev_ids_mixed = topk_ev_ids_bm25[:max(1,k//2)] + topk_ev_ids_ce[:max(1,k//2)]\n",
    "                topk_ev_ids_mixed = []\n",
    "                for ii in range(k):\n",
    "                    topk_ev_ids_mixed.append(topk_ev_ids_bm25[ii])\n",
    "                    topk_ev_ids_mixed.append(topk_ev_ids_ce[ii])\n",
    "                # remove duplicates while maintaining order\n",
    "                topk_ev_ids_mixed = list(dict.fromkeys(topk_ev_ids_mixed))[:k]           \n",
    "\n",
    "                intersection = set(topk_ev_ids_mixed).intersection(gold_evidence_list)\n",
    "                p = len(intersection) / len(topk_ev_ids_mixed)\n",
    "                r = len(intersection) / len(gold_evidence_list)\n",
    "                precision[i,2] += p\n",
    "                recall[i,2] += r\n",
    "                f1[i,2] += (2*p*r/(p + r)) if (p+r) > 0 else 0\n",
    "\n",
    "                # Compute Average Precision (AP) for 1/2 bm25 + 1/2 cross-encoder\n",
    "                precisions_mix = []\n",
    "                for j in range(k):\n",
    "                    #intersection = set(topk_ev_ids_bm25[:max(1,j//2)] + topk_ev_ids_ce[:max(1,j//2)]).intersection(gold_evidence_list)\n",
    "                    intersection = set(topk_ev_ids_mixed[:j+1]).intersection(gold_evidence_list)\n",
    "                    if intersection:\n",
    "                        precisions_mix.append(len(intersection)/(j+1))\n",
    "                ap_values[i, 2] += sum(precisions_mix) / len(precisions_mix) if precisions_mix else 0        \n",
    "\n",
    "    # Compute Mean Average Precision (MAP)\n",
    "    map_values = ap_values / len(claims_dataset)\n",
    "    # average over all claims\n",
    "    precision = precision / len(claims_dataset) \n",
    "    recall = recall / len(claims_dataset)\n",
    "    f1 = f1 / len(claims_dataset)\n",
    "\n",
    "    # convert to dictionary\n",
    "    if mixed:\n",
    "        metrics = {k:{\"precision\":{\"bm25\":precision[i,0], \"cross_encoder\":precision[i,1], \"mixed\":precision[i,2]}, \"recall\":{\"bm25\":recall[i,0], \"cross_encoder\":recall[i,1], \"mixed\":recall[i,2]}, \"f1\": {\"bm25\":f1[i,0], \"cross_encoder\":f1[i,1], \"mixed\":f1[i,2]}, \"map\": {\"bm25\":map_values[i,0], \"cross_encoder\":map_values[i,1], \"mixed\":map_values[i,2]}} for i,k in enumerate(k_values)}\n",
    "    else:\n",
    "        metrics = {k:{\"precision\":{\"bm25\":precision[i,0], \"cross_encoder\":precision[i,1]}, \"recall\":{\"bm25\":recall[i,0], \"cross_encoder\":recall[i,1]}, \"f1\": {\"bm25\":f1[i,0], \"cross_encoder\":f1[i,1]}, \"map\": {\"bm25\":map_values[i,0], \"cross_encoder\":map_values[i,1]}} for i,k in enumerate(k_values)}\n",
    "\n",
    "    return metrics, topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [24:03<00:00,  9.37s/it]\n"
     ]
    }
   ],
   "source": [
    "k_values = [2, 4, 6, 10, 20, 50, 100, 200, 300, 500, 1000]\n",
    "metrics, topk = reranking_eval(val_data, bm25, model, k_values, mixed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2\n",
      "\t bm25: precision --> 0.0882, recall -> 0.1176, f1 -> 0.0980, mAP --> 0.0882\n",
      "\t c-e:  precision --> 0.1176, recall -> 0.0529, f1 -> 0.0728, mAP --> 0.1471\n",
      "\t mixed:  precision --> 0.0882, recall -> 0.0559, f1 -> 0.0658, mAP --> 0.1029\n",
      "k = 4\n",
      "\t bm25: precision --> 0.0735, recall -> 0.1441, f1 -> 0.0905, mAP --> 0.0997\n",
      "\t c-e:  precision --> 0.1324, recall -> 0.1324, f1 -> 0.1291, mAP --> 0.1442\n",
      "\t mixed:  precision --> 0.1029, recall -> 0.1706, f1 -> 0.1183, mAP --> 0.1283\n",
      "k = 6\n",
      "\t bm25: precision --> 0.0490, recall -> 0.1441, f1 -> 0.0687, mAP --> 0.0814\n",
      "\t c-e:  precision --> 0.1176, recall -> 0.2176, f1 -> 0.1427, mAP --> 0.1442\n",
      "\t mixed:  precision --> 0.0980, recall -> 0.2088, f1 -> 0.1243, mAP --> 0.1167\n",
      "k = 10\n",
      "\t bm25: precision --> 0.0353, recall -> 0.1735, f1 -> 0.0564, mAP --> 0.0675\n",
      "\t c-e:  precision --> 0.0941, recall -> 0.2931, f1 -> 0.1363, mAP --> 0.1365\n",
      "\t mixed:  precision --> 0.0941, recall -> 0.3471, f1 -> 0.1393, mAP --> 0.1148\n",
      "k = 20\n",
      "\t bm25: precision --> 0.0235, recall -> 0.2147, f1 -> 0.0413, mAP --> 0.0511\n",
      "\t c-e:  precision --> 0.0588, recall -> 0.3833, f1 -> 0.0994, mAP --> 0.1066\n",
      "\t mixed:  precision --> 0.0588, recall -> 0.4225, f1 -> 0.0999, mAP --> 0.0950\n",
      "k = 50\n",
      "\t bm25: precision --> 0.0129, recall -> 0.3000, f1 -> 0.0244, mAP --> 0.0312\n",
      "\t c-e:  precision --> 0.0271, recall -> 0.4716, f1 -> 0.0506, mAP --> 0.0657\n",
      "\t mixed:  precision --> 0.0259, recall -> 0.4716, f1 -> 0.0484, mAP --> 0.0595\n",
      "k = 100\n",
      "\t bm25: precision --> 0.0082, recall -> 0.3608, f1 -> 0.0160, mAP --> 0.0207\n",
      "\t c-e:  precision --> 0.0153, recall -> 0.5098, f1 -> 0.0295, mAP --> 0.0437\n",
      "\t mixed:  precision --> 0.0141, recall -> 0.5304, f1 -> 0.0273, mAP --> 0.0391\n",
      "k = 200\n",
      "\t bm25: precision --> 0.0050, recall -> 0.4196, f1 -> 0.0098, mAP --> 0.0134\n",
      "\t c-e:  precision --> 0.0079, recall -> 0.5686, f1 -> 0.0156, mAP --> 0.0271\n",
      "\t mixed:  precision --> 0.0079, recall -> 0.5686, f1 -> 0.0156, mAP --> 0.0253\n",
      "k = 300\n",
      "\t bm25: precision --> 0.0043, recall -> 0.5069, f1 -> 0.0085, mAP --> 0.0109\n",
      "\t c-e:  precision --> 0.0057, recall -> 0.6098, f1 -> 0.0112, mAP --> 0.0201\n",
      "\t mixed:  precision --> 0.0053, recall -> 0.5686, f1 -> 0.0105, mAP --> 0.0188\n",
      "k = 500\n",
      "\t bm25: precision --> 0.0029, recall -> 0.5451, f1 -> 0.0058, mAP --> 0.0079\n",
      "\t c-e:  precision --> 0.0035, recall -> 0.6216, f1 -> 0.0070, mAP --> 0.0137\n",
      "\t mixed:  precision --> 0.0034, recall -> 0.6098, f1 -> 0.0068, mAP --> 0.0129\n",
      "k = 1000\n",
      "\t bm25: precision --> 0.0019, recall -> 0.6657, f1 -> 0.0039, mAP --> 0.0051\n",
      "\t c-e:  precision --> 0.0019, recall -> 0.6657, f1 -> 0.0039, mAP --> 0.0081\n",
      "\t mixed:  precision --> 0.0019, recall -> 0.6657, f1 -> 0.0039, mAP --> 0.0077\n"
     ]
    }
   ],
   "source": [
    "for k, k_metrics in metrics.items():\n",
    "    #print(k_metrics)\n",
    "    print(f\"k = {k}\")\n",
    "    bm25_precision = k_metrics[\"precision\"][\"bm25\"]\n",
    "    bm25_recall = k_metrics[\"recall\"][\"bm25\"]\n",
    "    bm25_f1 = k_metrics[\"f1\"][\"bm25\"]\n",
    "    bm25_map = k_metrics[\"map\"][\"bm25\"]\n",
    "    ce_precision = k_metrics[\"precision\"][\"cross_encoder\"]\n",
    "    ce_recall = k_metrics[\"recall\"][\"cross_encoder\"]\n",
    "    ce_f1 = k_metrics[\"f1\"][\"cross_encoder\"]\n",
    "    ce_map = k_metrics[\"map\"][\"cross_encoder\"]\n",
    "    mixed_precision = k_metrics[\"precision\"][\"mixed\"]\n",
    "    mixed_recall = k_metrics[\"recall\"][\"mixed\"]\n",
    "    mixed_f1 = k_metrics[\"f1\"][\"mixed\"]\n",
    "    mixed_map = k_metrics[\"map\"][\"mixed\"]\n",
    "    print(f\"\\t bm25: precision --> {bm25_precision:.4f}, recall -> {bm25_recall:.4f}, f1 -> {bm25_f1:.4f}, mAP --> {bm25_map:.4f}\")\n",
    "    print(f\"\\t c-e:  precision --> {ce_precision:.4f}, recall -> {ce_recall:.4f}, f1 -> {ce_f1:.4f}, mAP --> {ce_map:.4f}\")\n",
    "    print(f\"\\t mixed:  precision --> {mixed_precision:.4f}, recall -> {mixed_recall:.4f}, f1 -> {mixed_f1:.4f}, mAP --> {mixed_map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Retreival results to file, will need these later for classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save metrics dict to pickle file\n",
    "#with open(\"reranking_metrics_mAP_1.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(metrics, file)\n",
    "\n",
    "# save topk ranked documents to pickle file\n",
    "#with open(\"topk_reranker.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(topk, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load topk ranked documents from pickle file\n",
    "#with open(\"topk_reranker.pkl\", \"rb\") as file:\n",
    "#    topk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Also compute evaluation metrics on Training Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nk_values = [2, 4, 6, 10, 20, 50, 100, 200, 300, 500]\\nmetrics_train, topk_train = reranking_eval(train_data, bm25, model, k_values)\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "k_values = [2, 4, 6, 10, 20, 50, 100, 200, 300, 500]\n",
    "metrics_train, topk_train = reranking_eval(train_data, bm25, model, k_values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
