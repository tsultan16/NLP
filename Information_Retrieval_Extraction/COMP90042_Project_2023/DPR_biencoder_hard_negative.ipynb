{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "from utils import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of training the DPR bi-encoder with in-batch negatives, we will now train exclusively using hard-negatives which where mined by using the cross-encoder to rerank top-k passages retrieved by our old DPR. First, let's set up the dataset and batch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "document_store, train_data, val_data = load_data(clean=True)\n",
    "\n",
    "# load hard negatives from pickle file\n",
    "with open(\"dpr_embeddings/train_hard_negatives_reranked.pkl\", \"rb\") as f:\n",
    "    train_hard_negatives = pickle.load(f)\n",
    "\n",
    "with open(\"dpr_embeddings/val_hard_negatives_reranked.pkl\", \"rb\") as f:\n",
    "    val_hard_negatives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer parallelism to False\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "# now let's create a pytroch dataset\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, hard_negatives, hard_negative_topk=20, num_negatives=10, block_size=128):\n",
    "        self.claims_data = claims_data\n",
    "        self.document_store = document_store\n",
    "        self.hard_negatives = hard_negatives\n",
    "        assert num_negatives % 2 == 0, \"num_negatives must be even\"\n",
    "        self.num_negatives = num_negatives\n",
    "        self.hard_negative_topk = hard_negative_topk\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        all_passages_ids = list(self.document_store.keys())\n",
    "        all_positive_ids = set([ev for claim in self.claims_data.values() for ev in claim['evidences']])\n",
    "        self.all_negatives_ids = list(set(all_passages_ids) - set(all_positive_ids))\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_pairs)\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                # for each positive evidence, sample 1/2 * num_negatives evidences from hard negatives list\n",
    "                # and 1/2 * num_negatives evidences from all negatives list\n",
    "                negative_ids = random.sample(self.hard_negatives[claim_id][:self.hard_negative_topk], self.num_negatives//2)\n",
    "                negative_ids += random.sample(self.all_negatives_ids, self.num_negatives//2)\n",
    "                claim_pairs.append((claim_id, evidence_id, negative_ids))      \n",
    "        # shuffle the instances \n",
    "        random.shuffle(claim_pairs)                \n",
    "        return claim_pairs\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def tokenize_and_encode_claim(self, claim_text, to_tensor=True):\n",
    "        # tokenize  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        # add special tokens and padding\n",
    "        claim_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id]\n",
    "        claim_idx = claim_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(claim_idx))\n",
    "        # create attention masks\n",
    "        claim_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in claim_idx]\n",
    "        if to_tensor:\n",
    "            # convert to tensors\n",
    "            claim_idx = torch.tensor(claim_idx)\n",
    "            claim_attn_mask = torch.tensor(claim_attn_mask)\n",
    "        return claim_idx, claim_attn_mask\n",
    "\n",
    "    def tokenize_and_encode_evidence(self, evidence_text, to_tensor=True):\n",
    "        # tokenize  \n",
    "        evidence_encoding = self.tokenizer.encode_plus(evidence_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        evidence_idx = evidence_encoding['input_ids']\n",
    "        # select a window from the passage if it is longer than block size\n",
    "        if len(evidence_idx) > (self.block_size-2):\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(evidence_idx) - (self.block_size-2)))\n",
    "            # select the window\n",
    "            evidence_idx = evidence_idx[start_pos:start_pos+self.block_size-2]\n",
    "        # add special tokens and padding\n",
    "        evidence_idx = [self.tokenizer.cls_token_id] + evidence_idx + [self.tokenizer.sep_token_id]\n",
    "        evidence_idx = evidence_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(evidence_idx))\n",
    "        # create attention mask\n",
    "        evidence_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in evidence_idx]\n",
    "        if to_tensor:\n",
    "            # convert to tensors\n",
    "            evidence_idx = torch.tensor(evidence_idx)\n",
    "            evidence_attn_mask = torch.tensor(evidence_attn_mask)\n",
    "        return evidence_idx, evidence_attn_mask\n",
    "\n",
    "    def __getitem__(self, idx, to_tensor=True):\n",
    "        # get claim id and positive evidence id\n",
    "        claim_id, positive_id, negative_ids = self.claim_pairs[idx]\n",
    "        # get the claim, positive and negative text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        positive_text = self.document_store[positive_id]\n",
    "        negatives_text = [self.document_store[id] for id in negative_ids]\n",
    "        # tokenize and encode the claim\n",
    "        claim_idx, claim_attn_mask = self.tokenize_and_encode_claim(claim_text, to_tensor=to_tensor)\n",
    "        # tokenize and encode the positive evidence\n",
    "        positive_idx, positive_attn_mask = self.tokenize_and_encode_evidence(positive_text, to_tensor=to_tensor)\n",
    "        # tokenize and encode the negative evidences\n",
    "        negative_idx = []\n",
    "        negative_attn_mask = []\n",
    "        for negative_text in negatives_text:\n",
    "            negative_idx_i, negative_attn_mask_i = self.tokenize_and_encode_evidence(negative_text)\n",
    "            negative_idx.append(negative_idx_i)\n",
    "            negative_attn_mask.append(negative_attn_mask_i) \n",
    "        if to_tensor:    \n",
    "            negative_idx = torch.stack(negative_idx)\n",
    "            negative_attn_mask = torch.stack(negative_attn_mask)  \n",
    "        return claim_idx, claim_attn_mask, positive_idx, positive_attn_mask, negative_idx, negative_attn_mask\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unzip the batch\n",
    "    query_idx, query_attn_mask, pos_idx, pos_attn_mask, neg_idx, neg_attn_mask = zip(*batch)\n",
    "\n",
    "    # Convert to tensors and reshape negatives\n",
    "    query_idx = torch.stack(query_idx)\n",
    "    query_attn_mask = torch.stack(query_attn_mask)\n",
    "    pos_idx = torch.stack(pos_idx)\n",
    "    pos_attn_mask = torch.stack(pos_attn_mask)\n",
    "    # reshape: (batch_size, num_negatives, max_seq_len) ->  (batch_size*num_negatives, max_seq_len)\n",
    "    neg_idx = torch.cat(neg_idx).view(-1, neg_idx[0].shape[-1])  \n",
    "    neg_attn_mask = torch.cat(neg_attn_mask).view(-1, neg_attn_mask[0].shape[-1]) \n",
    "    \n",
    "    return query_idx, query_attn_mask, pos_idx, pos_attn_mask, neg_idx, neg_attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4122 491\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "train_dataset = ClaimsDataset(train_data, document_store, train_hard_negatives, num_negatives=4, block_size=block_size)\n",
    "val_dataset = ClaimsDataset(val_data, document_store, val_hard_negatives, num_negatives=4, block_size=block_size)\n",
    "print(len(train_dataset), len(val_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 132.72576 M\n",
      "RAM used: 2930.82 MB\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=False, pin_memory=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=False, pin_memory=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTBiEncoder(out_of_batch_negs=True).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_dpr_model_checkpoint(model, optimizer, filename='dpr_checkpoint_1.pth')\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Information_Retrieval_Extraction/COMP90042_Project_2023/wandb/run-20240123_011537-l89pyvj1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/l89pyvj1' target=\"_blank\">copper-spaceship-32</a></strong> to <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/l89pyvj1' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/l89pyvj1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"Automated Climate Fact Checker\", \n",
    "    config={\n",
    "        \"bi-encoder model\": \"DistillBERT DPR\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"COMP90042 2023 project\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epochs:   0%|          | 0/258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  6.04it/s] Loss:  0.000, Val Accuracy:  0.000:  38%|███▊      | 99/258 [00:48<01:16,  2.07it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  6.00it/s] Loss:  1.857, Val Accuracy:  0.532:  77%|███████▋  | 199/258 [01:42<00:28,  2.04it/s]\n",
      "Epoch 1, EMA Train Loss: 0.422, Train Accuracy:  0.832, Val Loss:  1.916, Val Accuracy:  0.544: 100%|██████████| 258/258 [02:16<00:00,  1.89it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.95it/s] Loss:  1.916, Val Accuracy:  0.544:  38%|███▊      | 99/258 [00:48<01:17,  2.05it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.89it/s] Loss:  2.051, Val Accuracy:  0.521:  77%|███████▋  | 199/258 [01:42<00:28,  2.04it/s]\n",
      "Epoch 2, EMA Train Loss: 0.387, Train Accuracy:  0.867, Val Loss:  2.077, Val Accuracy:  0.544: 100%|██████████| 258/258 [02:16<00:00,  1.88it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.96it/s] Loss:  2.077, Val Accuracy:  0.544:  38%|███▊      | 99/258 [00:48<01:17,  2.04it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.87it/s] Loss:  2.075, Val Accuracy:  0.540:  77%|███████▋  | 199/258 [01:42<00:28,  2.05it/s]\n",
      "Epoch 3, EMA Train Loss: 0.408, Train Accuracy:  0.892, Val Loss:  2.153, Val Accuracy:  0.532: 100%|██████████| 258/258 [02:16<00:00,  1.89it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.96it/s] Loss:  2.153, Val Accuracy:  0.532:  38%|███▊      | 99/258 [00:48<01:17,  2.04it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.98it/s] Loss:  2.220, Val Accuracy:  0.552:  77%|███████▋  | 199/258 [01:42<00:28,  2.04it/s]\n",
      "Epoch 4, EMA Train Loss: 0.297, Train Accuracy:  0.907, Val Loss:  2.361, Val Accuracy:  0.554: 100%|██████████| 258/258 [02:16<00:00,  1.89it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.95it/s] Loss:  2.361, Val Accuracy:  0.554:  38%|███▊      | 99/258 [00:48<01:17,  2.05it/s]\n",
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.92it/s] Loss:  2.369, Val Accuracy:  0.523:  77%|███████▋  | 199/258 [01:42<00:28,  2.04it/s]\n",
      "Epoch 5, EMA Train Loss: 0.256, Train Accuracy:  0.929, Val Loss:  2.451, Val Accuracy:  0.525: 100%|██████████| 258/258 [02:16<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=5, save_every=None, val_every=100, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val Epochs: 100%|██████████| 31/31 [00:05<00:00,  5.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.9114601612091064, 0.5315682281059063)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_dpr_model_checkpoint(model, optimizer, filename='dpr_checkpoint_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_passage_embeddings(document_store):\n",
    "    # precompute the passage embeddings for all passages in the document store\n",
    "    document_store_list = list(document_store.items())  # Convert the document_store slice into a list\n",
    "    num_passages = len(document_store_list)\n",
    "    passage_embeddings = torch.zeros((num_passages, 768), device=DEVICE)  # Preallocate memory\n",
    "    for i in tqdm(range(0, len(document_store_list), 16)):\n",
    "        # tokenize the passages in this batch\n",
    "        passages_idx_batch = []\n",
    "        passages_attn_mask_batch = []\n",
    "        for _, passage_text in document_store_list[i:i+16]:\n",
    "            passage_idx, passage_attn_mask = train_dataset.tokenize_and_encode_evidence(passage_text)\n",
    "            passages_idx_batch.append(passage_idx)\n",
    "            passages_attn_mask_batch.append(passage_attn_mask)\n",
    "\n",
    "        passages_idx_batch = torch.stack(passages_idx_batch).to(DEVICE)\n",
    "        passages_attn_mask_batch = torch.stack(passages_attn_mask_batch).to(DEVICE)    \n",
    "        passage_embedding = model.encode_passages(passages_idx_batch, passages_attn_mask_batch)\n",
    "        passage_embeddings[i:i+16] = passage_embedding\n",
    "        del passage_embedding, passages_idx_batch, passages_attn_mask_batch  # Delete tensors to free up memory\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory\n",
    "\n",
    "    return passage_embeddings\n",
    "\n",
    "passage_ids = list(document_store.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2231/74416 [01:15<36:44, 32.74it/s] "
     ]
    }
   ],
   "source": [
    "evidence_passage_embeds = precompute_passage_embeddings(document_store)\n",
    "\n",
    "# save precomputed embeddings\n",
    "#torch.save(evidence_passage_embeds, \"dpr_embeddings/evidence_passage_simple_dpr_embeds_1.pt\")\n",
    "\n",
    "# load embeddings from file\n",
    "#evidence_passage_embeds = torch.load(\"dpr_embeddings/evidence_passage_simple_dpr_embeds_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the passage_ids list to the pickle file\n",
    "#with open(\"dpr_embeddings/passage_ids_1.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(passage_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_topk_evidence(claim_text, passage_ids, k=5):\n",
    "    # tokenize claim text\n",
    "    claim_idx, claim_attn_mask = train_dataset.tokenize_and_encode_claim(claim_text)\n",
    "    claim_idx = claim_idx.unsqueeze(0).to(DEVICE)\n",
    "    claim_attn_mask = claim_attn_mask.unsqueeze(0).to(DEVICE)\n",
    "    # get BERT embedding of claim\n",
    "    claim_embedding = model.encode_queries(claim_idx, claim_attn_mask)\n",
    "    # find topk passages \n",
    "    scores = torch.mm(evidence_passage_embeds, claim_embedding.T)\n",
    "    topk_scores, topk_ids = torch.topk(scores.squeeze(1), k=k)\n",
    "    topk_scores = topk_scores.squeeze().tolist()\n",
    "    topk_ids = topk_ids.squeeze().tolist()\n",
    "    # get passage ids\n",
    "    topk_passage_ids = [passage_ids[i] for i in topk_ids]\n",
    "    return topk_passage_ids, topk_scores\n",
    "\n",
    "\n",
    "def eval(claims_list, passage_ids, topk=[5]):\n",
    "    precision_total = np.zeros(len(topk))\n",
    "    recall_total = np.zeros(len(topk))\n",
    "    f1_total = np.zeros(len(topk))\n",
    "\n",
    "    for idx in tqdm(range(len(claims_list))):\n",
    "        claim_text = claims_list[idx][1]['claim_text']\n",
    "        gold_evidence_list = claims_list[idx][1]['evidences']\n",
    "        # predict topk passages using model\n",
    "        topk_passage_ids, topk_scores = find_topk_evidence(claim_text, passage_ids, k=max(topk))\n",
    "        for i,k in enumerate(topk):\n",
    "            topk_passage_ids_k = topk_passage_ids[:k]\n",
    "            # evaluation (precision, recall, F1)\n",
    "            intersection = set(topk_passage_ids_k).intersection(gold_evidence_list)\n",
    "            precision = len(intersection) / len(topk_passage_ids_k)\n",
    "            recall = len(intersection) / len(gold_evidence_list)\n",
    "            f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "            precision_total[i] += precision\n",
    "            recall_total[i] += recall\n",
    "            f1_total[i] += f1\n",
    "\n",
    "    precision_avg = precision_total / len(claims_list)\n",
    "    recall_avg = recall_total / len(claims_list)\n",
    "    f1_avg = f1_total / len(claims_list)  \n",
    "\n",
    "    # convert to dictionary\n",
    "    precision_avg = {f\"Precision@{k}\":v for k,v in zip(topk, precision_avg)}\n",
    "    recall_avg = {f\"Recall@{k}\":v for k,v in zip(topk, recall_avg)}\n",
    "    f1_avg = {f\"F1@{k}\":v for k,v in zip(topk, f1_avg)}  \n",
    "\n",
    "    print(f\"\\nAvg Precision: {precision_avg}, Avg Recall: {recall_avg}, Avg F1: {f1_avg}\")\n",
    "    return precision_avg, recall_avg, f1_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test the model on a few claims\n",
    "claims_list_train = list(train_data.items()) \n",
    "claims_list_val = list(val_data.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval on training set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1228 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:26<00:00, 45.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: {'Precision@3': 0.1921824104234536, 'Precision@5': 0.15781758957654662, 'Precision@8': 0.12347312703583062, 'Precision@10': 0.11009771986970632, 'Precision@15': 0.08566775244299726, 'Precision@20': 0.07219055374592778, 'Precision@30': 0.05483170466883844, 'Precision@50': 0.038387622149837104, 'Precision@100': 0.02237785016286649, 'Precision@250': 0.010576547231270187, 'Precision@500': 0.00572801302931587}, Avg Recall: {'Recall@3': 0.18813789359391947, 'Recall@5': 0.25578175895765415, 'Recall@8': 0.32005971769815356, 'Recall@10': 0.3551845819761122, 'Recall@15': 0.40754614549402746, 'Recall@20': 0.4576954397394132, 'Recall@30': 0.5147122692725297, 'Recall@50': 0.5997692725298588, 'Recall@100': 0.6955890336590655, 'Recall@250': 0.8146579804560249, 'Recall@500': 0.8741313789359376}, Avg F1: {'F1@3': 0.17770280750736822, 'F1@5': 0.18370042914016857, 'F1@8': 0.16939439496442865, 'F1@10': 0.16075740220691445, 'F1@15': 0.13689085454743155, 'F1@20': 0.12130079039121686, 'F1@30': 0.09715713061710846, 'F1@50': 0.0712314593853206, 'F1@100': 0.04306475327780521, 'F1@250': 0.020823191533789583, 'F1@500': 0.01136526904614187}\n",
      "\n",
      "Eval on validation set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:03<00:00, 46.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: {'Precision@3': 0.07359307359307361, 'Precision@5': 0.062337662337662345, 'Precision@8': 0.05113636363636364, 'Precision@10': 0.04740259740259738, 'Precision@15': 0.03766233766233767, 'Precision@20': 0.030844155844155827, 'Precision@30': 0.02402597402597403, 'Precision@50': 0.018441558441558457, 'Precision@100': 0.011688311688311696, 'Precision@250': 0.006233766233766238, 'Precision@500': 0.003714285714285717}, Avg Recall: {'Recall@3': 0.07337662337662339, 'Recall@5': 0.11699134199134197, 'Recall@8': 0.15108225108225104, 'Recall@10': 0.1713203463203463, 'Recall@15': 0.20357142857142851, 'Recall@20': 0.218073593073593, 'Recall@30': 0.25541125541125537, 'Recall@50': 0.3254329004329004, 'Recall@100': 0.4024891774891775, 'Recall@250': 0.5054112554112555, 'Recall@500': 0.6012987012987012}, Avg F1: {'F1@3': 0.0701762523191095, 'F1@5': 0.076149247577819, 'F1@8': 0.0725274725274725, 'F1@10': 0.070787653904537, 'F1@15': 0.061344537815126006, 'F1@20': 0.05258230852527861, 'F1@30': 0.0430685804047025, 'F1@50': 0.03444213439673027, 'F1@100': 0.02255479121111367, 'F1@250': 0.012281182557798788, 'F1@500': 0.007372425411137104}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Precision@3': 0.07359307359307361,\n",
       "  'Precision@5': 0.062337662337662345,\n",
       "  'Precision@8': 0.05113636363636364,\n",
       "  'Precision@10': 0.04740259740259738,\n",
       "  'Precision@15': 0.03766233766233767,\n",
       "  'Precision@20': 0.030844155844155827,\n",
       "  'Precision@30': 0.02402597402597403,\n",
       "  'Precision@50': 0.018441558441558457,\n",
       "  'Precision@100': 0.011688311688311696,\n",
       "  'Precision@250': 0.006233766233766238,\n",
       "  'Precision@500': 0.003714285714285717},\n",
       " {'Recall@3': 0.07337662337662339,\n",
       "  'Recall@5': 0.11699134199134197,\n",
       "  'Recall@8': 0.15108225108225104,\n",
       "  'Recall@10': 0.1713203463203463,\n",
       "  'Recall@15': 0.20357142857142851,\n",
       "  'Recall@20': 0.218073593073593,\n",
       "  'Recall@30': 0.25541125541125537,\n",
       "  'Recall@50': 0.3254329004329004,\n",
       "  'Recall@100': 0.4024891774891775,\n",
       "  'Recall@250': 0.5054112554112555,\n",
       "  'Recall@500': 0.6012987012987012},\n",
       " {'F1@3': 0.0701762523191095,\n",
       "  'F1@5': 0.076149247577819,\n",
       "  'F1@8': 0.0725274725274725,\n",
       "  'F1@10': 0.070787653904537,\n",
       "  'F1@15': 0.061344537815126006,\n",
       "  'F1@20': 0.05258230852527861,\n",
       "  'F1@30': 0.0430685804047025,\n",
       "  'F1@50': 0.03444213439673027,\n",
       "  'F1@100': 0.02255479121111367,\n",
       "  'F1@250': 0.012281182557798788,\n",
       "  'F1@500': 0.007372425411137104})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation on train dataset for top-k reranked passages\n",
    "print(f\"Eval on training set:\")\n",
    "eval(claims_list_train, passage_ids, topk=[3,5,8,10,15,20,30,50,100,250,500])\n",
    "\n",
    "print(f\"\\nEval on validation set:\")\n",
    "eval(claims_list_val, passage_ids, topk=[3,5,8,10,15,20,30,50,100,250,500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
