{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "from utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instead of training the DPR bi-encoder with in-batch negatives, we will now train exclusively using hard-negatives which where mined by using the cross-encoder to rerank top-k passages retrieved by our old DPR. First, let's set up the dataset and batch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "document_store, train_data, val_data = load_data(clean=True)\n",
    "\n",
    "# load hard negatives from pickle file\n",
    "with open(\"dpr_embeddings/train_hard_negatives_reranked.pkl\", \"rb\") as f:\n",
    "    train_hard_negatives = pickle.load(f)\n",
    "\n",
    "with open(\"dpr_embeddings/val_hard_negatives_reranked.pkl\", \"rb\") as f:\n",
    "    val_hard_negatives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer parallelism to False\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "# now let's create a pytroch dataset\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, hard_negatives, num_negatives=10, block_size=128):\n",
    "        self.claims_data = claims_data\n",
    "        self.document_store = document_store\n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.num_negatives = num_negatives\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_pairs)\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                # for each positive evidence, sample num_negatives evidences from hard negatives list\n",
    "                negative_ids = random.sample(self.hard_negatives[claim_id], self.num_negatives)\n",
    "                claim_pairs.append((claim_id, evidence_id, negative_ids))      \n",
    "        # shuffle the instances \n",
    "        random.shuffle(claim_pairs)                \n",
    "        return claim_pairs\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def tokenize_and_encode_claim(self, claim_text, to_tensor=True):\n",
    "        # tokenize  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        # add special tokens and padding\n",
    "        claim_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id]\n",
    "        claim_idx = claim_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(claim_idx))\n",
    "        # create attention masks\n",
    "        claim_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in claim_idx]\n",
    "        if to_tensor:\n",
    "            # convert to tensors\n",
    "            claim_idx = torch.tensor(claim_idx)\n",
    "            claim_attn_mask = torch.tensor(claim_attn_mask)\n",
    "        return claim_idx, claim_attn_mask\n",
    "\n",
    "    def tokenize_and_encode_evidence(self, evidence_text, to_tensor=True):\n",
    "        # tokenize  \n",
    "        evidence_encoding = self.tokenizer.encode_plus(evidence_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        evidence_idx = evidence_encoding['input_ids']\n",
    "        # select a window from the passage if it is longer than block size\n",
    "        if len(evidence_idx) > (self.block_size-2):\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(evidence_idx) - (self.block_size-2)))\n",
    "            # select the window\n",
    "            evidence_idx = evidence_idx[start_pos:start_pos+self.block_size-2]\n",
    "        # add special tokens and padding\n",
    "        evidence_idx = [self.tokenizer.cls_token_id] + evidence_idx + [self.tokenizer.sep_token_id]\n",
    "        evidence_idx = evidence_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(evidence_idx))\n",
    "        # create attention mask\n",
    "        evidence_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in evidence_idx]\n",
    "        if to_tensor:\n",
    "            # convert to tensors\n",
    "            evidence_idx = torch.tensor(evidence_idx)\n",
    "            evidence_attn_mask = torch.tensor(evidence_attn_mask)\n",
    "        return evidence_idx, evidence_attn_mask\n",
    "\n",
    "    def __getitem__(self, idx, to_tensor=True):\n",
    "        # get claim id and positive evidence id\n",
    "        claim_id, positive_id, negative_ids = self.claim_pairs[idx]\n",
    "        # get the claim, positive and negative text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        positive_text = self.document_store[positive_id]\n",
    "        negatives_text = [self.document_store[id] for id in negative_ids]\n",
    "        # tokenize and encode the claim\n",
    "        claim_idx, claim_attn_mask = self.tokenize_and_encode_claim(claim_text, to_tensor=to_tensor)\n",
    "        # tokenize and encode the positive evidence\n",
    "        positive_idx, positive_attn_mask = self.tokenize_and_encode_evidence(positive_text, to_tensor=to_tensor)\n",
    "        # tokenize and encode the negative evidences\n",
    "        negative_idx = []\n",
    "        negative_attn_mask = []\n",
    "        for negative_text in negatives_text:\n",
    "            negative_idx_i, negative_attn_mask_i = self.tokenize_and_encode_evidence(negative_text)\n",
    "            negative_idx.append(negative_idx_i)\n",
    "            negative_attn_mask.append(negative_attn_mask_i) \n",
    "        if to_tensor:    \n",
    "            negative_idx = torch.stack(negative_idx)\n",
    "            negative_attn_mask = torch.stack(negative_attn_mask)  \n",
    "        return claim_idx, claim_attn_mask, positive_idx, positive_attn_mask, negative_idx, negative_attn_mask\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unzip the batch\n",
    "    query_idx, query_attn_mask, pos_idx, pos_attn_mask, neg_idx, neg_attn_mask = zip(*batch)\n",
    "\n",
    "    # Convert to tensors and reshape negatives\n",
    "    query_idx = torch.stack(query_idx)\n",
    "    query_attn_mask = torch.stack(query_attn_mask)\n",
    "    pos_idx = torch.stack(pos_idx)\n",
    "    pos_attn_mask = torch.stack(pos_attn_mask)\n",
    "    # reshape: (batch_size, num_negatives, max_seq_len) ->  (batch_size*num_negatives, max_seq_len)\n",
    "    neg_idx = torch.cat(neg_idx).view(-1, neg_idx[0].shape[-1])  \n",
    "    neg_attn_mask = torch.cat(neg_attn_mask).view(-1, neg_attn_mask[0].shape[-1]) \n",
    "    \n",
    "    return query_idx, query_attn_mask, pos_idx, pos_attn_mask, neg_idx, neg_attn_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4122 491\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "train_dataset = ClaimsDataset(train_data, document_store, train_hard_negatives, num_negatives=4, block_size=block_size)\n",
    "val_dataset = ClaimsDataset(val_data, document_store, val_hard_negatives, num_negatives=4, block_size=block_size)\n",
    "print(len(train_dataset), len(val_dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 132.72576 M\n",
      "RAM used: 2927.67 MB\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=False, pin_memory=True, num_workers=2, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=False, pin_memory=True, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTBiEncoder(out_of_batch_negs=True).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Information_Retrieval_Extraction/COMP90042_Project_2023/wandb/run-20240122_193532-arq03sjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/arq03sjv' target=\"_blank\">volcanic-galaxy-28</a></strong> to <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/arq03sjv' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/arq03sjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"Automated Climate Fact Checker\", \n",
    "    config={\n",
    "        \"bi-encoder model\": \"DistillBERT DPR\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"COMP90042 2023 project\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epochs:   0%|          | 0/258 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=10, save_every=None, val_every=100, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
