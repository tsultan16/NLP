{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous notebook, we tried out a simple BM25 retreiveal system and saw that it performed poorly. The average F1 score for top-5 retreival was only about 9%. Now, we will train a Dense Passage Retrieval (DPR) model and see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from DPR_biencoder_simple import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data and do filter out some of the bad passages like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the evidence passages\n",
    "with open(\"project-data/evidence.json\", \"r\") as train_file:\n",
    "    document_store = json.load(train_file)         \n",
    "print(f\"Number of evidence passages: {len(document_store)}\")\n",
    "\n",
    "# load the training data insttances\n",
    "with open(\"project-data/train-claims.json\", \"r\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "print(f\"Number of training instances: {len(train_data)}\")\n",
    "\n",
    "# load the validation data instances\n",
    "with open(\"project-data/dev-claims.json\", \"r\") as dev_file:\n",
    "    val_data = json.load(dev_file)    \n",
    "print(f\"Number of validation instances: {len(val_data)}\")\n",
    "\n",
    "# we remove duplicate values from the document_store dictionary (we arbitrarily keep the first one)\n",
    "seen = set()\n",
    "document_store_no_duplicates = {}\n",
    "for key, value in document_store.items():\n",
    "    if value not in seen:\n",
    "        document_store_no_duplicates[key] = value\n",
    "        seen.add(value)\n",
    "\n",
    "# remove all \"bad\" documents from the document store, except those that occur in claim gold evidence lists, we will define \"bad\" documents as ones that have less than 50 characters\n",
    "claim_evidence_list = [claim['evidences'] for claim in train_data.values()]\n",
    "claim_evidence_list = claim_evidence_list + [claim['evidences'] for claim in val_data.values()]\n",
    "claim_evidence_list = list(set([evidence for evidence_list in claim_evidence_list for evidence in evidence_list]))\n",
    "\n",
    "document_store_cleaned = {i: evidence_text for i, evidence_text in document_store_no_duplicates.items() if len(evidence_text) >= 30 or i in claim_evidence_list}\n",
    "print(f\"Number of evidence passages remaining after cleaning: {len(document_store_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique evidence passages in training data: 3121\n",
      "Number of unique evidence passages in validation data: 463\n"
     ]
    }
   ],
   "source": [
    "claim_evidence_list_train = set([ev for claim in train_data.values() for ev in claim['evidences']])\n",
    "claim_evidence_list_val = set([ev for claim in val_data.values() for ev in claim['evidences']])\n",
    "print(f\"Number of unique evidence passages in training data: {len(claim_evidence_list_train)}\")\n",
    "print(f\"Number of unique evidence passages in validation data: {len(claim_evidence_list_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the DPR retriever, we need to create pairs of $(\\text{claim}_i, \\text{evidence passage}_{i,1})$. However each claim can have multiple evidence passages, so we will create multiples pairs: $(\\text{claim}_i, \\text{evidence passage}_{i,1})$, $(\\text{claim}_i, \\text{evidence passage}_{i,2})$, ...\n",
    "\n",
    "Next, we prepare a minibatch of claims and corresponding passages. Then given a matrix $C$ of shape $(B,d)$ containing the batch of encoded claim vectors (where $d$ is the hidden dimensions of the encoded vectors) and a matrix $P$ of the same shape containing the batch of encoded passage vectors, we can compute the matrix $CP^T$ of shape $(B,B)$ whose $(i,j)th$ entry given us the dot product between the ith question and the jth passage. The elements along the diagonal of $CP^T$ are the scores for positive pairs and off-diagonal entries are for negative pairs. Then we can train a softmax classifier to classify the diagonal term in each row as the score for the positive class and the remaining B-1 terms as non-positive classes. This is the trick of `in-batch negatves` \n",
    "\n",
    "However, one issue is that out of the ~1M different passages, only ~3000 of them appear as positive evidences in (claim, evidence) pairs. Therefore, the in-batch negatives will also be restricted to these few passages. For good performance, we need to be able to select negatives from the all of the evidence passages from the document store. That's why in addition to passing a minibatch of claims $C$ and corresponding positive passages $P$, we will also pass in a batch of negatives $N$ which are selectled from the set of passgaes outside of the 3000 that appear as positives. Then we compute the matrix $CN^T$ which has shape $(B,B)$. All terms in the $ith$ row of this matrix are scores for the $ith$ claim with $B$ negatives. Then by horizontally concatenating: $[CP^T; CN^T]$, we get a matrix of shape $(B,2B)$ and we can just train a softmax classifier to classify the diagonal term in each row as the score for the positive class and the remaining 2B-1 terms as non-positive classes.\n",
    "\n",
    "`Hard-negative mining`: We could simply just create a batch of N negatives by randomly sampling the set fo all negatives. However, a slightly better option would be to select the \"hard\" negatives. These are passages which are very similar to the positive ones. We could either use the highest scoring non-positive documents from a BM25 retreiver as hard negatives. Or we could first train our model with random negative selection. Then use the highest scoring non-positives from our trained model as negatives and do some finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate out all ~3000 positive evidence passages from the document store and define the remaining as negatives.\n",
    "all_passages_ids = list(document_store_cleaned.keys())\n",
    "positives_ids_train = claim_evidence_list_train\n",
    "negatives_ids_train = list(set(all_passages_ids) - set(positives_ids_train))\n",
    "                         \n",
    "# create claim-positive pairs\n",
    "claim_positive_pairs_train = []\n",
    "for claim_id in train_data.keys():\n",
    "    for evidence_id in train_data[claim_id]['evidences']:\n",
    "        claim_positive_pairs_train.append((claim_id, evidence_id))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a pytroch dataset\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, block_size=256):\n",
    "        self.claims_data = claims_data\n",
    "        self.document_store = document_store\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_evidence_list = set([ev for claim in self.claims_data.values() for ev in claim['evidences']])\n",
    "        all_passages_ids = list(document_store.keys())\n",
    "        positives_ids = claim_evidence_list\n",
    "        negatives_ids = list(set(all_passages_ids) - set(positives_ids))\n",
    "        claim_positive_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_positive_pairs.append((claim_id, evidence_id))    \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the question and context passage\n",
    "        q = self.questions[idx][0]\n",
    "        passage_idx = self.questions[idx][1]\n",
    "        question = q['question']\n",
    "        passage = self.passages[passage_idx]\n",
    "        # tokenize the context passage\n",
    "        passage_encoding = self.tokenizer.encode_plus(passage, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        passage_idx = passage_encoding['input_ids']\n",
    "        # tokenize the question\n",
    "        question_encoding = self.tokenizer.encode_plus(question, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        question_idx = question_encoding['input_ids']\n",
    "\n",
    "        # get answer span start and end character positions, for multiple answers, we will only use the first answer\n",
    "        first_answer_idx = 0\n",
    "        answer_start_char = q['answers'][first_answer_idx]['answer_start']\n",
    "        answer_end_char = answer_start_char + len(q['answers'][first_answer_idx]['text'])\n",
    "        # convert char positions to token positions\n",
    "        answer_start_token = passage_encoding.char_to_token(answer_start_char)\n",
    "        answer_end_token = passage_encoding.char_to_token(answer_end_char-1)\n",
    "\n",
    "        # select a window size so that the passage sequence will be no longer than the block size\n",
    "        window_size_tokens = self.block_size - 2 # 2 special tokens ([CLS], [SEP])\n",
    "        # now create a window around the answer span, pick the window start position randomly\n",
    "        window_start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "        window_start_max = min(answer_start_token, max(0,len(passage_idx) - window_size_tokens)) # we want to make the window as large as possible, but not go over the end of the context\n",
    "        window_start = random.randint(window_start_min, window_start_max)\n",
    "        window_end = window_start + window_size_tokens        \n",
    "        # select window of passage tokens\n",
    "        passage_window_tokens = passage_idx[window_start:window_end]\n",
    "\n",
    "        # create padded query and passage sequences\n",
    "        question_idx = [self.tokenizer.cls_token_id] + question_idx + [self.tokenizer.sep_token_id]\n",
    "        question_idx = question_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(question_idx))\n",
    "        passage_idx = [self.tokenizer.cls_token_id] + passage_window_tokens + [self.tokenizer.sep_token_id]\n",
    "        passage_idx = passage_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(passage_idx))\n",
    "\n",
    "        # make sure the passage sequence and query sequences are not longer than max_length\n",
    "        if len(question_idx) > self.block_size or len(passage_idx) > self.block_size:\n",
    "            raise Exception(f\"Passage sequence length {len(passage_idx)} or question sequence length {len(question_idx)} is longer than max_length {self.block_size}!\")\n",
    "        \n",
    "        # create attention masks\n",
    "        question_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in question_idx]\n",
    "        passage_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in passage_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        question_idx = torch.tensor(question_idx)\n",
    "        question_attn_mask = torch.tensor(question_attn_mask)\n",
    "        passage_idx = torch.tensor(passage_idx)\n",
    "        passage_attn_mask = torch.tensor(passage_attn_mask)\n",
    "\n",
    "        return question_idx, question_attn_mask, passage_idx, passage_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
