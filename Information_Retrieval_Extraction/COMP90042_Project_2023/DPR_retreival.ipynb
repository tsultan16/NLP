{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous notebook, we tried out a simple BM25 retreiveal system and saw that it performed poorly. The average F1 score for top-5 retreival was only about 9%. Now, we will train a Dense Passage Retrieval (DPR) model and see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data and do filter out some of the bad passages like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the evidence passages\n",
    "with open(\"project-data/evidence.json\", \"r\") as train_file:\n",
    "    document_store = json.load(train_file)         \n",
    "print(f\"Number of evidence passages: {len(document_store)}\")\n",
    "\n",
    "# load the training data insttances\n",
    "with open(\"project-data/train-claims.json\", \"r\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "print(f\"Number of training instances: {len(train_data)}\")\n",
    "\n",
    "# load the validation data instances\n",
    "with open(\"project-data/dev-claims.json\", \"r\") as dev_file:\n",
    "    val_data = json.load(dev_file)    \n",
    "print(f\"Number of validation instances: {len(val_data)}\")\n",
    "\n",
    "# we remove duplicate values from the document_store dictionary (we arbitrarily keep the first one)\n",
    "seen = set()\n",
    "document_store_no_duplicates = {}\n",
    "for key, value in document_store.items():\n",
    "    if value not in seen:\n",
    "        document_store_no_duplicates[key] = value\n",
    "        seen.add(value)\n",
    "\n",
    "# remove all \"bad\" documents from the document store, except those that occur in claim gold evidence lists, we will define \"bad\" documents as ones that have less than 50 characters\n",
    "claim_evidence_list = [claim['evidences'] for claim in train_data.values()]\n",
    "claim_evidence_list = claim_evidence_list + [claim['evidences'] for claim in val_data.values()]\n",
    "claim_evidence_list = list(set([evidence for evidence_list in claim_evidence_list for evidence in evidence_list]))\n",
    "\n",
    "document_store_cleaned = {i: evidence_text for i, evidence_text in document_store_no_duplicates.items() if len(evidence_text) >= 30 or i in claim_evidence_list}\n",
    "print(f\"Number of evidence passages remaining after cleaning: {len(document_store_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique evidence passages in training data: 3121\n",
      "Number of unique evidence passages in validation data: 463\n"
     ]
    }
   ],
   "source": [
    "claim_evidence_list_train = set([ev for claim in train_data.values() for ev in claim['evidences']])\n",
    "claim_evidence_list_val = set([ev for claim in val_data.values() for ev in claim['evidences']])\n",
    "print(f\"Number of unique evidence passages in training data: {len(claim_evidence_list_train)}\")\n",
    "print(f\"Number of unique evidence passages in validation data: {len(claim_evidence_list_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the DPR retriever, we need to create pairs of $(\\text{claim}_i, \\text{evidence passage}_{i,1})$. However each claim can have multiple evidence passages, so we will create multiples pairs: $(\\text{claim}_i, \\text{evidence passage}_{i,1})$, $(\\text{claim}_i, \\text{evidence passage}_{i,2})$, ...\n",
    "\n",
    "Next, we prepare a minibatch of claims and corresponding passages. Then given a matrix $C$ of shape $(B,d)$ containing the batch of encoded claim vectors (where $d$ is the hidden dimensions of the encoded vectors) and a matrix $P$ of the same shape containing the batch of encoded passage vectors, we can compute the matrix $CP^T$ of shape $(B,B)$ whose $(i,j)th$ entry given us the dot product between the ith question and the jth passage. The elements along the diagonal of $CP^T$ are the scores for positive pairs and off-diagonal entries are for negative pairs. Then we can train a softmax classifier to classify the diagonal term in each row as the score for the positive class and the remaining B-1 terms as non-positive classes. This is the trick of `in-batch negatves` \n",
    "\n",
    "However, one issue is that out of the ~1M different passages, only ~3000 of them appear as positive evidences in (claim, evidence) pairs. Therefore, the in-batch negatives will also be restricted to these few passages. For good performance, we need to be able to select negatives from the all of the evidence passages from the document store. That's why in addition to passing a minibatch of claims $C$ and corresponding positive passages $P$, we will also pass in a batch of negatives $N$ which are selectled from the set of passgaes outside of the 3000 that appear as positives. Then we compute the matrix $CN^T$ which has shape $(B,B)$. All terms in the $ith$ row of this matrix are scores for the $ith$ claim with $B$ negatives. Then by horizontally concatenating: $[CP^T; CN^T]$, we get a matrix of shape $(B,2B)$ and we can just train a softmax classifier to classify the diagonal term in each row as the score for the positive class and the remaining 2B-1 terms as non-positive classes.\n",
    "\n",
    "`Hard-negative mining`: We could simply just create a batch of N negatives by randomly sampling the set of all negatives. However, a slightly better option would be to select the \"hard\" negatives. These are passages which are very similar to the positive ones. We could either use the highest scoring non-positive documents from a BM25 retreiver as hard negatives. Or we could first train our model with random negative selection. Then use the highest scoring non-positives from our trained model as negatives and do some finetuning. \n",
    "\n",
    "Note: Because we need to use fixed size passages for BERT, if a passage exceeds the block size, we will take a random window of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate out all ~3000 positive evidence passages from the document store and define the remaining as negatives.\n",
    "all_passages_ids = list(document_store_cleaned.keys())\n",
    "positives_ids_train = claim_evidence_list_train\n",
    "negatives_ids_train = list(set(all_passages_ids) - set(positives_ids_train))\n",
    "                         \n",
    "# create claim-positive pairs\n",
    "claim_positive_pairs_train = []\n",
    "for claim_id in train_data.keys():\n",
    "    for evidence_id in train_data[claim_id]['evidences']:\n",
    "        claim_positive_pairs_train.append((claim_id, evidence_id))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a pytroch dataset\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, block_size=128):\n",
    "        self.claims_data = claims_data\n",
    "        self.document_store = document_store\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.claim_positive_pairs, self.negatives_ids = self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_evidence_list = set([ev for claim in self.claims_data.values() for ev in claim['evidences']])\n",
    "        all_passages_ids = list(self.document_store.keys())\n",
    "        positives_ids = claim_evidence_list\n",
    "        negatives_ids = list(set(all_passages_ids) - set(positives_ids))\n",
    "        claim_positive_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_positive_pairs.append((claim_id, evidence_id))    \n",
    "        return claim_positive_pairs, negatives_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_positive_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim id and positive evidence id\n",
    "        claim_id, positive_id = self.claim_positive_pairs[idx]\n",
    "        # randomly sample a negative evidence id\n",
    "        negative_id = random.choice(self.negatives_ids)\n",
    "        # get the claim, positive and negative text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        positive_text = self.document_store[positive_id]\n",
    "        negative_text = self.document_store[negative_id]\n",
    "\n",
    "        # tokenize the claim, positive and negative text  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        \n",
    "        positive_encoding = self.tokenizer.encode_plus(positive_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        positive_idx = positive_encoding['input_ids']\n",
    "        \n",
    "        negative_encoding = self.tokenizer.encode_plus(negative_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        negative_idx = negative_encoding['input_ids']\n",
    "\n",
    "        # select a window from the positive passage if it is longer than block size\n",
    "        if len(positive_idx) > (self.block_size-2):\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(positive_idx) - (self.block_size-2)))\n",
    "            # select the window\n",
    "            positive_idx = positive_idx[start_pos:start_pos+self.block_size-2]\n",
    "\n",
    "        # select a window from the negative passage if it is longer than block size\n",
    "        if len(negative_idx) > (self.block_size-2):\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(negative_idx) - (self.block_size-2)))\n",
    "            # select the window\n",
    "            negative_idx = negative_idx[start_pos:start_pos+self.block_size-2]    \n",
    "\n",
    "        # add special tokens and padding\n",
    "        claim_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id]\n",
    "        claim_idx = claim_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(claim_idx))\n",
    "        positive_idx = [self.tokenizer.cls_token_id] + positive_idx + [self.tokenizer.sep_token_id]\n",
    "        positive_idx = positive_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(positive_idx))\n",
    "        negative_idx = [self.tokenizer.cls_token_id] + negative_idx + [self.tokenizer.sep_token_id]\n",
    "        negative_idx = negative_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(negative_idx))\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(claim_idx) > self.block_size or len(positive_idx) > self.block_size or len(negative_idx) > self.block_size:\n",
    "            raise Exception(f\"Claim sequence length {len(claim_idx)} or positive sequence length {len(positive_idx)} or negative sequence length: {len(negative_idx)} is longer than max_length {self.block_size}!\")\n",
    "        \n",
    "        # create attention masks\n",
    "        claim_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in claim_idx]\n",
    "        positive_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in positive_idx]\n",
    "        negative_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in negative_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        claim_idx = torch.tensor(claim_idx)\n",
    "        positive_idx = torch.tensor(positive_idx)\n",
    "        negative_idx = torch.tensor(negative_idx)\n",
    "        claim_attn_mask = torch.tensor(claim_attn_mask)\n",
    "        positive_attn_mask = torch.tensor(positive_attn_mask)\n",
    "        negative_attn_mask = torch.tensor(negative_attn_mask)\n",
    "\n",
    "        return claim_idx, claim_attn_mask, positive_idx, positive_attn_mask, negative_idx, negative_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 192\n",
    "train_dataset = ClaimsDataset(train_data, document_store_cleaned, block_size=block_size)\n",
    "val_dataset = ClaimsDataset(val_data, document_store_cleaned, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4122, 491)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the dataset has been prepared, let's train a DPR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint!\n",
      "Total number of parameters in transformer network: 132.72576 M\n",
      "RAM used: 3659.47 MB\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTBiEncoder().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# create a W&B run\\nrun = wandb.init(\\n    project=\"Automated Climate Fact Checker\", \\n    config={\\n        \"bi-encoder model\": \"MobileBERT\",\\n        \"learning_rate\": learning_rate, \\n        \"epochs\": 5,\\n        \"batch_size\": B, \\n        \"corpus\": \"SQuAD v1\"},)   \\n\\ndef log_metrics(metrics):\\n    wandb.log(metrics)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"Automated Climate Fact Checker\", \n",
    "    config={\n",
    "        \"bi-encoder model\": \"MobileBERT\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"SQuAD v1\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=8, save_every=2, val_every=100, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.645892322063446, 0.8126272912423625)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have a trained bi-encoder model, we can precompute the dense embeddings for all the documents in the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_passage(passage_text, block_size):\n",
    "        positive_encoding = tokenizer.encode_plus(passage_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        passage_idx = positive_encoding['input_ids']\n",
    "        if len(passage_idx) > (block_size-2):\n",
    "            start_pos = random.randint(0, max(0,len(passage_idx) - (block_size-2)))\n",
    "            passage_idx = passage_idx[start_pos:start_pos+block_size-2]\n",
    "\n",
    "        passage_idx = [tokenizer.cls_token_id] + passage_idx + [tokenizer.sep_token_id]    \n",
    "        passage_idx = passage_idx + [tokenizer.pad_token_id]*(block_size-len(passage_idx))\n",
    "\n",
    "        if len(passage_idx) > block_size:\n",
    "            raise Exception(f\"Sequence length {len(passage_idx)} is longer than max_length {block_size}!\")\n",
    "\n",
    "        passage_attn_mask  = [1 if idx != tokenizer.pad_token_id else 0 for idx in passage_idx]\n",
    "        passage_idx = torch.tensor(passage_idx)\n",
    "        passage_attn_mask = torch.tensor(passage_attn_mask)\n",
    "        return passage_idx, passage_attn_mask\n",
    "\n",
    "def tokenize_claim(claim_text):\n",
    "        claim_encoding = tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        claim_idx = [tokenizer.cls_token_id] + claim_idx + [tokenizer.sep_token_id]    \n",
    "        claim_idx = claim_idx + [tokenizer.pad_token_id]*(block_size-len(claim_idx))\n",
    "\n",
    "        if len(claim_idx) > block_size:\n",
    "            raise Exception(f\"Sequence length {len(claim_idx)} is longer than max_length {block_size}!\")\n",
    "\n",
    "        claim_attn_mask  = [1 if idx != tokenizer.pad_token_id else 0 for idx in claim_idx]\n",
    "        claim_idx = torch.tensor(claim_idx)\n",
    "        claim_attn_mask = torch.tensor(claim_attn_mask)\n",
    "        return claim_idx, claim_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_passage_embeddings(document_store, block_size):\n",
    "    # precompute the passage embeddings for all passages in the document store\n",
    "    document_store_list = list(document_store.items())  # Convert the document_store slice into a list\n",
    "    num_passages = len(document_store_list)\n",
    "    passage_embeddings = torch.zeros((num_passages, 768), device=DEVICE)  # Preallocate memory\n",
    "    for i in tqdm(range(0, len(document_store_list), 8)):\n",
    "        # tokenize the passages in this batch\n",
    "        passages_idx_batch = []\n",
    "        passages_attn_mask_batch = []\n",
    "        for _, passage_text in document_store_list[i:i+8]:\n",
    "            passage_idx, passage_attn_mask = tokenize_passage(passage_text, block_size)\n",
    "            passages_idx_batch.append(passage_idx)\n",
    "            passages_attn_mask_batch.append(passage_attn_mask)\n",
    "\n",
    "        passages_idx_batch = torch.stack(passages_idx_batch).to(DEVICE)\n",
    "        passages_attn_mask_batch = torch.stack(passages_attn_mask_batch).to(DEVICE)    \n",
    "        passage_embedding = model.encode_passages(passages_idx_batch, passages_attn_mask_batch)\n",
    "        passage_embeddings[i:i+8] = passage_embedding\n",
    "        del passage_embedding, passages_idx_batch, passages_attn_mask_batch  # Delete tensors to free up memory\n",
    "        torch.cuda.empty_cache()  # Clear GPU memory\n",
    "\n",
    "    return passage_embeddings\n",
    "\n",
    "passage_ids = list(document_store_cleaned.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evidence_passage_embeds = precompute_passage_embeddings(document_store_cleaned, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save precomputed embeddings to file\n",
    "#torch.save(evidence_passage_embeds, \"dpr_embeddings/evidence_passage_simple_dpr_embeds.pt\")\n",
    "\n",
    "# load embeddings from file\n",
    "evidence_passage_embeds = torch.load(\"dpr_embeddings/evidence_passage_simple_dpr_embeds.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we will do nearest neghbor search for finding the most similar passage embeddings to a given query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topk_evidence(claim_text, passage_ids, k=5):\n",
    "    # tokenize claim text\n",
    "    claim_idx, claim_attn_mask = tokenize_claim(claim_text)\n",
    "    claim_idx = claim_idx.unsqueeze(0).to(DEVICE)\n",
    "    claim_attn_mask = claim_attn_mask.unsqueeze(0).to(DEVICE)\n",
    "    # get BERT embedding of claim\n",
    "    claim_embedding = model.encode_queries(claim_idx, claim_attn_mask)\n",
    "    # find topk passages \n",
    "    scores = torch.mm(evidence_passage_embeds, claim_embedding.T)\n",
    "    topk_scores, topk_ids = torch.topk(scores.squeeze(1), k=k)\n",
    "    topk_scores = topk_scores.squeeze().tolist()\n",
    "    topk_ids = topk_ids.squeeze().tolist()\n",
    "    # get passage ids\n",
    "    topk_passage_ids = [passage_ids[i] for i in topk_ids]\n",
    "    return topk_passage_ids, topk_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test the model on a few claims\n",
    "claims_list_train = list(train_data.items()) \n",
    "claims_list_val = list(val_data.items()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: \"Twentieth century global warming did not start until  1910.\n",
      "\n",
      "Gold evidence:\n",
      "evidence-697238 --> The average global temperature on Earth has increased by about 0.8° Celsius (1.4° Fahrenheit) since 1880; Two-thirds of the warming has occurred since 1975, at a rate of roughly 0.15-0.20 °C per decade.\n",
      "\n",
      "Predicted evidence:\n",
      "evidence-697238 --> The average global temperature on Earth has increased by about 0.8° Celsius (1.4° Fahrenheit) since 1880; Two-thirds of the warming has occurred since 1975, at a rate of roughly 0.15-0.20 °C per decade. --> 30.43377113342285\n",
      "evidence-847689 --> An example of such an episode is the slower rate of surface temperature increase from 1998 to 2012, which was dubbed the global warming hiatus. --> 27.94475746154785\n",
      "evidence-1086865 --> Global warming refers to the warming caused by human technology since the 19th century or earlier. --> 27.944530487060547\n",
      "evidence-354405 --> century and first part of the Twentieth. --> 27.341690063476562\n",
      "evidence-256566 --> In addition, the end of the 20th century drying trend may be due to global warming. --> 27.32545280456543\n",
      "evidence-298391 --> The first started in September 1926, the second in 1932. --> 26.515981674194336\n",
      "evidence-262351 --> 1780, with the invention of the steam engine. --> 26.36502456665039\n",
      "evidence-1077763 --> The longest-running quasi-global record starts in 1850. --> 26.157451629638672\n",
      "evidence-898633 --> The 1850–1900 period is a reasonable pragmatic surrogate for preindustrial global mean temperature.\" --> 26.12220001220703\n",
      "evidence-1123049 --> The industrialization of Sweden began during the second half of the 19th century. --> 26.033231735229492\n",
      "\n",
      "Matching evidence passages: {'evidence-697238'}\n",
      "\n",
      "Precision: 0.1, Recall: 1.0, F1: 0.18181818181818182\n"
     ]
    }
   ],
   "source": [
    "idx = 34\n",
    "claim_text = claims_list_val[idx][1]['claim_text']\n",
    "gold_evidence_list = claims_list_val[idx][1]['evidences']\n",
    "\n",
    "print(f\"Claim: {claim_text}\")\n",
    "print(f\"\\nGold evidence:\")\n",
    "for evidence_id in gold_evidence_list:\n",
    "    print(f\"{evidence_id} --> {document_store_cleaned[evidence_id]}\")  \n",
    "\n",
    "# predict topk passages using model\n",
    "topk_passage_ids, topk_scores = find_topk_evidence(claim_text, passage_ids, k=10)\n",
    "\n",
    "print(f\"\\nPredicted evidence:\")\n",
    "for i, evidence_id in enumerate(topk_passage_ids):\n",
    "    print(f\"{evidence_id} --> {document_store_cleaned[evidence_id]} --> {topk_scores[i]}\")\n",
    "\n",
    "# evaluation (precision, recall, F1)\n",
    "intersection = set(topk_passage_ids).intersection(gold_evidence_list)\n",
    "print(f\"\\nMatching evidence passages: {intersection}\")\n",
    "precision = len(intersection) / len(topk_passage_ids)\n",
    "recall = len(intersection) / len(gold_evidence_list)\n",
    "f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "print(f\"\\nPrecision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(claims_list, k=5):\n",
    "    precision_total = 0\n",
    "    recall_total = 0\n",
    "    f1_total = 0\n",
    "    for idx in tqdm(range(len(claims_list))):\n",
    "        claim_text = claims_list[idx][1]['claim_text']\n",
    "        gold_evidence_list = claims_list[idx][1]['evidences']\n",
    "        # predict topk passages using model\n",
    "        topk_passage_ids, topk_scores = find_topk_evidence(claim_text, passage_ids, k)\n",
    "        # evaluation (precision, recall, F1)\n",
    "        intersection = set(topk_passage_ids).intersection(gold_evidence_list)\n",
    "        precision = len(intersection) / len(topk_passage_ids)\n",
    "        recall = len(intersection) / len(gold_evidence_list)\n",
    "        f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "        precision_total += precision\n",
    "        recall_total += recall\n",
    "        f1_total += f1\n",
    "\n",
    "    precision_avg = precision_total / len(claims_list)\n",
    "    recall_avg = recall_total / len(claims_list)\n",
    "    f1_avg = f1_total / len(claims_list)    \n",
    "\n",
    "    print(f\"\\nAvg Precision: {precision_avg}, Avg Recall: {recall_avg}, Avg F1: {f1_avg}\")\n",
    "    return precision_avg, recall_avg, f1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1228 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:25<00:00, 48.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.31270358306188895, Avg Recall: 0.30913409337676284, Avg F1: 0.28957848611757375\n",
      "\n",
      "Top-k = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:24<00:00, 50.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.26498371335504695, Avg Recall: 0.424334961997827, Avg F1: 0.3064571376867784\n",
      "\n",
      "Top-k = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:24<00:00, 49.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.21376221498371337, Avg Recall: 0.5377578718783924, Avg F1: 0.29095477450526447\n",
      "\n",
      "Top-k = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:24<00:00, 49.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.1875407166123767, Avg Recall: 0.5875950054288812, Avg F1: 0.2719671208270564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1875407166123767, 0.5875950054288812, 0.2719671208270564)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation on training data \n",
    "print(\"Top-k = 3\")\n",
    "eval(claims_list_train, k=3)\n",
    "print(\"\\nTop-k = 5\")\n",
    "eval(claims_list_train, k=5)\n",
    "print(\"\\nTop-k = 8\")\n",
    "eval(claims_list_train, k=8)\n",
    "print(\"\\nTop-k = 10\")\n",
    "eval(claims_list_train, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-k = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:40<00:00, 30.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.1194218241042345, Avg Recall: 0.7314332247556999, Avg F1: 0.19976560780483915\n",
      "\n",
      "Top-k = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:42<00:00, 29.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.05799674267101, Avg Recall: 0.8800488599348525, Avg F1: 0.10743198940722995\n",
      "\n",
      "Top-k = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [00:43<00:00, 27.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.03114006514657987, Avg Recall: 0.9379885993485332, Avg F1: 0.05987173164997901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03114006514657987, 0.9379885993485332, 0.05987173164997901)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTop-k = 20\")\n",
    "eval(claims_list_train, k=20)\n",
    "print(\"\\nTop-k = 50\")\n",
    "eval(claims_list_train, k=50)\n",
    "print(\"\\nTop-k = 100\")\n",
    "eval(claims_list_train, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-k = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:03<00:00, 50.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.15151515151515138, Avg Recall: 0.17424242424242417, Avg F1: 0.14901051329622755\n",
      "\n",
      "Top-k = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:03<00:00, 50.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.11558441558441546, Avg Recall: 0.2178571428571428, Avg F1: 0.14001236858379704\n",
      "\n",
      "Top-k = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:03<00:00, 50.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.09740259740259741, Avg Recall: 0.29112554112554123, Avg F1: 0.1374615283706192\n",
      "\n",
      "Top-k = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:03<00:00, 50.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.08571428571428556, Avg Recall: 0.31049783549783555, Avg F1: 0.1275646431490587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08571428571428556, 0.31049783549783555, 0.1275646431490587)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation on validation data\n",
    "print(\"Top-k = 3\")\n",
    "eval(claims_list_val, k=3)\n",
    "print(\"\\nTop-k = 5\")\n",
    "eval(claims_list_val, k=5)\n",
    "print(\"\\nTop-k = 8\")\n",
    "eval(claims_list_val, k=8)\n",
    "print(\"\\nTop-k = 10\")\n",
    "eval(claims_list_val, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-k = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.05454545454545449, Avg Recall: 0.38019480519480536, Avg F1: 0.09245175396784666\n",
      "\n",
      "Top-k = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 28.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.030129870129870118, Avg Recall: 0.5043290043290043, Avg F1: 0.05608721689339182\n",
      "\n",
      "Top-k = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 27.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.018246753246753226, Avg Recall: 0.6058441558441557, Avg F1: 0.035164573556819775\n",
      "\n",
      "Top-k = 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.008675324675324681, Avg Recall: 0.7167748917748918, Avg F1: 0.017091195591119386\n",
      "\n",
      "Top-k = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 28.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.00498701298701299, Avg Recall: 0.8148268398268396, Avg F1: 0.009898210976499868\n",
      "\n",
      "Top-k = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.0027272727272727292, Avg Recall: 0.8830086580086579, Avg F1: 0.0054335969628614995\n",
      "\n",
      "Top-k = 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:05<00:00, 27.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: 0.0014512987012987024, Avg Recall: 0.9284632034632033, Avg F1: 0.002896960299800299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0014512987012987024, 0.9284632034632033, 0.002896960299800299)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTop-k = 20\")\n",
    "eval(claims_list_val, k=20)\n",
    "print(\"\\nTop-k = 50\")\n",
    "eval(claims_list_val, k=50)\n",
    "print(\"\\nTop-k = 100\")\n",
    "eval(claims_list_val, k=100)\n",
    "print(\"\\nTop-k = 250\")\n",
    "eval(claims_list_val, k=250)\n",
    "print(\"\\nTop-k = 500\")\n",
    "eval(claims_list_val, k=500)\n",
    "print(\"\\nTop-k = 1000\")\n",
    "eval(claims_list_val, k=1000)\n",
    "print(\"\\nTop-k = 2000\")\n",
    "eval(claims_list_val, k=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the performance on the validation set is much better than the BM25 retreiver. In particular, DPR is able to achieve much higher average recall, although the precision is still pretty low. \n",
    "\n",
    "One issue that needs to be addressed is the fact that each claim can have multiple positive passages. There is also the possibility that different claims can share the same positive contexts. This can be problematic if a minibatch contains such instances and in that case some in-batch negatives for some instances would actually be their positives and that can cause conflicting training signals. To avoid this problem, we could more carefully choose the negatives for each minibatch.\n",
    "\n",
    "Another totally different approach would be to not use bi-encoders at all and instead use a single BERT model which takes in a claim-passage pair as a single sequence and performs binary classification that says whether that passage is relevant to the claim or not. This method, also called `cross-encoder` model, generally tends to outperform the bi-encoders so its worth looking into next. The main drawback of the cross-encoder is that it is less efficient, as passage embeddings cannot be precomputed and can inly be computed in the context of a given query. \n",
    "\n",
    "For greater efficiency, a cross-encoder model can be combines with a bi-encoder. We first use a bi-encoder model to retreive a large number of candidate relevant passages, say 100 passages. Then we use the cross-encoder to `re-rank` these retreived passages to filter out the most relevant ones (using some thresholding on the scores). This method is promising because note that for larger-k values, the recall for the bi-encoder is over 90%, so given a large enough-k, we can guarantee that the gold relevant documents will be among the top-k documents retreived by the bi-encoder. Then re-ranking from this much smaller pool is easier than if we were to perform cross-encoder ranking on the entire document store.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
