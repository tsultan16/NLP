{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous notebook, we tried out a simple BM25 retreiveal system and saw that it performed poorly. The average F1 score for top-5 retreival was only about 9%. Now, we will train a Dense Passage Retrieval (DPR) model and see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data and do filter out some of the bad passages like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the evidence passages\n",
    "with open(\"project-data/evidence.json\", \"r\") as train_file:\n",
    "    document_store = json.load(train_file)         \n",
    "print(f\"Number of evidence passages: {len(document_store)}\")\n",
    "\n",
    "# load the training data insttances\n",
    "with open(\"project-data/train-claims.json\", \"r\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "print(f\"Number of training instances: {len(train_data)}\")\n",
    "\n",
    "# load the validation data instances\n",
    "with open(\"project-data/dev-claims.json\", \"r\") as dev_file:\n",
    "    val_data = json.load(dev_file)    \n",
    "print(f\"Number of validation instances: {len(val_data)}\")\n",
    "\n",
    "# we remove duplicate values from the document_store dictionary (we arbitrarily keep the first one)\n",
    "seen = set()\n",
    "document_store_no_duplicates = {}\n",
    "for key, value in document_store.items():\n",
    "    if value not in seen:\n",
    "        document_store_no_duplicates[key] = value\n",
    "        seen.add(value)\n",
    "\n",
    "# remove all \"bad\" documents from the document store, except those that occur in claim gold evidence lists, we will define \"bad\" documents as ones that have less than 50 characters\n",
    "claim_evidence_list = [claim['evidences'] for claim in train_data.values()]\n",
    "claim_evidence_list = claim_evidence_list + [claim['evidences'] for claim in val_data.values()]\n",
    "claim_evidence_list = list(set([evidence for evidence_list in claim_evidence_list for evidence in evidence_list]))\n",
    "\n",
    "document_store_cleaned = {i: evidence_text for i, evidence_text in document_store_no_duplicates.items() if len(evidence_text) >= 30 or i in claim_evidence_list}\n",
    "print(f\"Number of evidence passages remaining after cleaning: {len(document_store_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique evidence passages in training data: 3121\n",
      "Number of unique evidence passages in validation data: 463\n"
     ]
    }
   ],
   "source": [
    "claim_evidence_list_train = set([ev for claim in train_data.values() for ev in claim['evidences']])\n",
    "claim_evidence_list_val = set([ev for claim in val_data.values() for ev in claim['evidences']])\n",
    "print(f\"Number of unique evidence passages in training data: {len(claim_evidence_list_train)}\")\n",
    "print(f\"Number of unique evidence passages in validation data: {len(claim_evidence_list_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the DPR retriever, we need to create pairs of $(\\text{claim}_i, \\text{evidence passage}_{i,1})$. However each claim can have multiple evidence passages, so we will create multiples pairs: $(\\text{claim}_i, \\text{evidence passage}_{i,1})$, $(\\text{claim}_i, \\text{evidence passage}_{i,2})$, ...\n",
    "\n",
    "Next, we prepare a minibatch of claims and corresponding passages. Then given a matrix $C$ of shape $(B,d)$ containing the batch of encoded claim vectors (where $d$ is the hidden dimensions of the encoded vectors) and a matrix $P$ of the same shape containing the batch of encoded passage vectors, we can compute the matrix $CP^T$ of shape $(B,B)$ whose $(i,j)th$ entry given us the dot product between the ith question and the jth passage. The elements along the diagonal of $CP^T$ are the scores for positive pairs and off-diagonal entries are for negative pairs. Then we can train a softmax classifier to classify the diagonal term in each row as the score for the positive class and the remaining B-1 terms as non-positive classes. This is the trick of `in-batch negatves` \n",
    "\n",
    "However, one issue is that out of the ~1M different passages, only ~3000 of them appear as positive evidences in (claim, evidence) pairs. Therefore, the in-batch negatives will also be restricted to these few passages. For good performance, we need to be able to select negatives from the all of the evidence passages from the document store. That's why in addition to passing a minibatch of claims $C$ and corresponding positive passages $P$, we will also pass in a batch of negatives $N$ which are selectled from the set of passgaes outside of the 3000 that appear as positives. Then we compute the matrix $CN^T$ which has shape $(B,B)$. All terms in the $ith$ row of this matrix are scores for the $ith$ claim with $B$ negatives. Then by horizontally concatenating: $[CP^T; CN^T]$, we get a matrix of shape $(B,2B)$ and we can just train a softmax classifier to classify the diagonal term in each row as the score for the positive class and the remaining 2B-1 terms as non-positive classes.\n",
    "\n",
    "`Hard-negative mining`: We could simply just create a batch of N negatives by randomly sampling the set of all negatives. However, a slightly better option would be to select the \"hard\" negatives. These are passages which are very similar to the positive ones. We could either use the highest scoring non-positive documents from a BM25 retreiver as hard negatives. Or we could first train our model with random negative selection. Then use the highest scoring non-positives from our trained model as negatives and do some finetuning. \n",
    "\n",
    "Note: Because we need to use fixed size passages for BERT, if a passage exceeds the block size, we will take a random window of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's separate out all ~3000 positive evidence passages from the document store and define the remaining as negatives.\n",
    "all_passages_ids = list(document_store_cleaned.keys())\n",
    "positives_ids_train = claim_evidence_list_train\n",
    "negatives_ids_train = list(set(all_passages_ids) - set(positives_ids_train))\n",
    "                         \n",
    "# create claim-positive pairs\n",
    "claim_positive_pairs_train = []\n",
    "for claim_id in train_data.keys():\n",
    "    for evidence_id in train_data[claim_id]['evidences']:\n",
    "        claim_positive_pairs_train.append((claim_id, evidence_id))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a pytroch dataset\n",
    "class ClaimsDataset(Dataset):\n",
    "    def __init__(self, claims_data, document_store, block_size=128):\n",
    "        self.claims_data = claims_data\n",
    "        self.document_store = document_store\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.claim_positive_pairs, self.negatives_ids = self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_evidence_list = set([ev for claim in self.claims_data.values() for ev in claim['evidences']])\n",
    "        all_passages_ids = list(self.document_store.keys())\n",
    "        positives_ids = claim_evidence_list\n",
    "        negatives_ids = list(set(all_passages_ids) - set(positives_ids))\n",
    "        claim_positive_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_positive_pairs.append((claim_id, evidence_id))    \n",
    "        return claim_positive_pairs, negatives_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_positive_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim id and positive evidence id\n",
    "        claim_id, positive_id = self.claim_positive_pairs[idx]\n",
    "        # randomly sample a negative evidence id\n",
    "        negative_id = random.choice(self.negatives_ids)\n",
    "        # get the claim, positive and negative text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        positive_text = self.document_store[positive_id]\n",
    "        negative_text = self.document_store[negative_id]\n",
    "\n",
    "        # tokenize the claim, positive and negative text  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        \n",
    "        positive_encoding = self.tokenizer.encode_plus(positive_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        positive_idx = positive_encoding['input_ids']\n",
    "        \n",
    "        negative_encoding = self.tokenizer.encode_plus(negative_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        negative_idx = negative_encoding['input_ids']\n",
    "\n",
    "        # select a window from the positive passage if it is longer than block size\n",
    "        if len(positive_idx) > (self.block_size-2):\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(positive_idx) - (self.block_size-2)))\n",
    "            # select the window\n",
    "            positive_idx = positive_idx[start_pos:start_pos+self.block_size-2]\n",
    "\n",
    "        # select a window from the negative passage if it is longer than block size\n",
    "        if len(negative_idx) > (self.block_size-2):\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(negative_idx) - (self.block_size-2)))\n",
    "            # select the window\n",
    "            negative_idx = negative_idx[start_pos:start_pos+self.block_size-2]    \n",
    "\n",
    "        # add special tokens and padding\n",
    "        claim_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id]\n",
    "        claim_idx = claim_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(claim_idx))\n",
    "        positive_idx = [self.tokenizer.cls_token_id] + positive_idx + [self.tokenizer.sep_token_id]\n",
    "        positive_idx = positive_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(positive_idx))\n",
    "        negative_idx = [self.tokenizer.cls_token_id] + negative_idx + [self.tokenizer.sep_token_id]\n",
    "        negative_idx = negative_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(negative_idx))\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(claim_idx) > self.block_size or len(positive_idx) > self.block_size or len(negative_idx) > self.block_size:\n",
    "            raise Exception(f\"Claim sequence length {len(claim_idx)} or positive sequence length {len(positive_idx)} or negative sequence length: {len(negative_idx)} is longer than max_length {self.block_size}!\")\n",
    "        \n",
    "        # create attention masks\n",
    "        claim_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in claim_idx]\n",
    "        positive_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in positive_idx]\n",
    "        negative_attn_mask  = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in negative_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        claim_idx = torch.tensor(claim_idx)\n",
    "        positive_idx = torch.tensor(positive_idx)\n",
    "        negative_idx = torch.tensor(negative_idx)\n",
    "        claim_attn_mask = torch.tensor(claim_attn_mask)\n",
    "        positive_attn_mask = torch.tensor(positive_attn_mask)\n",
    "        negative_attn_mask = torch.tensor(negative_attn_mask)\n",
    "\n",
    "        return claim_idx, claim_attn_mask, positive_idx, positive_attn_mask, negative_idx, negative_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 192\n",
    "train_dataset = ClaimsDataset(train_data, document_store_cleaned, block_size=block_size)\n",
    "val_dataset = ClaimsDataset(val_data, document_store_cleaned, block_size=block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4122, 491)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the dataset has been prepared, let's train a DPR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 132.72576 M\n",
      "RAM used: 3025.48 MB\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTBiEncoder().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Information_Retrieval_Extraction/COMP90042_Project_2023/wandb/run-20240121_132215-aebnpeej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/aebnpeej' target=\"_blank\">olive-wildflower-3</a></strong> to <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/aebnpeej' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/aebnpeej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a W&B run\n",
    "run = wandb.init(\n",
    "    project=\"Automated Climate Fact Checker\", \n",
    "    config={\n",
    "        \"bi-encoder model\": \"MobileBERT\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"SQuAD v1\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/258 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 1.808, Train Accuracy:  0.502, Val Loss:  1.342, Val Accuracy:  0.658:  86%|████████▋ | 223/258 [01:41<00:13,  2.53it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=2, save_every=2, val_every=100, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
