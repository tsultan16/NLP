{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Encoder Passage Retrieval Model\n",
    "\n",
    "We will now explore an alternative mnodel for document retreival, called a `cross-encoder`. Unlike the bi-encoder, we will use a single BERT model and feed it input sequence which is a concatenation of a claim-passage pair. Then using the output embedding of the [CLS] token, we perform a `binary classification` of whether or not, the passage is relevant to this claim or not. We can set up training instances of both positive/relevant and negative/non-relevant pairs and train using binary cross-entropy loss. Then the sigmoid of the output logit can be interpreted as a relevancy score between $[0,1]$. \n",
    "\n",
    "Since each claim can have multiple relevant evidence passages, we can create multiple positive pairs. Then to have a balanced distribution of the two classes, we would also create the same number of negative pairs. However, we need to figure out a way to select the negative passages. The simplest way is to just `randomly sample` a passage from the document store which is also not in the list of positive passages. Howerever, these random samples may be too easy for our model to detect, and so ideally we would want to use some form of `hard-negative mining` to select good negative passages. \n",
    "\n",
    "So first, lets set up our dataset and implement hard-negative mining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from utils import *\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "document_store, train_data, val_data = load_data(clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-negative Mining: We will use our pre-trained dpr model to mine for hard negatives. We will retrieve the top-k passages for each training query and then sample hard negatives from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDEVICE = \"cuda\"\\ndpr_model = BERTBiEncoder().to(DEVICE)\\ndpr_model = load_dpr_model_checkpoint(dpr_model)\\n\\n# load dpr passage embeddings\\nevidence_passage_embeds, passage_ids = load_dpr_passage_embeddings()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained dpr model\n",
    "\"\"\"\n",
    "DEVICE = \"cuda\"\n",
    "dpr_model = BERTBiEncoder().to(DEVICE)\n",
    "dpr_model = load_dpr_model_checkpoint(dpr_model)\n",
    "\n",
    "# load dpr passage embeddings\n",
    "evidence_passage_embeds, passage_ids = load_dpr_passage_embeddings()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer = RobertaTokenizer.from_pretrained(\\'roberta-base\\')\\nblock_size = 196\\n\\n# now get the hard negatives for each question\\ndef get_hard_negatives(dpr_model, tokenizer, data, passage_ids, block_size, k=100):\\n    \\n    Get the k hard negatives for each claim in the dataset\\n    \\n    claims_list = list(data.items()) \\n    hard_negatives = {}\\n    for claim_id, claim in tqdm(claims_list):\\n        claim_text = claim[\"claim_text\"]\\n        gold_evidence_list = claim[\"evidences\"]\\n        topk_passage_ids, topk_scores = find_topk_evidence_dpr(dpr_model, tokenizer, claim_text, evidence_passage_embeds, passage_ids, block_size, k=k)\\n        # remove the gold evidence from the topk passages\\n        topk_passage_ids = [p_id for p_id in topk_passage_ids if p_id not in gold_evidence_list]\\n        hard_negatives[claim_id] = topk_passage_ids\\n\\n    return hard_negatives\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "block_size = 196\n",
    "\n",
    "# now get the hard negatives for each question\n",
    "def get_hard_negatives(dpr_model, tokenizer, data, passage_ids, block_size, k=100):\n",
    "    \n",
    "    Get the k hard negatives for each claim in the dataset\n",
    "    \n",
    "    claims_list = list(data.items()) \n",
    "    hard_negatives = {}\n",
    "    for claim_id, claim in tqdm(claims_list):\n",
    "        claim_text = claim[\"claim_text\"]\n",
    "        gold_evidence_list = claim[\"evidences\"]\n",
    "        topk_passage_ids, topk_scores = find_topk_evidence_dpr(dpr_model, tokenizer, claim_text, evidence_passage_embeds, passage_ids, block_size, k=k)\n",
    "        # remove the gold evidence from the topk passages\n",
    "        topk_passage_ids = [p_id for p_id in topk_passage_ids if p_id not in gold_evidence_list]\n",
    "        hard_negatives[claim_id] = topk_passage_ids\n",
    "\n",
    "    return hard_negatives\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate ~100 hard negatives for each claim and save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_hard_negatives = get_hard_negatives(dpr_model, tokenizer, train_data, passage_ids, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_hard_negatives = get_hard_negatives(dpr_model, tokenizer, val_data, passage_ids, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save the hard negatives to the pickle file\n",
    "with open(\"dpr_embeddings/train_hard_negatives.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_hard_negatives, f)\n",
    "\n",
    "# Save the hard negatives to the pickle file\n",
    "with open(\"dpr_embeddings/val_hard_negatives.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_hard_negatives, f)\n",
    "\"\"\"\n",
    "\n",
    "# load from pickle file\n",
    "with open(\"dpr_embeddings/train_hard_negatives.pkl\", \"rb\") as f:\n",
    "    train_hard_negatives = pickle.load(f)\n",
    "\n",
    "with open(\"dpr_embeddings/val_hard_negatives.pkl\", \"rb\") as f:\n",
    "    val_hard_negatives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: [Jonathan Overpeck:] ‘No doubt about it anymore — humans, mainly by burning fossil fuels, are cooking the planet,’ Overpeck said.\n",
      "\n",
      "Gold evidence: \n",
      "\tThe main sources of greenhouse gases due to human activity are: burning of fossil fuels and deforestation leading to higher carbon dioxide concentrations in the air.\n",
      "\tAt present, the primary source of CO 2 emissions is the burning of coal, natural gas, and petroleum for electricity and heat.\n",
      "\tThe vast majority of anthropogenic carbon dioxide emissions come from combustion of fossil fuels, principally coal, oil, and natural gas, with additional contributions coming from deforestation, changes in land use, soil erosion and agriculture (including livestock).\n",
      "\tMany of the actions taken by humans that contribute to a heated environment stem from the burning of fossil fuel from a variety of sources, such as: electricity, cars, planes, space heating, manufacturing, or the destruction of forests.\n",
      "\n",
      "Top-5 Hard negatives: \n",
      "\tCoal is the most abundant fossil fuel on the planet, and widely used as the source of energy in thermal power stations and is a relatively cheap fuel.\n",
      "\tAs a fossil fuel burned for heat, coal supplies about a quarter of the world's primary energy and two-fifths of its electricity.\n",
      "\tWood, coal and oil are used as fuel for production of energy and heating.\n",
      "\tThree billion people in developing countries across the globe rely on biomass in the form of wood, charcoal, dung, and crop residue, as their domestic cooking fuel.\n",
      "\tFor all these processes, charcoal was required as fuel.\n",
      "\tIn the old economy, energy was produced by burning something — oil, coal, or natural gas — leading to the carbon emissions that have come to define our economy.\n",
      "\tFossil fuel (\"primary non-renewable fossil\") sources burn coal or hydrocarbon fuels, which are the remains of the decomposition of plants and animals.\n",
      "\tMan-made sources of carbon dioxide include the burning of fossil fuels for heating, power generation and transport, as well as some industrial processes such as cement making.\n",
      "\tUntil the beginning of the nineteenth century biomass was the predominant fuel, today it has only a small share of the overall energy supply.\n",
      "\tCurrently, the main resources for electricity in the Netherlands are fossil fuels, such as natural gas and coal.\n"
     ]
    }
   ],
   "source": [
    "# show some examples of the hard negatives\n",
    "claim_id = random.choice(list(train_hard_negatives.keys()))\n",
    "print(\"Claim:\", train_data[claim_id][\"claim_text\"])\n",
    "print(f\"\\nGold evidence: \")\n",
    "for evidence in train_data[claim_id][\"evidences\"]:\n",
    "    print(f\"\\t{document_store[evidence]}\")\n",
    "print(f\"\\nTop-5 Hard negatives: \")\n",
    "for passage_id in train_hard_negatives[claim_id][:10]:\n",
    "    print(f\"\\t{document_store[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a pytroch dataset for cross-encoder training. For every positive pair, we've included the option of multiple hard negative pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a pytroch dataset\n",
    "\n",
    "# set tokenizer parallelism to False\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "class CrossEncoderDataset(Dataset):\n",
    "    def __init__(self, claims_data, hard_negatives, document_store, block_size=192):\n",
    "        self.claims_data = claims_data\n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.document_store = document_store\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.positive_label = 1\n",
    "        self.negative_label = 0\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def create_pairs(self, num_negatives=1):\n",
    "        claim_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_pairs.append((claim_id, evidence_id, self.positive_label))  \n",
    "                # for each positive evidence, sample two negative evidences from hard negatives list\n",
    "                negative_ids = random.sample(self.hard_negatives[claim_id], num_negatives)\n",
    "                for negative_id in negative_ids:\n",
    "                    claim_pairs.append((claim_id, negative_id, self.negative_label))\n",
    "        # shuffle the pairs \n",
    "        random.shuffle(claim_pairs)                \n",
    "        return claim_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim id and evidence id\n",
    "        claim_id, evidence_id, target_label = self.claim_pairs[idx]\n",
    "        # get the claim and evidence text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        evidence_text = self.document_store[evidence_id]\n",
    "        # tokenize the claim and evidence text  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        evidence_encoding = self.tokenizer.encode_plus(evidence_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        evidence_idx = evidence_encoding['input_ids']\n",
    "\n",
    "        # select a random window from the evidence passage if it won't fit in block size\n",
    "        max_evidence_size = self.block_size - len(claim_idx) - 3\n",
    "        if len(evidence_idx) > max_evidence_size:\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(evidence_idx)-max_evidence_size))\n",
    "            # select the window\n",
    "            evidence_idx = evidence_idx[start_pos:start_pos+max_evidence_size]\n",
    " \n",
    "        # concatenate the claim and evidence, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id] + evidence_idx + [self.tokenizer.sep_token_id]\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "\n",
    "        # create segment ids\n",
    "        claim_len = len(claim_idx) + 2\n",
    "        evidence_len = len(evidence_idx) + 1\n",
    "        token_type_idx = [0] * claim_len + [1] * evidence_len + [0] * (self.block_size - claim_len - evidence_len)\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.block_size}!\")\n",
    "    \n",
    "        # create attention masks\n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        target_label = torch.tensor(target_label)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask)\n",
    "        token_type_idx = torch.tensor(token_type_idx)  # don't need this for roberta\n",
    "        return input_idx, input_attn_mask, token_type_idx, target_label\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.claim_pairs = self.create_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 192\n",
    "train_dataset = CrossEncoderDataset(train_data, train_hard_negatives, document_store, block_size=block_size)\n",
    "val_dataset = CrossEncoderDataset(val_data, val_hard_negatives, document_store, block_size=block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the cross-encoder model. This is just a single BERT model and sentence level binary classification using the [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCrossEncoder(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head\n",
    "        self.classifier_head = torch.nn.Linear(768, 2)\n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, token_type_idx, targets=None):\n",
    "        # compute BERT encodings, extract the pooler output (which is just the [CLS] embedding fed through a feedforward network or just the [CLS] embedding), apply dropout        \n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask, token_type_ids=token_type_idx)\n",
    "        pooled_output = self.dropout(bert_output.last_hidden_state[:,0]) # shape: (batch_size, hidden_size)\n",
    "        # compute output logits\n",
    "        logits = self.classifier_head(pooled_output)\n",
    "        # compute cross-entropy loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, accumulation_steps=1, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            # move batch to device\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # apply gradient step \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                # optimizer step\n",
    "                optimizer.step()\n",
    "                # reset gradients\n",
    "                optimizer.zero_grad()\n",
    "   \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "\n",
    "            if val_every is not None:\n",
    "                if i%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                    pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\":loss.item(), \"Moving Avg Loss\":avg_loss, \"Train Accuracy\":train_acc, \"Val Loss\": val_loss, \"Val Accuracy\":val_acc}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        # run optimizer step for remainder batches\n",
    "        if len(train_dataloader) % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'cross_enc_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer=None, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('cross_enc_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 2954.11 MB\n"
     ]
    }
   ],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=False, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=False, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTCrossEncoder(dropout_rate=0.1).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tanzid/Code/NLP/Information_Retrieval_Extraction/COMP90042_Project_2023/wandb/run-20240122_125935-88lssbgt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/88lssbgt' target=\"_blank\">warm-fog-12</a></strong> to <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/88lssbgt' target=\"_blank\">https://wandb.ai/tanzids/Automated%20Climate%20Fact%20Checker/runs/88lssbgt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project=\"Automated Climate Fact Checker\", \n",
    "    config={\n",
    "        \"bi-encoder model\": \"RoBERTa\",\n",
    "        \"learning_rate\": learning_rate, \n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": B, \n",
    "        \"corpus\": \"COMP90042 2023 project\"},)   \n",
    "\n",
    "def log_metrics(metrics):\n",
    "    wandb.log(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 0.532, Train Accuracy:  0.698, Val Loss:  0.527, Val Accuracy:  0.742: 100%|██████████| 516/516 [02:52<00:00,  3.00it/s]\n",
      "Epoch 2, EMA Train Loss: 0.481, Train Accuracy:  0.777, Val Loss:  0.518, Val Accuracy:  0.749:  78%|███████▊  | 400/516 [02:11<00:31,  3.72it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=5, save_every=None, val_every=100, log_metrics=log_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
