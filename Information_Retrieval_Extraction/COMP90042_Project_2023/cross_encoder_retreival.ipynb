{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Encoder Passage Retrieval Model\n",
    "\n",
    "We will now explore an alternative mnodel for document retreival, called a `cross-encoder`. Unlike the bi-encoder, we will use a single BERT model and feed it input sequence which is a concatenation of a claim-passage pair. Then using the output embedding of the [CLS] token, we perform a `binary classification` of whether or not, the passage is relevant to this claim or not. We can set up training instances of both positive/relevant and negative/non-relevant pairs and train using binary cross-entropy loss. Then the sigmoid of the output logit can be interpreted as a relevancy score between $[0,1]$. \n",
    "\n",
    "Since each claim can have multiple relevant evidence passages, we can create multiple positive pairs. Then to have a balanced distribution of the two classes, we would also create the same number of negative pairs. However, we need to figure out a way to select the negative passages. The simplest way is to just `randomly sample` a passage from the document store which is also not in the list of positive passages. Howerever, these random samples may be too easy for our model to detect, and so ideally we would want to use some form of `hard-negative mining` to select good negative passages. \n",
    "\n",
    "So first, lets set up our dataset and implement hard-negative mining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from utils import *\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1190647\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "document_store, train_data, val_data = load_data(clean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard-negative Mining: We will use our pre-trained dpr model to mine for hard negatives. We will retrieve the top-k passages for each training query and then sample hard negatives from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDEVICE = \"cuda\"\\ndpr_model = BERTBiEncoder().to(DEVICE)\\ndpr_model = load_dpr_model_checkpoint(dpr_model)\\n\\n# load dpr passage embeddings\\nevidence_passage_embeds, passage_ids = load_dpr_passage_embeddings()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pretrained dpr model\n",
    "\"\"\"\n",
    "DEVICE = \"cuda\"\n",
    "dpr_model = BERTBiEncoder().to(DEVICE)\n",
    "dpr_model = load_dpr_model_checkpoint(dpr_model)\n",
    "\n",
    "# load dpr passage embeddings\n",
    "evidence_passage_embeds, passage_ids = load_dpr_passage_embeddings()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "block_size = 196\n",
    "\n",
    "# now get the hard negatives for each question\n",
    "def get_hard_negatives(dpr_model, tokenizer, data, passage_ids, block_size, k=100):\n",
    "    \"\"\"\n",
    "    Get the k hard negatives for each claim in the dataset\n",
    "    \"\"\"\n",
    "    claims_list = list(data.items()) \n",
    "    hard_negatives = {}\n",
    "    for claim_id, claim in tqdm(claims_list):\n",
    "        claim_text = claim[\"claim_text\"]\n",
    "        gold_evidence_list = claim[\"evidences\"]\n",
    "        topk_passage_ids, topk_scores = find_topk_evidence_dpr(dpr_model, tokenizer, claim_text, evidence_passage_embeds, passage_ids, block_size, k=k)\n",
    "        # remove the gold evidence from the topk passages\n",
    "        topk_passage_ids = [p_id for p_id in topk_passage_ids if p_id not in gold_evidence_list]\n",
    "        hard_negatives[claim_id] = topk_passage_ids\n",
    "\n",
    "    return hard_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate ~100 hard negatives for each claim and save them to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_hard_negatives = get_hard_negatives(dpr_model, tokenizer, train_data, passage_ids, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_hard_negatives = get_hard_negatives(dpr_model, tokenizer, val_data, passage_ids, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Save the hard negatives to the pickle file\n",
    "with open(\"dpr_embeddings/train_hard_negatives.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_hard_negatives, f)\n",
    "\n",
    "# Save the hard negatives to the pickle file\n",
    "with open(\"dpr_embeddings/val_hard_negatives.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_hard_negatives, f)\n",
    "\"\"\"\n",
    "\n",
    "# load from pickle file\n",
    "with open(\"dpr_embeddings/train_hard_negatives.pkl\", \"rb\") as f:\n",
    "    train_hard_negatives = pickle.load(f)\n",
    "\n",
    "with open(\"dpr_embeddings/val_hard_negatives.pkl\", \"rb\") as f:\n",
    "    val_hard_negatives = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: Empirical measurements of the Earth's heat content show the planet is still accumulating heat and global warming is still happening.\n",
      "\n",
      "Gold evidence: \n",
      "\t\"Evidence is now 'unequivocal' that humans are causing global warming – UN report\".\n",
      "\tThis is predicted to produce changes such as the melting of glaciers and ice sheets, more extreme temperature ranges, significant changes in weather and a global rise in average sea levels.\n",
      "\tSince the pre-industrial period, global average land temperatures have increased almost twice as fast as global average temperatures.\n",
      "\tThis is much colder than the conditions that actually exist at the Earth's surface (the global mean surface temperature is about 14 °C).\n",
      "\tThe global average and combined land and ocean surface temperature, show a warming of 0.85 [0.65 to 1.06] °C, in the period 1880 to 2012, based on multiple independently produced datasets.\n",
      "\n",
      "Top-5 Hard negatives: \n",
      "\tInfrared Thermography is the science of measuring and mapping surface temperatures.\n",
      "\tGlobal warming is the long-term rise in the average temperature of the Earth's climate system.\n",
      "\tGlobal warming refers to the long-term rise in the average temperature of the Earth's climate system.\n",
      "\tClimate data: Air temperature, Soil temperature, Snow depth, Precipitation, Cloud cover, Wind, global radiation etc.\n",
      "\t\"Warming of the climate system is unequivocal, as is now evident from observations of increases in global average air and ocean temperatures, widespread melting of snow and ice and rising global average sea level.\"\n",
      "\tThe Earth's heat content is about .\n",
      "\tIt is a major aspect of current climate change, and has been demonstrated by direct temperature measurements and by measurements of various effects of the warming.\n",
      "\tGlobal warming refers to global averages, with the amount of warming varying by region.\n",
      "\tsunlight, wind, biomass, rain, tides, waves and geothermal heat.\n",
      "\tObservations of the changes in \"heat content\" of the ocean are important for providing realistic estimates of how the ocean is changing with global warming.\n"
     ]
    }
   ],
   "source": [
    "# show some examples of the hard negatives\n",
    "claim_id = random.choice(list(train_hard_negatives.keys()))\n",
    "print(\"Claim:\", train_data[claim_id][\"claim_text\"])\n",
    "print(f\"\\nGold evidence: \")\n",
    "for evidence in train_data[claim_id][\"evidences\"]:\n",
    "    print(f\"\\t{document_store[evidence]}\")\n",
    "print(f\"\\nTop-5 Hard negatives: \")\n",
    "for passage_id in train_hard_negatives[claim_id][:10]:\n",
    "    print(f\"\\t{document_store[passage_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a pytroch dataset for cross-encoder training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a pytroch dataset\n",
    "class CrossEncoderDataset(Dataset):\n",
    "    def __init__(self, claims_data, hard_negatives, document_store, block_size=192):\n",
    "        self.claims_data = claims_data\n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.document_store = document_store\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.positive_label = 1\n",
    "        self.negative_label = 0\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_pairs.append((claim_id, evidence_id, self.positive_label))  \n",
    "                # for each positive evidence, sample a negative evidence from hard negatives list\n",
    "                negative_id = random.choice(self.hard_negatives[claim_id])\n",
    "                claim_pairs.append((claim_id, negative_id, self.negative_label)) \n",
    "        # shuffle the pairs \n",
    "        random.shuffle(claim_pairs)                \n",
    "        return claim_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim id and evidence id\n",
    "        claim_id, evidence_id, target_label = self.claim_pairs[idx]\n",
    "        # get the claim and evidence text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        evidence_text = self.document_store[evidence_id]\n",
    "        # tokenize the claim and evidence text  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        evidence_encoding = self.tokenizer.encode_plus(evidence_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        evidence_idx = evidence_encoding['input_ids']\n",
    "\n",
    "        # select a random window from the evidence passage if it won't fit in block size\n",
    "        max_evidence_size = self.block_size - len(claim_idx) - 3\n",
    "        if len(evidence_idx) > max_evidence_size:\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(evidence_idx)-max_evidence_size))\n",
    "            # select the window\n",
    "            evidence_idx = evidence_idx[start_pos:start_pos+max_evidence_size]\n",
    " \n",
    "        # concatenate the claim and evidence, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id] + evidence_idx + [self.tokenizer.sep_token_id]\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "\n",
    "        # create segment ids\n",
    "        claim_len = len(claim_idx) + 2\n",
    "        evidence_len = len(evidence_idx) + 1\n",
    "        token_type_idx = [0] * claim_len + [1] * evidence_len + [0] * (self.block_size - claim_len - evidence_len)\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.block_size}!\")\n",
    "    \n",
    "        # create attention masks\n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        target_label = torch.tensor(target_label)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask)\n",
    "        token_type_idx = torch.tensor(token_type_idx)\n",
    "\n",
    "        return input_idx, input_attn_mask, token_type_idx, target_label\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.claim_pairs = self.create_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 192\n",
    "train_dataset = CrossEncoderDataset(train_data, train_hard_negatives, document_store, block_size=block_size)\n",
    "val_dataset = CrossEncoderDataset(val_data, val_hard_negatives, document_store, block_size=block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the cross-encoder model. This is just a single BERT model and sentence level binary classification using the [CLS] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCrossEncoder(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head\n",
    "        self.classifier_head = torch.nn.Linear(768, 2)\n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, token_type_idx, targets=None):\n",
    "        # compute BERT encodings, extract the `[CLS]` encoding (first element of the sequence), apply dropout        \n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask, token_type_ids=token_type_idx)\n",
    "        cls_enc = self.dropout(bert_output.last_hidden_state[:, 0]) # shape: (batch_size, hidden_size)\n",
    "        # compute output logits\n",
    "        logits = self.classifier_head(cls_enc)\n",
    "        # compute cross-entropy loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, accumulation_steps=1, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            # move batch to device\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # apply gradient step \n",
    "            if (i+1) % accumulation_steps == 0:\n",
    "                # optimizer step\n",
    "                optimizer.step()\n",
    "                # reset gradients\n",
    "                optimizer.zero_grad()\n",
    "   \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "\n",
    "            if val_every is not None:\n",
    "                if i%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                    pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        # run optimizer step for remainder batches\n",
    "        if len(train_dataloader) % accumulation_steps != 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_dpr_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = batch\n",
    "            input_idx, input_attn_mask, token_type_idx, targets = input_idx.to(device), input_attn_mask.to(device), token_type_idx.to(device), targets.to(device)\n",
    "            logits, loss = model(input_idx, input_attn_mask, token_type_idx, targets)\n",
    "            B, _ = input_idx.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += (y_pred.eq(targets.view(-1))).sum().item()      \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "def save_dpr_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'dpr_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_dpr_model_checkpoint(model, optimizer=None, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('dpr_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
