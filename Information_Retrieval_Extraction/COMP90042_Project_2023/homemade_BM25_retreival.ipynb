{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMP90042 2023 Project: Automated Fact Checking For Climate Science Claims\n",
    "\n",
    "The goal of this project is to design and implement a system that takes a given `claim` (which is a single sentence) and then retrieves (one or more) `evidence passages` (each passage is also a single sentence) from a document store, then using these evidence passages classifies the claim as one of these four labels: `[SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]`.  \n",
    "\n",
    "Our training data consists of a document store which is a set of $D$ evidence passages `{evidence-1, evidence-2, ..., evidence-D}` and training and validation data instances where each instance is a tuple of the form `(claim_text, claim_label, evidence_list)`, where evidence list is a subset of passages from the document store.\n",
    "\n",
    "In this notebook, we explore the use of `BM25` for passage retreival and evaluate it's performance on the project dataset. Evaluation will involve comparing retreived evidence passages for each data instance with the ground truth evidence list  and computing precision, recall and F1 score. Then we will use the `average F1 score` across all data instances as the final evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "import math, random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is my implementation of a simple BM25 retreival system. Will later replace this with the PyLucene implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IR_System():\n",
    "    def __init__(self, k = 1.25, b = 0.75):\n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+') \n",
    "\n",
    "    def train(self, documents):\n",
    "        self.documents = documents\n",
    "        self.TFIDF, self.inverted_index, self.doc_tfidf_norms = self.create_inverted_index()\n",
    "        \n",
    "    def tokenize(self, sent):\n",
    "        # Replace accented letters with regular letters\n",
    "        sent = unidecode(sent)\n",
    "        # tokenize into words, remove punctuation\n",
    "        return self.tokenizer.tokenize(sent.lower())\n",
    "\n",
    "    def create_inverted_index(self):\n",
    "        N = len(self.documents)\n",
    "        TFIDF = defaultdict(float)\n",
    "        inverted_index = defaultdict(list)\n",
    "\n",
    "        # compute term frequency and document frequencies\n",
    "        TF, term_docs = self.compute_TF_weighted()\n",
    "\n",
    "        # create inverted index\n",
    "        print(f\"Computing TFIDF and creating inverted index...\")\n",
    "        for w, docs in tqdm(term_docs.items(), total=len(term_docs)):\n",
    "            for d in sorted(list(docs)):\n",
    "                tfidf = TF[(w,d)] * math.log10(N/len(docs))\n",
    "                inverted_index[w].append(d)\n",
    "                TFIDF[(w,d)] = tfidf\n",
    "\n",
    "        # compute document TFIDF vector norms\n",
    "        print(f\"Computing TFIDF vector norms...\")\n",
    "        doc_tfidf_norms = [0] * N\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = self.tokenize(doc)\n",
    "            for w in words:\n",
    "                doc_tfidf_norms[d] = doc_tfidf_norms[d] +  TFIDF[(w,d)]**2\n",
    "            doc_tfidf_norms[d] = math.sqrt(doc_tfidf_norms[d])\n",
    "\n",
    "        return TFIDF, inverted_index, doc_tfidf_norms  \n",
    "          \n",
    "    # weighted TF for BM25\n",
    "    def compute_TF_weighted(self):\n",
    "        TF = defaultdict(int)\n",
    "        term_docs = defaultdict(set)\n",
    "        doc_length = defaultdict(float)\n",
    "        Dtotal = 0\n",
    "        print(f\"Computing TFIDF...\")\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = self.tokenize(doc)\n",
    "            for w in words:\n",
    "                TF[(w, d)] += 1\n",
    "                term_docs[w].add(d)\n",
    "            doc_length[d] = len(words)\n",
    "            Dtotal += len(words)\n",
    "        Davg = Dtotal / len(self.documents)\n",
    "\n",
    "        # compute BM25 weighted term frequencies\n",
    "        TF_weighted = defaultdict(float)\n",
    "        for (w,d), tf in TF.items():\n",
    "            TF_weighted[(w,d)] = (tf * (self.k + 1)) / (tf + self.k * (1 - self.b + self.b * (doc_length[d]/Davg)))\n",
    "        return TF_weighted, term_docs\n",
    "    \n",
    "\n",
    "    def retrieve_docs(self, query, topk=1):\n",
    "        query_words = self.tokenizer.tokenize(query.lower())\n",
    "        #print(f\"query words: {query_words}\")\n",
    "        # get all documents which contain words from query\n",
    "        docs = []\n",
    "        for w in query_words:\n",
    "            docs.extend([d for d in self.inverted_index[w]])\n",
    "        # remove duplicates\n",
    "        docs = list(set(docs))\n",
    "        #print(f\"docs: {docs}\")    \n",
    "        # score all these documents\n",
    "        scores = np.zeros(len(docs))\n",
    "        for i in range(len(docs)):\n",
    "            d = docs[i]\n",
    "            for w in query_words:\n",
    "                scores[i] += self.TFIDF[(w,d)]\n",
    "            scores[i] = scores[i] / self.doc_tfidf_norms[d]        \n",
    "        #print(f\"scores: {scores}\")  \n",
    "\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "        best_indices = sorted_indices[:topk]\n",
    "        best_scores = scores[best_indices]\n",
    "        topk_doc_indices = [docs[idx] for idx in best_indices]\n",
    "        return topk_doc_indices, best_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load up the data, both document store (`evidence.json`) and training/validation instances (`train-claims.json`, `dev-claims.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n"
     ]
    }
   ],
   "source": [
    "# load the evidence passages\n",
    "with open(\"project-data/evidence.json\", \"r\") as train_file:\n",
    "    document_store = json.load(train_file)         \n",
    "print(f\"Number of evidence passages: {len(document_store)}\")\n",
    "\n",
    "# load the training data insttances\n",
    "with open(\"project-data/train-claims.json\", \"r\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "print(f\"Number of training instances: {len(train_data)}\")\n",
    "\n",
    "# load the validation data instances\n",
    "with open(\"project-data/dev-claims.json\", \"r\") as dev_file:\n",
    "    val_data = json.load(dev_file)    \n",
    "print(f\"Number of validation instances: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence-0: John Bennet Lawes, English entrepreneur and agricultural scientist\n",
      "evidence-1: Lindberg began his professional career at the age of 16, eventually moving to New York City in 1977.\n",
      "evidence-2: ``Boston (Ladies of Cambridge)'' by Vampire Weekend\n",
      "evidence-3: Gerald Francis Goyer (born October 20, 1936) was a professional ice hockey player who played 40 games in the National Hockey League.\n",
      "evidence-4: He detected abnormalities of oxytocinergic function in schizoaffective mania, post-partum psychosis and how ECT modified oxytocin release.\n",
      "evidence-5: With peak winds of 110 mph (175 km/h) and a minimum pressure of 972 mbar (hPa ; 28.71 inHg), Florence was the strongest storm of the 1994 Atlantic hurricane season.\n",
      "evidence-6: He is currently a professor of piano at the University of Wisconsin -- Madison since August 2000.\n",
      "evidence-7: In addition to known and tangible risks, unforeseeable black swan extinction events may occur, presenting an additional methodological problem.\n",
      "evidence-8: Sir John Sherbrooke was able to hold her off for some five hours until Robson suffered a severe wound that almost killed him.\n",
      "evidence-9: Aslan Tlebzu (Аслъан ЛIыбзэу [- adyaːsɬaːn ɬʼəbzaw], Russian : Аслан Тлебзу), born 24 February 1981, Teuchezhsk, Adygea, USSR ; is a Russian Adyghe folk musician.\n",
      "\n",
      "claim-1937: {'claim_text': 'Not only is there no scientific evidence that CO2 is a pollutant, higher CO2 concentrations actually help ecosystems support more plant and animal life.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-442946', 'evidence-1194317', 'evidence-12171']}\n",
      "claim-126: {'claim_text': 'El Niño drove record highs in global temperatures suggesting rise may not be down to man-made emissions.', 'claim_label': 'REFUTES', 'evidences': ['evidence-338219', 'evidence-1127398']}\n",
      "claim-2510: {'claim_text': 'In 1946, PDO switched to a cool phase.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-530063', 'evidence-984887']}\n",
      "claim-2021: {'claim_text': 'Weather Channel co-founder John Coleman provided evidence that convincingly refutes the concept of anthropogenic global warming.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-1177431', 'evidence-782448', 'evidence-540069', 'evidence-352655', 'evidence-1007867']}\n",
      "claim-2449: {'claim_text': '\"January 2008 capped a 12 month period of global temperature drops on all of the major well respected indicators.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-1010750', 'evidence-91661', 'evidence-722725', 'evidence-554161', 'evidence-430839']}\n",
      "claim-851: {'claim_text': 'The last time the planet was even four degrees warmer, Peter Brannen points out in The Ends of the World, his new history of the planet’s major extinction events, the oceans were hundreds of feet higher.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-226174', 'evidence-1049316', 'evidence-358301', 'evidence-493329', 'evidence-610497']}\n",
      "claim-2773: {'claim_text': 'Tree-ring proxy reconstructions are reliable before 1960, tracking closely with the instrumental record and other independent proxies.', 'claim_label': 'DISPUTED', 'evidences': ['evidence-974673', 'evidence-602109']}\n",
      "claim-949: {'claim_text': 'Under the most ambitious scenarios, they found a strong likelihood that Antarctica would remain fairly stable.”', 'claim_label': 'DISPUTED', 'evidences': ['evidence-707654', 'evidence-28478', 'evidence-491579']}\n",
      "claim-1019: {'claim_text': 'An additional kick was supplied by an El Niño weather pattern that peaked in 2016 and temporarily warmed much of the surface of the planet, causing the hottest year in a historical record dating to 1880.', 'claim_label': 'NOT_ENOUGH_INFO', 'evidences': ['evidence-863309', 'evidence-61462', 'evidence-639818', 'evidence-757821', 'evidence-263527']}\n",
      "claim-2834: {'claim_text': 'When stomata-derived CO2 (red) is compared to ice core-derived CO2 (blue), the stomata generally show much more variability in the atmospheric CO2 level and often show levels much higher than the ice cores.', 'claim_label': 'SUPPORTS', 'evidences': ['evidence-439640']}\n"
     ]
    }
   ],
   "source": [
    "# examples of some evidence passages\n",
    "j = 0\n",
    "for i, evidence_text in document_store.items():\n",
    "    print(f\"{i}: {evidence_text}\")\n",
    "    j += 1\n",
    "    if j == 10:\n",
    "        break\n",
    "\n",
    "# examples of some training instances\n",
    "print(\"\")\n",
    "j = 0\n",
    "for i, claim in train_data.items():\n",
    "    print(f\"{i}: {claim}\")\n",
    "    j += 1\n",
    "    if j == 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train a BM25 model on this document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1208827 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208827/1208827 [00:25<00:00, 46668.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF and creating inverted index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 576915/576915 [00:34<00:00, 16624.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF vector norms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208827/1208827 [00:20<00:00, 60324.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# instantiate and train a retirever \n",
    "retreiver = IR_System()\n",
    "passages = list(document_store.values()) \n",
    "retreiver.train(passages)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's run some tests. First, we pick a few example training instances and compare the retreived passages with the ground truth evidence list. We also have to pick a value for the top-k parameter for the retreiver, we'll use k=5 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_ids = list(document_store.keys())\n",
    "train_claims = list(train_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Severe ‘snowmageddon’ winters are now strongly linked to soaring polar temperatures, say researchers, with deadly summer heatwaves and torrential floods also probably linked.\n",
      "Label: NOT_ENOUGH_INFO\n",
      "Gold evidence list:\n",
      "evidence-611438: The El Niño phenomenon was blamed for the unusually high sea surface temperatures in the Pacific Ocean that moved east, thus pulling rainfall along with it.\n",
      "evidence-392043: Eric Klinenberg has noted that in the United States, the loss of human life in hot spells in summer exceeds that caused by all other weather events combined, including lightning, rain, floods, hurricanes, and tornadoes.\n",
      "evidence-236630: The most deadly heat wave in the history of Pakistan is the record-breaking heat wave of summer 2010 which occurred in the last ten days of May.\n",
      "evidence-516337: The climate is characterized by hot, dry summers and cool, wet winters.\n",
      "evidence-717735: Rapid, dramatic temperature swings were common, with temperatures sometimes reverting from normal or above-normal summer temperatures as high as 95 °F (35 °C) to near-freezing within hours.\n",
      "Score: 1.4880754542744612, evidence-713071: Most of the people of this village are linked with agriculture.\n",
      "Score: 1.4571130774896661, evidence-633996: Health concerns around the world can be linked to floods.\n",
      "Score: 1.2938025706102005, evidence-310843: The title remains strongly linked with the town of Guisborough.\n",
      "Score: 1.189511175945183, evidence-1081305: It is now linked on the east side to the Biotech Building of 1999.\n",
      "Score: 1.1182776223417223, evidence-83917: It is linked to the peninsula by a causeway.\n",
      "Score: 1.1121776544846362, evidence-509660: Most enzyme-linked receptors are of this type.\n",
      "Score: 1.1022355920286546, evidence-1096100: It 's linked to the city centre by metro line C.\n",
      "Score: 1.0993991794607092, evidence-1005059: About 25 % of cases are linked to smoking, and 5 -- 10 % are linked to inherited genes.\n",
      "Score: 1.0945815338024545, evidence-853782: The breakup events may be linked to the dramatic polar warming trends that are part of global warming.\n",
      "Score: 1.0853566675308364, evidence-760936: They are linked to the Christian Concern campaigning organisation.\n",
      "Score: 1.0828021401456611, evidence-765547: \"Q&A: How is Arctic warming linked to the 'polar vortex' and other extreme weather?\".\n",
      "Score: 1.0815309856805642, evidence-1046024: Climate researchers have suggested that the unusual weather leading to the floods may be linked to this year's appearance of La Nina in the Pacific Ocean, and the jet stream being further south than normal.\n",
      "Score: 1.0794646492744395, evidence-694758: There are also more than 600 lay judges () linked to the courts of appeal.\n",
      "Score: 1.0498552894426618, evidence-212215: Experts say that power plants are responsible for about 40 percent of U.S. carbon dioxide pollution linked to global warming.\n",
      "Score: 1.0479726978521577, evidence-54145: Some clubs have been linked to organized crime.\n",
      "Score: 1.0412370736595409, evidence-1132111: The Sora is of torrential character and often floods.\n",
      "Score: 1.0399432827869086, evidence-1082864: The canal is linked (in order, from the Severn) to:\n",
      "Score: 1.0341007243337363, evidence-166649: Summer temperatures in the area are frequently 50 to, and the night is commonly 40 to.\n",
      "Score: 1.011557663812767, evidence-1162042: Later historians have linked him with two individuals of the same name.\n",
      "Score: 1.000773449268538, evidence-59725: The newspaper was linked to the Sudanese Communist Party.\n",
      "Precision: 0.0, Recall: 0.0, F1: 0\n"
     ]
    }
   ],
   "source": [
    "example_claim = train_claims[125]\n",
    "query = example_claim[\"claim_text\"]\n",
    "gold_evidence_list = example_claim[\"evidences\"]\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Label: {example_claim['claim_label']}\")\n",
    "print(f\"Gold evidence list:\")\n",
    "for evidence_id in gold_evidence_list:\n",
    "    print(f\"{evidence_id}: {document_store[evidence_id]}\")\n",
    "\n",
    "# retrieve relevant evidence passages\n",
    "topk_doc_indices, best_scores = retreiver.retrieve_docs(query, topk=20)\n",
    "topk_evidence_ids = [passage_ids[idx] for idx in topk_doc_indices]\n",
    "for i, evidence_id in enumerate(topk_evidence_ids):\n",
    "    print(f\"Score: {best_scores[i]}, {evidence_id}: {document_store[evidence_id]}\")\n",
    "\n",
    "# evaluation (precision, recall, F1)\n",
    "intersection = set(topk_evidence_ids).intersection(gold_evidence_list)\n",
    "precision = len(intersection) / len(topk_evidence_ids)\n",
    "recall = len(intersection) / len(gold_evidence_list)\n",
    "\n",
    "precision = precision / len(topk_evidence_ids)\n",
    "recall = recall / len(gold_evidence_list)\n",
    "f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
