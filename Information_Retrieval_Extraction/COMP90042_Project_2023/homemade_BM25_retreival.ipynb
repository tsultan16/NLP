{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMP90042 2023 Project: Automated Fact Checking For Climate Science Claims\n",
    "\n",
    "The goal of this project is to design and implement a system that takes a given `claim` (which is a single sentence) and then retrieves (one or more) `evidence passages` (each passage is also a single sentence) from a document store, then using these evidence passages classifies the claim as one of these four labels: `[SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]`.  \n",
    "\n",
    "Our training data consists of a document store which is a set of $D$ evidence passages `{evidence-1, evidence-2, ..., evidence-D}` and training and validation data instances where each instance is a tuple of the form `(claim_text, claim_label, evidence_list)`, where evidence list is a subset of passages from the document store.\n",
    "\n",
    "In this notebook, we explore the use of `BM25` for passage retreival and evaluate it's performance on the project dataset. Evaluation will involve comparing retreived evidence passages for each data instance with the ground truth evidence list  and computing precision, recall and F1 score. Then we will use the `average F1 score` across all data instances as the final evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "import math, random\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is my implementation of a simple BM25 retreival system. Will later replace this with the PyLucene implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IR_System():\n",
    "    def __init__(self, k = 1.25, b = 0.75):\n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+') \n",
    "\n",
    "\n",
    "    def train(self, documents):\n",
    "        self.documents = documents\n",
    "        self.TFIDF, self.inverted_index, self.doc_tfidf_norms = self.create_inverted_index()\n",
    "        \n",
    "    def tokenize(self, sent):\n",
    "        # Replace accented letters with regular letters\n",
    "        sent = unidecode(sent)\n",
    "        # tokenize into words, remove punctuation\n",
    "        return self.tokenizer.tokenize(sent.lower())\n",
    "\n",
    "    def create_inverted_index(self):\n",
    "        N = len(self.documents)\n",
    "        TFIDF = defaultdict(float)\n",
    "        inverted_index = defaultdict(list)\n",
    "\n",
    "        # compute term frequency and document frequencies\n",
    "        TF, term_docs = self.compute_TF_weighted()\n",
    "\n",
    "        # create inverted index\n",
    "        print(f\"Computing TFIDF and creating inverted index...\")\n",
    "        for w, docs in tqdm(term_docs.items(), total=len(term_docs)):\n",
    "            for d in sorted(list(docs)):\n",
    "                tfidf = TF[(w,d)] * math.log10(N/len(docs))\n",
    "                inverted_index[w].append(d)\n",
    "                TFIDF[(w,d)] = tfidf\n",
    "\n",
    "        # compute document TFIDF vector norms\n",
    "        print(f\"Computing TFIDF vector norms...\")\n",
    "        doc_tfidf_norms = [0] * N\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = self.tokenize(doc)\n",
    "            for w in words:\n",
    "                doc_tfidf_norms[d] = doc_tfidf_norms[d] +  TFIDF[(w,d)]**2\n",
    "            doc_tfidf_norms[d] = math.sqrt(doc_tfidf_norms[d])\n",
    "\n",
    "        return TFIDF, inverted_index, doc_tfidf_norms  \n",
    "          \n",
    "    # weighted TF for BM25\n",
    "    def compute_TF_weighted(self):\n",
    "        TF = defaultdict(int)\n",
    "        term_docs = defaultdict(set)\n",
    "        doc_length = defaultdict(float)\n",
    "        Dtotal = 0\n",
    "        print(f\"Computing TFIDF...\")\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = self.tokenize(doc)\n",
    "            for w in words:\n",
    "                TF[(w, d)] += 1\n",
    "                term_docs[w].add(d)\n",
    "            doc_length[d] = len(words)\n",
    "            Dtotal += len(words)\n",
    "        Davg = Dtotal / len(self.documents)\n",
    "\n",
    "        # compute BM25 weighted term frequencies\n",
    "        TF_weighted = defaultdict(float)\n",
    "        for (w,d), tf in TF.items():\n",
    "            TF_weighted[(w,d)] = (tf * (self.k + 1)) / (tf + self.k * (1 - self.b + self.b * (doc_length[d]/Davg)))\n",
    "        return TF_weighted, term_docs\n",
    "    \n",
    "\n",
    "    def retrieve_docs(self, query, topk=1):\n",
    "        query_words = self.tokenizer.tokenize(query.lower())\n",
    "        #print(f\"query words: {query_words}\")\n",
    "        # get all documents which contain words from query\n",
    "        docs = []\n",
    "        for w in query_words:\n",
    "            docs.extend([d for d in self.inverted_index[w]])\n",
    "        #print(f\"docs: \")    \n",
    "        # score all these documents\n",
    "        scores = defaultdict(float)\n",
    "        for d in docs:\n",
    "            for w in query_words:\n",
    "                scores[d] += self.TFIDF[(w,d)]\n",
    "            scores[d] = scores[d] / self.doc_tfidf_norms[d]        \n",
    "        #print(f\"scores: {scores}\")    \n",
    "        # return topk documents\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        #print(f\"sorted scores: {sorted_scores}\")\n",
    "        best = sorted_scores[:topk]\n",
    "        topk_docs = []\n",
    "        for doc, score in best:\n",
    "            topk_docs.append((self.documents[doc], score))\n",
    "        return topk_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load up the data, both document store (`evidence.json`) and training/validation instances (`train-claims.json`, `dev-claims.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n"
     ]
    }
   ],
   "source": [
    "# load the evidence passages\n",
    "with open(\"project-data/evidence.json\", \"r\") as train_file:\n",
    "    document_store = json.load(train_file)         \n",
    "print(f\"Number of evidence passages: {len(document_store)}\")\n",
    "\n",
    "# load the training data insttances\n",
    "with open(\"project-data/train-claims.json\", \"r\") as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "print(f\"Number of training instances: {len(train_data)}\")\n",
    "\n",
    "\n",
    "# load the validation data instances\n",
    "with open(\"project-data/dev-claims.json\", \"r\") as dev_file:\n",
    "    val_data = json.load(dev_file)    \n",
    "print(f\"Number of validation instances: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1228"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
