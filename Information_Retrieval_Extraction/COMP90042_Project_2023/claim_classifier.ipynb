{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claim Label Classification\n",
    "\n",
    "Recall that the goal of this project is to design and implement a system that takes a given `claim` and then retrieves (one or more) `evidence passages` from a document store, then using these evidence passages classifies the claim as one of these four labels: `[SUPPORTS, REFUTES, NOT_ENOUGH_INFO, DISPUTED]. \n",
    "\n",
    "Our first step will be to use train a simple BERT based clasifier which takes as input a sequence containing a `(claim, single evidence passage)` pair in the format: `[CLS] claim text [SEP] evidence text [SEP]`and classifies it by passing the `[CLS]` output embedding to a softmax classifier. If for a given claim `c`, if we have multiple evidence passages `[e_1, e_2, .., e_n]`, then we will have separate input pairs `(c, e_1)`, .., `(c, e_n)` all of which are assigned the same label. Then during inference time, given that we have multiple evidence passages and a claim, we classify every pair `(c,e_i)` and take a majority vote of the label. Note that this model assumes that the multiple evidences passages for a single claim independently determine the class label, which may not be true in some cases where the different evidences can interact in some complex way to determine the label. \n",
    "\n",
    "The next step will be to aggregate all the evidence passgaes into a single passage and classify input sequence containing `(claim, aggregated evidence passages)`. The simplest form of aggregation would be to directly concatenate all the input evidence passages into a single passage. Howevere, due to the maximum input sequence length imposed by BERT, we may need to do some truncation of these passages. This model will be able to learn the interactions between the different evidence passages which may lead to improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, DistilBertTokenizerFast\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import os\n",
    "from DPR_biencoder_simple import *\n",
    "import wandb\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's load the data and look at some examples and the label distribution\n",
    "label_dict = {'SUPPORTS':0, 'REFUTES':1, 'NOT_ENOUGH_INFO':2, 'DISPUTED':3}\n",
    "document_store, train_data, val_data = load_data(clean=True, clean_threshold=40)\n",
    "train_labels = [label_dict[claim['claim_label']] for claim_id, claim in train_data.items()]\n",
    "\n",
    "# plot label distribution histogram\n",
    "plt.hist(train_labels, bins=4)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Training Label Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some examples of claims, evidence, and labels\n",
    "c = random.sample(train_data.items(), 10)\n",
    "for claim_id, claim in c:\n",
    "    print(f\"{claim_id} --> {claim['claim_text']}\")\n",
    "    print(f\"Evidences:\")\n",
    "    for ev in claim['evidences']:\n",
    "        print(f\"\\t{document_store[ev]}\")\n",
    "    print(f\"Claim Label: {claim['claim_label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer parallelism to False\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  \n",
    "\n",
    "class ClaimsDatasetSingle(Dataset):\n",
    "    def __init__(self, claims_data, document_store, label_dict, block_size=192):\n",
    "        self.claims_data = claims_data\n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.document_store = document_store\n",
    "        self.label_dict = label_dict\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "    def create_pairs(self):\n",
    "        claim_pairs = []\n",
    "        for claim_id in self.claims_data.keys():\n",
    "            for evidence_id in self.claims_data[claim_id]['evidences']:\n",
    "                claim_pairs.append((claim_id, evidence_id))  \n",
    "        return claim_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claim_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim id and evidence id\n",
    "        claim_id, evidence_id = self.claim_pairs[idx]\n",
    "        target_label = self.label_dict[self.claims_data[claim_id]['claim_label']]\n",
    "        # get the claim and evidence text\n",
    "        claim_text = self.claims_data[claim_id]['claim_text']\n",
    "        evidence_text = self.document_store[evidence_id]\n",
    "        # encode and create tensors\n",
    "        input_idx, input_attn_mask, token_type_idx = self.tokenize_and_encode(claim_text, evidence_text)\n",
    "        target_label = torch.tensor(target_label)\n",
    "        return input_idx, input_attn_mask, token_type_idx, target_label\n",
    "\n",
    "    def tokenize_and_encode(self, claim_text, evidence_text):\n",
    "        # tokenize the claim and evidence text  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        evidence_encoding = self.tokenizer.encode_plus(evidence_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        evidence_idx = evidence_encoding['input_ids']\n",
    "\n",
    "        # select a random window from the evidence passage if it won't fit in block size\n",
    "        max_evidence_size = self.block_size - len(claim_idx) - 3\n",
    "        if len(evidence_idx) > max_evidence_size:\n",
    "            # pick a random start position\n",
    "            start_pos = random.randint(0, max(0,len(evidence_idx)-max_evidence_size))\n",
    "            # select the window\n",
    "            evidence_idx = evidence_idx[start_pos:start_pos+max_evidence_size]\n",
    " \n",
    "        # concatenate the claim and evidence, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id] + evidence_idx + [self.tokenizer.sep_token_id]\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "\n",
    "        # create segment ids\n",
    "        claim_len = len(claim_idx) + 2\n",
    "        evidence_len = len(evidence_idx) + 1\n",
    "        token_type_idx = [0] * claim_len + [1] * evidence_len + [0] * (self.block_size - claim_len - evidence_len)\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.block_size}!\")\n",
    "    \n",
    "        # create attention masks\n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask)\n",
    "        token_type_idx = torch.tensor(token_type_idx)  # don't need this for roberta\n",
    "\n",
    "        return input_idx, input_attn_mask, token_type_idx\n",
    "\n",
    "\n",
    "\n",
    "class ClaimsDatasetAggregate(Dataset):\n",
    "    def __init__(self, claims_data, document_store, label_dict, block_size=192):\n",
    "        self.claims_data = claims_data.items()\n",
    "        self.hard_negatives = hard_negatives\n",
    "        self.document_store = document_store\n",
    "        self.label_dict = label_dict\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        self.claim_pairs = self.create_pairs()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.claims_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get claim and evidence texts\n",
    "        claim_id, claim = self.claims_data[idx]\n",
    "        claim_text = claim['claim_text']\n",
    "        evidences_text = [self.document_store[evidence_id] for evidence_id in claim['evidences']]\n",
    "        target_label = self.label_dict[claim['claim_label']]\n",
    "        # encode and create tensors\n",
    "        input_idx, input_attn_mask, token_type_idx = self.tokenize_and_encode(claim_text, evidences_text)\n",
    "        target_label = torch.tensor(target_label)\n",
    "        return input_idx, input_attn_mask, token_type_idx, target_label\n",
    "\n",
    "    def tokenize_and_encode(self, claim_text, evidences_text):\n",
    "        # tokenize the claim and evidence text  \n",
    "        claim_encoding = self.tokenizer.encode_plus(claim_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        claim_idx = claim_encoding['input_ids']\n",
    "        evidence_encoding = self.tokenizer.batch_encode_plus(evidences_text, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        evidence_idx = evidence_encoding['input_ids']\n",
    "\n",
    "        total_evidence_length = sum([len(evidence) for evidence in evidence_idx])\n",
    "        max_evidence_size = self.block_size - len(claim_idx) - 3\n",
    "        # take a separate random window from each evidence passage, make sure the proportion of evidence is the same, and that it fits in max_evidence_size\n",
    "        if total_evidence_length > max_evidence_size:\n",
    "            windowed_evidence_idx = []\n",
    "            for evidence in evidence_idx:\n",
    "                desired_length = max_evidence_size*len(evidence)//total_evidence_length\n",
    "                # pick a random start position\n",
    "                start_pos = random.randint(0, max(0,len(evidence)-desired_length))\n",
    "                # select the window\n",
    "                evidence = evidence[start_pos:start_pos+desired_length]\n",
    "                windowed_evidence_idx.append(evidence)\n",
    "            evidence_idx = windowed_evidence_idx\n",
    "\n",
    "        # concatenate the evidences\n",
    "        evidence_idx = [idx for evidence in evidence_idx for idx in evidence] \n",
    "                  \n",
    "        # concatenate the claim and evidence, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id] + claim_idx + [self.tokenizer.sep_token_id] + evidence_idx + [self.tokenizer.sep_token_id]\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "\n",
    "        # create segment ids\n",
    "        claim_len = len(claim_idx) + 2\n",
    "        evidence_len = len(evidence_idx) + 1\n",
    "        token_type_idx = [0] * claim_len + [1] * evidence_len + [0] * (self.block_size - claim_len - evidence_len)\n",
    "\n",
    "        # make sure the passage sequences and claim sequences are not longer than max_length\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.block_size}!\")\n",
    "    \n",
    "        # create attention masks\n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask)\n",
    "        token_type_idx = torch.tensor(token_type_idx)  # don't need this for roberta\n",
    "\n",
    "        return input_idx, input_attn_mask, token_type_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimClassifier(torch.nn.Module):\n",
    "    def __init__(self, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head\n",
    "        self.classifier_head = torch.nn.Linear(768, 4)\n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, token_type_idx, targets=None):\n",
    "        # compute BERT encodings, extract the pooler output (which is just the [CLS] embedding fed through a feedforward network or just the [CLS] embedding), apply dropout        \n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask, token_type_ids=token_type_idx)\n",
    "        pooled_output = self.dropout(bert_output.last_hidden_state[:,0]) # shape: (batch_size, hidden_size)\n",
    "        # compute output logits\n",
    "        logits = self.classifier_head(pooled_output) # shape: (batch_size, 4)\n",
    "        # compute cross-entropy loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
