{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Haystack Passage Retirevers\n",
    "\n",
    "Previously we implemented our own BM25 and DPR retreiver models. We will now switch to using retreiver models provided by the Haystack library, which are optimized for better performance and have loads of useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore, FAISSDocumentStore\n",
    "from haystack.nodes import BM25Retriever, DensePassageRetriever\n",
    "from utils import *\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evidence passages: 1208827\n",
      "Number of training instances: 1228\n",
      "Number of validation instances: 154\n",
      "Number of evidence passages remaining after cleaning: 1204715\n"
     ]
    }
   ],
   "source": [
    "# load data from file\n",
    "passages, train_data, val_data = load_data(clean=True, clean_threshold=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating BM25 representation...: 100%|██████████| 1204715/1204715 [00:17<00:00, 69241.03 docs/s]\n"
     ]
    }
   ],
   "source": [
    "documents = [{\"id\":p_id, \"content\": p_text} for p_id, p_text in list(passages.items())]\n",
    "\n",
    "# create haystack in-memory document store\n",
    "document_store = InMemoryDocumentStore(use_bm25=True)\n",
    "document_store.write_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a haystack BM-25 retreiver\n",
    "retreiver = BM25Retriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims = list(train_data.items())\n",
    "val_claims = list(val_data.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out this BM25 retreiver on some example claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim --> that atmospheric CO2 increase that we observe is a product of temperature  increase, and not the other way around, meaning it is a product of  natural variation...\n",
      "\n",
      "Gold evidences: \n",
      "\t evidence-368192 --> Increases in atmospheric concentrations of CO 2 and other long-lived greenhouse gases such as methane, nitrous oxide and ozone have correspondingly strengthened their absorption and emission of infrared radiation, causing the rise in average global temperature since the mid-20th century.\n",
      "\t evidence-423643 --> During the late 20th century, a scientific consensus evolved that increasing concentrations of greenhouse gases in the atmosphere cause a substantial rise in global temperatures and changes to other parts of the climate system, with consequences for the environment and for human health.\n",
      "\n",
      "BM25 top-5 documents:\n",
      "\tevidence-100018 --> The ice core data shows that temperature change causes the level of atmospheric CO2 to change - not the other way round.\n",
      "\tevidence-548766 --> Due to the increase in temperature of the soil, CO2 levels in our atmosphere increase, and as such the mean average temperature of the Earth is rising.\n",
      "\tevidence-498380 --> Current annual increase in atmospheric CO2 is approximately 4 gigatons of carbon.\n",
      "\tevidence-296134 --> The ice core data shows that temperature change causes the level of atmospheric to change — not the other way round.\n",
      "\tevidence-382866 --> The heat needed to raise an average temperature increase of the entire world ocean by 0.01 °C would increase the atmospheric temperature by approximately 10 °C.\n"
     ]
    }
   ],
   "source": [
    "# now do a quick test of the retriever\n",
    "idx = random.randint(0, len(train_claims))  \n",
    "claim_text = train_claims[idx][1]['claim_text']\n",
    "gold_evidence_list = train_claims[idx][1][\"evidences\"]\n",
    "\n",
    "# retreive BM25 top-5 documents  \n",
    "topk_documents = retreiver.retrieve(query=claim_text, top_k=5)\n",
    "\n",
    "print(f\"Claim --> {claim_text}\")\n",
    "print(f\"\\nGold evidences: \")\n",
    "for evidence in gold_evidence_list:\n",
    "    print(f\"\\t {evidence} --> {passages[evidence]}\")\n",
    "\n",
    "print(f\"\\nBM25 top-5 documents:\")\n",
    "for doc in topk_documents:\n",
    "    print(f\"\\t{doc.id} --> {doc.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the evaluation metrics for this BM25 retreiver and see how much it differs from our \"home-made\" BM25 implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(claims_list, retreiver, topk=[5]):\n",
    "    precision_total = np.zeros(len(topk))\n",
    "    recall_total = np.zeros(len(topk))\n",
    "    f1_total = np.zeros(len(topk))\n",
    "\n",
    "    for claim_id, claim in tqdm(claims_list):\n",
    "        claim_text = claim['claim_text']\n",
    "        gold_evidence_list = claim['evidences']\n",
    "        # get BM25 top-k passages \n",
    "        topk_documents = retreiver.retrieve(query=claim_text, top_k=max(topk)) \n",
    "        \n",
    "        # keep top-k reranked passages\n",
    "        for i,k in enumerate(topk):\n",
    "            retreived_doc_ids = [doc.id for doc in topk_documents[:k]]\n",
    "            intersection = set(retreived_doc_ids).intersection(gold_evidence_list)\n",
    "            precision = len(intersection) / len(retreived_doc_ids)\n",
    "            recall = len(intersection) / len(gold_evidence_list)\n",
    "            f1 = (2*precision*recall/(precision + recall)) if (precision + recall) > 0 else 0 \n",
    "            precision_total[i] += precision\n",
    "            recall_total[i] += recall\n",
    "            f1_total[i] += f1\n",
    "\n",
    "    precision_avg = precision_total / len(claims_list)\n",
    "    recall_avg = recall_total / len(claims_list)\n",
    "    f1_avg = f1_total / len(claims_list)    \n",
    "\n",
    "    # convert to dictionary\n",
    "    precision_avg = {f\"Precision@{k}\":v for k,v in zip(topk, precision_avg)}\n",
    "    recall_avg = {f\"Recall@{k}\":v for k,v in zip(topk, recall_avg)}\n",
    "    f1_avg = {f\"F1@{k}\":v for k,v in zip(topk, f1_avg)}\n",
    "\n",
    "    print(f\"\\nAvg Precision: {precision_avg}, Avg Recall: {recall_avg}, Avg F1: {f1_avg}\")\n",
    "    return precision_avg, recall_avg, f1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1228/1228 [1:21:53<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: {'Precision@1': 0.16042345276872963, 'Precision@3': 0.11264929424538521, 'Precision@5': 0.08762214983713407, 'Precision@10': 0.06001628664495125, 'Precision@20': 0.03811074918566769, 'Precision@50': 0.020114006514657747, 'Precision@100': 0.011995114006514512, 'Precision@250': 0.005954397394136726, 'Precision@500': 0.0034055374592833368, 'Precision@1000': 0.0019144951140064824}, Avg Recall: {'Recall@1': 0.05359663409337685, 'Recall@3': 0.11235070575461469, 'Recall@5': 0.14132736156351794, 'Recall@10': 0.1941096634093376, 'Recall@20': 0.24214169381107453, 'Recall@50': 0.3116042345276867, 'Recall@100': 0.3753528773072739, 'Recall@250': 0.46114277958740424, 'Recall@500': 0.5226520086862111, 'Recall@1000': 0.5875814332247552}, Avg F1: {'F1@1': 0.07581433224755703, 'F1@3': 0.10515549868155738, 'F1@5': 0.1021747841373252, 'F1@10': 0.08788494350383635, 'F1@20': 0.06414886876329999, 'F1@50': 0.03733490936246122, 'F1@100': 0.02309283869110577, 'F1@250': 0.011725165815050799, 'F1@500': 0.006757844324751354, 'F1@1000': 0.0038139337455072022}\n",
      "Evalutating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [10:13<00:00,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg Precision: {'Precision@1': 0.15584415584415584, 'Precision@3': 0.11471861471861472, 'Precision@5': 0.08831168831168827, 'Precision@10': 0.0571428571428571, 'Precision@20': 0.036363636363636334, 'Precision@50': 0.021038961038961055, 'Precision@100': 0.012597402597402607, 'Precision@250': 0.006077922077922082, 'Precision@500': 0.003428571428571431, 'Precision@1000': 0.0019480519480519494}, Avg Recall: {'Recall@1': 0.07835497835497834, 'Recall@3': 0.12911255411255412, 'Recall@5': 0.16320346320346313, 'Recall@10': 0.21277056277056272, 'Recall@20': 0.25930735930735943, 'Recall@50': 0.3464285714285715, 'Recall@100': 0.4102813852813853, 'Recall@250': 0.49058441558441546, 'Recall@500': 0.5627705627705626, 'Recall@1000': 0.6457792207792207}, Avg F1: {'F1@1': 0.09632034632034633, 'F1@3': 0.11162646876932593, 'F1@5': 0.10682333539476398, 'F1@10': 0.08540420618342694, 'F1@20': 0.061792711335342615, 'F1@50': 0.039133716128973914, 'F1@100': 0.024275545967757394, 'F1@250': 0.011972195531450198, 'F1@500': 0.006805071949602836, 'F1@1000': 0.003881297488010652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Precision@1': 0.15584415584415584,\n",
       "  'Precision@3': 0.11471861471861472,\n",
       "  'Precision@5': 0.08831168831168827,\n",
       "  'Precision@10': 0.0571428571428571,\n",
       "  'Precision@20': 0.036363636363636334,\n",
       "  'Precision@50': 0.021038961038961055,\n",
       "  'Precision@100': 0.012597402597402607,\n",
       "  'Precision@250': 0.006077922077922082,\n",
       "  'Precision@500': 0.003428571428571431,\n",
       "  'Precision@1000': 0.0019480519480519494},\n",
       " {'Recall@1': 0.07835497835497834,\n",
       "  'Recall@3': 0.12911255411255412,\n",
       "  'Recall@5': 0.16320346320346313,\n",
       "  'Recall@10': 0.21277056277056272,\n",
       "  'Recall@20': 0.25930735930735943,\n",
       "  'Recall@50': 0.3464285714285715,\n",
       "  'Recall@100': 0.4102813852813853,\n",
       "  'Recall@250': 0.49058441558441546,\n",
       "  'Recall@500': 0.5627705627705626,\n",
       "  'Recall@1000': 0.6457792207792207},\n",
       " {'F1@1': 0.09632034632034633,\n",
       "  'F1@3': 0.11162646876932593,\n",
       "  'F1@5': 0.10682333539476398,\n",
       "  'F1@10': 0.08540420618342694,\n",
       "  'F1@20': 0.061792711335342615,\n",
       "  'F1@50': 0.039133716128973914,\n",
       "  'F1@100': 0.024275545967757394,\n",
       "  'F1@250': 0.011972195531450198,\n",
       "  'F1@500': 0.006805071949602836,\n",
       "  'F1@1000': 0.003881297488010652})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval on training set\n",
    "print(\"Evaluating on training set...\")\n",
    "eval(train_claims, retreiver, topk=[1, 3, 5, 10, 20, 50, 100, 250, 500, 1000])\n",
    "\n",
    "print(\"Evalutating on validation set...\")\n",
    "eval(val_claims, retreiver, topk=[1, 3, 5, 10, 20, 50, 100, 250, 500, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best F1-score is about 11% for top-3 retreived documents. \n",
    "\n",
    "Now we will finetune a DPR model using haystack and see how well it performs. First, let's put the data in the right format and save it in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pairs of claim and evidence\n",
    "\"\"\"\n",
    "train_pairs = []\n",
    "for claim_id, claim in train_data.items():\n",
    "    claim_text = claim['claim_text']\n",
    "    for evidence_id in claim['evidences']:\n",
    "        train_pairs.append((claim_id, evidence_id)) \n",
    "        \n",
    "val_pairs = []\n",
    "for claim_id, claim in val_data.items():\n",
    "    claim_text = claim['claim_text']\n",
    "    for evidence_id in claim['evidences']:\n",
    "        val_pairs.append((claim_id, evidence_id)) \n",
    "\n",
    "# now let's get some hard negatives that we obtained using our old DPR implementation\n",
    "with open(\"dpr_embeddings/train_hard_negatives.pkl\", \"rb\") as f:\n",
    "    train_hard_negatives = pickle.load(f)\n",
    "with open(\"dpr_embeddings/val_hard_negatives.pkl\", \"rb\") as f:\n",
    "    val_hard_negatives = pickle.load(f)  \n",
    "\n",
    "# now lets create the DPR training and validation instances, keep top 5 hard negatives\n",
    "train_instances = []\n",
    "for claim_id, evidence_id in train_pairs:\n",
    "    train_instances.append({\"question\": train_data[claim_id]['claim_text'], \"positive_ctxs\": [{\"title\": \"no_title\", \"text\":passages[evidence_id], \"passages_id\":evidence_id}], \"hard_negative_ctxs\": [{\"title\": \"no_title\", \"text\":passages[neg_id], \"passages_id\":neg_id} for neg_id in train_hard_negatives[claim_id][:5]]}) \n",
    "val_instances = []\n",
    "for claim_id, evidence_id in val_pairs:\n",
    "    val_instances.append({\"question\": val_data[claim_id]['claim_text'], \"positive_ctxs\": [{\"title\": \"no_title\", \"text\":passages[evidence_id], \"passages_id\":evidence_id}], \"hard_negative_ctxs\": [{\"title\": \"no_title\", \"text\":passages[neg_id], \"passages_id\":neg_id} for neg_id in val_hard_negatives[claim_id][:5]]})        \n",
    "\n",
    "# Write to JSON file\n",
    "with open('project-data/train_data_dpr.json', 'w') as f:\n",
    "    json.dump(train_instances, f)       \n",
    "\n",
    "with open('project-data/dev_data_dpr.json', 'w') as f:\n",
    "    json.dump(val_instances, f)       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up the haystack retreiver and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanzid/.local/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the document store\n",
    "document_store = FAISSDocumentStore()\n",
    "\n",
    "# Initialize the retriever\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Specify the directory where your training data is located\n",
    "doc_dir = \"project-data\"\n",
    "\n",
    "# Specify the names of your training, development, and test files\n",
    "train_filename = \"train_data_dpr.json\"\n",
    "dev_filename = \"dev_data_dpr.json\"\n",
    "\n",
    "# Specify the directory where the trained model should be saved\n",
    "save_dir = \"haystack_dpr_finetuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset: 100%|██████████| 9/9 [00:02<00:00,  3.69 Dicts/s]\n",
      "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  3.49 Dicts/s]\n",
      "Train epoch 0/0 (Cur. train loss: 10.8034):   0%|          | 1/258 [00:08<36:22,  8.49s/it]"
     ]
    }
   ],
   "source": [
    "# Start training the model\n",
    "retriever.train(\n",
    "    data_dir=doc_dir,\n",
    "    train_filename=train_filename,\n",
    "    dev_filename=dev_filename,\n",
    "    n_epochs=1,\n",
    "    batch_size=16,\n",
    "    grad_acc_steps=8,\n",
    "    save_dir=save_dir,\n",
    "    evaluate_every=100,\n",
    "    embed_title=False,\n",
    "    num_positives=1,\n",
    "    num_hard_negatives=1,\n",
    ")\n",
    "\n",
    "# load trained retreiver from file\n",
    "#retriever = DensePassageRetriever.load(load_dir=save_dir, document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write documents to document store\n",
    "documents = [{\"id\":p_id, \"content\": p_text} for p_id, p_text in list(passages.items())]\n",
    "document_store.write_documents(documents)\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'retriever' is your trained DPR model and 'document_store' is your document stor\n",
    "query = \"What is artificial intelligence?\"\n",
    "\n",
    "# Retrieve top-k documents\n",
    "results = retriever.retrieve(query, top_k=5)\n",
    "\n",
    "# 'results' is a list of Document objects. You can access the text of each document with the '.text' attribute.\n",
    "for result in results:\n",
    "    print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
