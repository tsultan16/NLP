{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Passage Retrieval\n",
    "\n",
    "A simple and efficient way of retreiving relevant documents (e.g. sentence or paragraph) from a document store is using `TFIDF`. Given a collection of documents, we can create TFIDF vectors for each document, which is a vector of TFIDF weights (one for each word from the vocabulary). \n",
    "\n",
    "Term-frequency for term $t$ in document $d$: \n",
    "\n",
    "$TF(t,d) = 1 + \\log_{10}(\\text{count}(t,d))$\n",
    "\n",
    "where $\\text{count}(t,d)$ is the frequency with which term $t$ appears in document $d$. (Note that we take the log to supress the range of count values).\n",
    "\n",
    "Inverse-document-frequency for term $t$:\n",
    "\n",
    "$IDF(t) = \\log_{10}(\\frac{N}{\\text{df}(t)})$\n",
    "\n",
    "where $N$ is the total number of documents in the collectino and $\\text{df}(t)$ is the number of documents in which term $t$ occurs. Then the TFIDF is given by the following product:\n",
    "\n",
    "$TFIDF(t,d) = TF(t,d) IDF(t)$\n",
    "\n",
    "\n",
    "Then given a query $q$ which is a sequence of terms, we can construct a TFIDF vector for this query. However, since queries are usually short and is likely to contain a single occurance of each unique term, we can simplify it's TFIDF vector by setting the TFIDF weight for each unique term to 1. Then we can compute a score for each document as the cosine similarity between the query vector and the corresponding document TFIDF vector $d$:\n",
    "\n",
    "$score(q,d) = \\frac{q \\cdot d}{|q| |d|}$\n",
    "\n",
    "Since $|q|$ is a fixed constant, we can ignore it because it will not affect the ranking of document scores. Then using our simplifying assumption of $q$ being a vector of binary weights, we have the following document score function:\n",
    "\n",
    "$score(q,d) = \\frac{\\sum_{t\\in q} TFIDF(t,d)}{\\sqrt{\\sum_{t\\in d} TFIDF(t,d)^2}}$\n",
    "\n",
    "where the square root term in the denominator is the norm of the document TFIDF vector. So the score for each document is just the TFIDF weights for the query terms which also appear in that document, normalized by the norm of that documents TFIDF vector.\n",
    "\n",
    "Now each word in the query will not occur in all documents, so we need to only consider documents that actually contain these query words instead of iterating over all documents in the collection. We can maintain an `inverted index` data structure which is a dictionary that maps each unique word to a list of tuples, each tuple containing a document and the TFIDF weight.\n",
    "\n",
    "e.g. `inverted_index = {'w1' : [(d1, TFIDF(w1,d1)), (d2, TFIDF(w1,d2),..)], 'w2': ...}`\n",
    "\n",
    "This data structure will allow us to compute and rank the document scores very efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test documents\n",
    "test_documents = [\"To be brief, I write for various reasons.\", \n",
    "                  \"I will confess that I have a fancy to be numbered among their honourable company.\",  \n",
    "                  \"Sir Henry Curtis, as everybody acquainted with him knows, is one of the most hospitable men on earth\", \n",
    "                  \"Everybody turned and stared politely at the curious-looking little lame man, and though his size was insignificant, he was quite worth staring at.\",\n",
    "                  \"Once it was a dense forest, now it's open level country cultivated here and there, but for the most part barren.\",\n",
    "                  \"Christian, the number of casualties from sickness has been very small indeed, and this although they frequently sleep in the trenches of newly-turned earth at all seasons of the year.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentence string into words, punctutaions removed\n",
    "def tokenize(sent):\n",
    "    # Replace accented letters with regular letters\n",
    "    sent = unidecode(sent)\n",
    "    # Tokenize\n",
    "    return tokenizer.tokenize(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class IR_System():\n",
    "    def __init__(self, documents, BM_25=False, k = 1.25, b = 0.75):\n",
    "        self.documents = documents\n",
    "        self.BM_25 = BM_25\n",
    "        self.k = k\n",
    "        self.b = b\n",
    "        self.TFIDF, self.inverted_index, self.doc_tfidf_norms = self.create_inverted_index()\n",
    "        \n",
    "    def create_inverted_index(self):\n",
    "        N = len(self.documents)\n",
    "        TFIDF = defaultdict(float)\n",
    "        inverted_index = defaultdict(list)\n",
    "\n",
    "        # compute term frequency and document frequencies\n",
    "        if self.BM_25:\n",
    "            TF, term_docs = self.compute_TF_weighted()\n",
    "        else:    \n",
    "            TF, term_docs = self.compute_TF()\n",
    "\n",
    "        # create inverted index\n",
    "        print(f\"Computing TFIDF and creating inverted index...\")\n",
    "        for w, docs in tqdm(term_docs.items(), total=len(term_docs)):\n",
    "            for d in sorted(list(docs)):\n",
    "                tfidf = TF[(w,d)] * math.log10(N/len(docs))\n",
    "                inverted_index[w].append(d)\n",
    "                TFIDF[(w,d)] = tfidf\n",
    "\n",
    "        # compute document TFIDF vector norms\n",
    "        print(f\"Computing TFIDF vector norms...\")\n",
    "        doc_tfidf_norms = [0] * N\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = tokenize(doc)\n",
    "            for w in words:\n",
    "                doc_tfidf_norms[d] = doc_tfidf_norms[d] +  TFIDF[(w,d)]**2\n",
    "            doc_tfidf_norms[d] = math.sqrt(doc_tfidf_norms[d])\n",
    "\n",
    "        return TFIDF, inverted_index, doc_tfidf_norms  \n",
    "      \n",
    "    # regular TF\n",
    "    def compute_TF(self):\n",
    "        TF = defaultdict(int)\n",
    "        term_docs = defaultdict(set)\n",
    "        print(f\"Computing TFIDF...\")\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = tokenize(doc)\n",
    "            for w in words:\n",
    "                TF[(w, d)] += 1\n",
    "                term_docs[w].add(d)\n",
    "        # apply log\n",
    "        for (w,d), tf in TF.items():\n",
    "            TF[(w,d)] = 1 + math.log10(tf) \n",
    "        return TF, term_docs\n",
    "    \n",
    "    # weighted TF for BM25\n",
    "    def compute_TF_weighted(self):\n",
    "        TF = defaultdict(int)\n",
    "        term_docs = defaultdict(set)\n",
    "        doc_length = defaultdict(float)\n",
    "        Dtotal = 0\n",
    "        print(f\"Computing TFIDF...\")\n",
    "        for d, doc in tqdm(enumerate(self.documents), total=len(self.documents)):\n",
    "            words = tokenize(doc)\n",
    "            for w in words:\n",
    "                TF[(w, d)] += 1\n",
    "                term_docs[w].add(d)\n",
    "            doc_length[d] = len(words)\n",
    "            Dtotal += len(words)\n",
    "        Davg = Dtotal / len(self.documents)\n",
    "\n",
    "        # compute BM25 weighted term frequencies\n",
    "        TF_weighted = defaultdict(float)\n",
    "        for (w,d), tf in TF.items():\n",
    "            TF_weighted[(w,d)] = (tf * (self.k + 1)) / (tf + self.k * (1 - self.b + self.b * (doc_length[d]/Davg)))\n",
    "        return TF_weighted, term_docs\n",
    "    \n",
    "\n",
    "    def retrieve_docs(self, query, topk=1):\n",
    "        query_words = tokenizer.tokenize(query.lower())\n",
    "        #print(f\"query words: {query_words}\")\n",
    "        # get all documents which contain words from query\n",
    "        docs = []\n",
    "        for w in query_words:\n",
    "            docs.extend([d for d in self.inverted_index[w]])\n",
    "        #print(f\"docs: \")    \n",
    "        # score all these documents\n",
    "        scores = defaultdict(float)\n",
    "        for d in docs:\n",
    "            for w in query_words:\n",
    "                scores[d] += self.TFIDF[(w,d)]\n",
    "            scores[d] = scores[d] / self.doc_tfidf_norms[d]        \n",
    "        #print(f\"scores: {scores}\")    \n",
    "        # return topk documents\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        #print(f\"sorted scores: {sorted_scores}\")\n",
    "        best = sorted_scores[:topk]\n",
    "        topk_docs = []\n",
    "        for doc, score in best:\n",
    "            topk_docs.append((self.documents[doc], score))\n",
    "        return topk_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 35696.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF and creating inverted index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 660746.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF vector norms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 57985.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'to': [0, 1], 'be': [0, 1], 'brief': [0], 'i': [0, 1], 'write': [0], 'for': [0, 4], 'various': [0], 'reasons': [0], 'will': [1], 'confess': [1], 'that': [1], 'have': [1], 'a': [1, 4], 'fancy': [1], 'numbered': [1], 'among': [1], 'their': [1], 'honourable': [1], 'company': [1], 'sir': [2], 'henry': [2], 'curtis': [2], 'as': [2], 'everybody': [2, 3], 'acquainted': [2], 'with': [2], 'him': [2], 'knows': [2], 'is': [2], 'one': [2], 'of': [2, 5], 'the': [2, 3, 4, 5], 'most': [2, 4], 'hospitable': [2], 'men': [2], 'on': [2], 'earth': [2, 5], 'turned': [3, 5], 'and': [3, 4, 5], 'stared': [3], 'politely': [3], 'at': [3, 5], 'curious': [3], 'looking': [3], 'little': [3], 'lame': [3], 'man': [3], 'though': [3], 'his': [3], 'size': [3], 'was': [3, 4], 'insignificant': [3], 'he': [3], 'quite': [3], 'worth': [3], 'staring': [3], 'once': [4], 'it': [4], 'dense': [4], 'forest': [4], 'now': [4], 's': [4], 'open': [4], 'level': [4], 'country': [4], 'cultivated': [4], 'here': [4], 'there': [4], 'but': [4], 'part': [4], 'barren': [4], 'christian': [5], 'number': [5], 'casualties': [5], 'from': [5], 'sickness': [5], 'has': [5], 'been': [5], 'very': [5], 'small': [5], 'indeed': [5], 'this': [5], 'although': [5], 'they': [5], 'frequently': [5], 'sleep': [5], 'in': [5], 'trenches': [5], 'newly': [5], 'all': [5], 'seasons': [5], 'year': [5]})\n",
      "[2.54655801449643, 3.1199322759948185, 3.093863797007719, 3.1399022169809108, 3.304396489602076, 3.1684998761575063]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "IR = IR_System(test_documents, BM_25=True)\n",
    "#for w, docs in IR.inverted_index.items():\n",
    "#    print(f\"{w}: {[(d, IR.TFIDF[(w,d)]) for d in docs]}\")  \n",
    "print(IR.inverted_index)      \n",
    "print(IR.doc_tfidf_norms)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Once it was a dense forest, now it's open level country cultivated here and there, but for the most part barren.\",\n",
       "  0.5806318569148247),\n",
       " ('I will confess that I have a fancy to be numbered among their honourable company.',\n",
       "  0.28126663765766186)]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IR.retrieve_docs(query=\"open country fancy\", topk=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will train our IR system on the SQuAD v1.0 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "squad_train_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"  \n",
    "squad_dev_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\" \n",
    "\n",
    "# Cache the train JSON document to a file\n",
    "response_train = requests.get(squad_train_url)\n",
    "with open(\"train.json\", \"w\") as train_file:\n",
    "    json.dump(response_train.json(), train_file)\n",
    "\n",
    "# Cache the dev JSON document to a file\n",
    "response_dev = requests.get(squad_dev_url)\n",
    "with open(\"dev.json\", \"w\") as dev_file:\n",
    "    json.dump(response_dev.json(), dev_file)\n",
    "\"\"\"\n",
    "\n",
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQuAD v1.0 data json file structure:\n",
    "\n",
    "    data: This is a list where each element represents a topic. Each topic is a dictionary with the following keys:\n",
    "    \n",
    "        title: The title of the topic (usually the title of the Wikipedia article).\n",
    "    \n",
    "        paragraphs: This is a list where each element represents a context passage from the topic. Each passage is a dictionary with the following keys:\n",
    "    \n",
    "            context: The context passage text.\n",
    "        \n",
    "            qas: This is a list where each element represents a question and its answer(s). Each question-answer pair is a dictionary with the following keys:\n",
    "        \n",
    "                answers: This is a list where each element represents an answer to the question. Each answer is a dictionary with the following keys:\n",
    "        \n",
    "                    answer_start: The character index in the context passage where the answer starts.\n",
    "        \n",
    "                    text: The text of the answer.\n",
    "        \n",
    "                question: The question text.\n",
    "        \n",
    "                id: A unique identifier for the question.\n",
    "\n",
    "    version: The version of the SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qas': [{'question': 'When did Beyonce start becoming popular?',\n",
       "   'id': '56be85543aeaaa14008c9063',\n",
       "   'answers': [{'text': 'in the late 1990s', 'answer_start': 269}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What areas did Beyonce compete in when she was growing up?',\n",
       "   'id': '56be85543aeaaa14008c9065',\n",
       "   'answers': [{'text': 'singing and dancing', 'answer_start': 207}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
       "   'id': '56be85543aeaaa14008c9066',\n",
       "   'answers': [{'text': '2003', 'answer_start': 526}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In what city and state did Beyonce  grow up? ',\n",
       "   'id': '56bf6b0f3aeaaa14008c9601',\n",
       "   'answers': [{'text': 'Houston, Texas', 'answer_start': 166}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In which decade did Beyonce become famous?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9602',\n",
       "   'answers': [{'text': 'late 1990s', 'answer_start': 276}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'In what R&B group was she the lead singer?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9603',\n",
       "   'answers': [{'text': \"Destiny's Child\", 'answer_start': 320}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What album made her a worldwide known artist?',\n",
       "   'id': '56bf6b0f3aeaaa14008c9604',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"Who managed the Destiny's Child group?\",\n",
       "   'id': '56bf6b0f3aeaaa14008c9605',\n",
       "   'answers': [{'text': 'Mathew Knowles', 'answer_start': 360}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When did Beyoncé rise to fame?',\n",
       "   'id': '56d43c5f2ccc5a1400d830a9',\n",
       "   'answers': [{'text': 'late 1990s', 'answer_start': 276}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What role did Beyoncé have in Destiny's Child?\",\n",
       "   'id': '56d43c5f2ccc5a1400d830aa',\n",
       "   'answers': [{'text': 'lead singer', 'answer_start': 290}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'What was the first album Beyoncé released as a solo artist?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ab',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'When did Beyoncé release Dangerously in Love?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ac',\n",
       "   'answers': [{'text': '2003', 'answer_start': 526}],\n",
       "   'is_impossible': False},\n",
       "  {'question': 'How many Grammy awards did Beyoncé win for her first solo album?',\n",
       "   'id': '56d43c5f2ccc5a1400d830ad',\n",
       "   'answers': [{'text': 'five', 'answer_start': 590}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What was Beyoncé's role in Destiny's Child?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b4',\n",
       "   'answers': [{'text': 'lead singer', 'answer_start': 290}],\n",
       "   'is_impossible': False},\n",
       "  {'question': \"What was the name of Beyoncé's first solo album?\",\n",
       "   'id': '56d43ce42ccc5a1400d830b5',\n",
       "   'answers': [{'text': 'Dangerously in Love', 'answer_start': 505}],\n",
       "   'is_impossible': False}],\n",
       " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get some passages for out training data. This is a large dataset, so we won't use every single passage. We will only draw from a subset of titles.\n",
    "\n",
    "Note that we also append the title to each passage and question. This inevitably makes the retrieval task eaier, but some questions in the dataset sometimes refer to persons using their pronouns. Without the actual name of the entity present in the query can be problematic, so we simplify things a bit by appending the title to each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of titles: 442\n",
      "Title# 0: Beyoncé, Number of passages: 66\n",
      "Title# 1: Frédéric_Chopin, Number of passages: 82\n",
      "Title# 2: Sino-Tibetan_relations_during_the_Ming_dynasty, Number of passages: 72\n",
      "Title# 3: IPod, Number of passages: 60\n",
      "Title# 4: The_Legend_of_Zelda:_Twilight_Princess, Number of passages: 32\n",
      "Title# 5: Spectre_(2015_film), Number of passages: 43\n",
      "Title# 6: 2008_Sichuan_earthquake, Number of passages: 77\n",
      "Title# 7: New_York_City, Number of passages: 148\n",
      "Title# 8: To_Kill_a_Mockingbird, Number of passages: 62\n",
      "Title# 9: Solar_energy, Number of passages: 52\n",
      "Title# 10: Kanye_West, Number of passages: 79\n",
      "Title# 11: Buddhism, Number of passages: 149\n",
      "Title# 12: American_Idol, Number of passages: 127\n",
      "Title# 13: Dog, Number of passages: 75\n",
      "Title# 14: 2008_Summer_Olympics_torch_relay, Number of passages: 74\n",
      "Title# 15: Genome, Number of passages: 25\n",
      "Title# 16: Comprehensive_school, Number of passages: 25\n",
      "Title# 17: Republic_of_the_Congo, Number of passages: 39\n",
      "Title# 18: Prime_minister, Number of passages: 36\n",
      "Title# 19: Institute_of_technology, Number of passages: 77\n",
      "Title# 20: Wayback_Machine, Number of passages: 26\n",
      "Title# 21: Dutch_Republic, Number of passages: 21\n",
      "Title# 22: Symbiosis, Number of passages: 23\n",
      "Title# 23: Canadian_Armed_Forces, Number of passages: 46\n",
      "Title# 24: Cardinal_(Catholicism), Number of passages: 45\n",
      "Title# 25: Iranian_languages, Number of passages: 21\n",
      "Title# 26: Lighting, Number of passages: 57\n",
      "Title# 27: Separation_of_powers_under_the_United_States_Constitution, Number of passages: 29\n",
      "Title# 28: Architecture, Number of passages: 38\n",
      "Title# 29: Human_Development_Index, Number of passages: 38\n",
      "Title# 30: Southern_Europe, Number of passages: 25\n",
      "Title# 31: BBC_Television, Number of passages: 31\n",
      "Title# 32: Arnold_Schwarzenegger, Number of passages: 86\n",
      "Title# 33: Plymouth, Number of passages: 81\n",
      "Title# 34: Heresy, Number of passages: 29\n",
      "Title# 35: Warsaw_Pact, Number of passages: 23\n",
      "Title# 36: Materialism, Number of passages: 33\n",
      "Title# 37: Christian, Number of passages: 28\n",
      "Title# 38: Sony_Music_Entertainment, Number of passages: 35\n",
      "Title# 39: Oklahoma_City, Number of passages: 82\n",
      "Title# 40: Hunter-gatherer, Number of passages: 34\n",
      "Title# 41: United_Nations_Population_Fund, Number of passages: 22\n",
      "Title# 42: Russian_Soviet_Federative_Socialist_Republic, Number of passages: 34\n",
      "Title# 43: Alexander_Graham_Bell, Number of passages: 74\n",
      "Title# 44: Pub, Number of passages: 90\n",
      "Title# 45: Internet_service_provider, Number of passages: 21\n",
      "Title# 46: Comics, Number of passages: 37\n",
      "Title# 47: Saint_Helena, Number of passages: 82\n",
      "Title# 48: Aspirated_consonant, Number of passages: 34\n",
      "Title# 49: Hydrogen, Number of passages: 60\n",
      "Title# 50: Space_Race, Number of passages: 70\n",
      "Title# 51: Web_browser, Number of passages: 24\n",
      "Title# 52: BeiDou_Navigation_Satellite_System, Number of passages: 34\n",
      "Title# 53: Canon_law, Number of passages: 22\n",
      "Title# 54: Communications_in_Somalia, Number of passages: 25\n",
      "Title# 55: Catalan_language, Number of passages: 80\n",
      "Title# 56: Boston, Number of passages: 79\n",
      "Title# 57: Universal_Studios, Number of passages: 42\n",
      "Title# 58: Estonian_language, Number of passages: 33\n",
      "Title# 59: Paper, Number of passages: 32\n",
      "Title# 60: Adult_contemporary_music, Number of passages: 53\n",
      "Title# 61: Daylight_saving_time, Number of passages: 63\n",
      "Title# 62: Royal_Institute_of_British_Architects, Number of passages: 27\n",
      "Title# 63: National_Archives_and_Records_Administration, Number of passages: 28\n",
      "Title# 64: Tristan_da_Cunha, Number of passages: 29\n",
      "Title# 65: University_of_Kansas, Number of passages: 36\n",
      "Title# 66: Nanjing, Number of passages: 66\n",
      "Title# 67: Arena_Football_League, Number of passages: 55\n",
      "Title# 68: Dialect, Number of passages: 44\n",
      "Title# 69: Bern, Number of passages: 52\n",
      "Title# 70: Westminster_Abbey, Number of passages: 46\n",
      "Title# 71: Political_corruption, Number of passages: 57\n",
      "Title# 72: Classical_music, Number of passages: 68\n",
      "Title# 73: Slavs, Number of passages: 49\n",
      "Title# 74: Southampton, Number of passages: 95\n",
      "Title# 75: Treaty, Number of passages: 57\n",
      "Title# 76: Josip_Broz_Tito, Number of passages: 67\n",
      "Title# 77: Marshall_Islands, Number of passages: 51\n",
      "Title# 78: Szlachta, Number of passages: 63\n",
      "Title# 79: Virgil, Number of passages: 23\n",
      "Title# 80: Alps, Number of passages: 87\n",
      "Title# 81: Gene, Number of passages: 56\n",
      "Title# 82: Guinea-Bissau, Number of passages: 50\n",
      "Title# 83: List_of_numbered_streets_in_Manhattan, Number of passages: 50\n",
      "Title# 84: Brain, Number of passages: 64\n",
      "Title# 85: Near_East, Number of passages: 59\n",
      "Title# 86: Zhejiang, Number of passages: 38\n",
      "Title# 87: Ministry_of_Defence_(United_Kingdom), Number of passages: 25\n",
      "Title# 88: High-definition_television, Number of passages: 46\n",
      "Title# 89: Wood, Number of passages: 70\n",
      "Title# 90: Somalis, Number of passages: 71\n",
      "Title# 91: Middle_Ages, Number of passages: 93\n",
      "Title# 92: Phonology, Number of passages: 23\n",
      "Title# 93: Computer, Number of passages: 80\n",
      "Title# 94: Black_people, Number of passages: 61\n",
      "Title# 95: The_Times, Number of passages: 53\n",
      "Title# 96: New_Delhi, Number of passages: 52\n",
      "Title# 97: Bird_migration, Number of passages: 56\n",
      "Title# 98: Atlantic_City,_New_Jersey, Number of passages: 47\n",
      "Title# 99: Immunology, Number of passages: 18\n",
      "Title# 100: MP3, Number of passages: 62\n",
      "Title# 101: House_music, Number of passages: 44\n",
      "Title# 102: Letter_case, Number of passages: 12\n",
      "Title# 103: Chihuahua_(state), Number of passages: 66\n",
      "Title# 104: Imamah_(Shia_doctrine), Number of passages: 14\n",
      "Title# 105: Pitch_(music), Number of passages: 10\n",
      "Title# 106: England_national_football_team, Number of passages: 12\n",
      "Title# 107: Houston, Number of passages: 48\n",
      "Title# 108: Copper, Number of passages: 31\n",
      "Title# 109: Identity_(social_science), Number of passages: 21\n",
      "Title# 110: Himachal_Pradesh, Number of passages: 23\n",
      "Title# 111: Communication, Number of passages: 12\n",
      "Title# 112: Grape, Number of passages: 10\n",
      "Title# 113: Computer_security, Number of passages: 25\n",
      "Title# 114: Orthodox_Judaism, Number of passages: 25\n",
      "Title# 115: Animal, Number of passages: 17\n",
      "Title# 116: Beer, Number of passages: 34\n",
      "Title# 117: Race_and_ethnicity_in_the_United_States_Census, Number of passages: 12\n",
      "Title# 118: United_States_dollar, Number of passages: 32\n",
      "Title# 119: Imperial_College_London, Number of passages: 25\n",
      "Title# 120: Hanover, Number of passages: 21\n",
      "Title# 121: Emotion, Number of passages: 44\n",
      "Title# 122: Everton_F.C., Number of passages: 24\n",
      "Title# 123: Old_English, Number of passages: 23\n",
      "Title# 124: Aircraft_carrier, Number of passages: 34\n",
      "Title# 125: Federal_Aviation_Administration, Number of passages: 12\n",
      "Title# 126: Lancashire, Number of passages: 20\n",
      "Title# 127: Mesozoic, Number of passages: 16\n",
      "Title# 128: Videoconferencing, Number of passages: 21\n",
      "Title# 129: Gregorian_calendar, Number of passages: 23\n",
      "Title# 130: Xbox_360, Number of passages: 26\n",
      "Title# 131: Military_history_of_the_United_States, Number of passages: 42\n",
      "Title# 132: Hard_rock, Number of passages: 25\n",
      "Title# 133: Great_Plains, Number of passages: 13\n",
      "Title# 134: Infrared, Number of passages: 20\n",
      "Title# 135: Biodiversity, Number of passages: 32\n",
      "Title# 136: ASCII, Number of passages: 26\n",
      "Title# 137: Digestion, Number of passages: 18\n",
      "Title# 138: Gymnastics, Number of passages: 23\n",
      "Title# 139: FC_Barcelona, Number of passages: 50\n",
      "Title# 140: Federal_Bureau_of_Investigation, Number of passages: 45\n",
      "Title# 141: Mary_(mother_of_Jesus), Number of passages: 41\n",
      "Title# 142: Melbourne, Number of passages: 64\n",
      "Title# 143: John,_King_of_England, Number of passages: 76\n",
      "Title# 144: Macintosh, Number of passages: 49\n",
      "Title# 145: Anti-aircraft_warfare, Number of passages: 70\n",
      "Title# 146: Sanskrit, Number of passages: 22\n",
      "Title# 147: Valencia, Number of passages: 51\n",
      "Title# 148: General_Electric, Number of passages: 16\n",
      "Title# 149: United_States_Army, Number of passages: 34\n",
      "Title# 150: Franco-Prussian_War, Number of passages: 58\n",
      "Title# 151: Adolescence, Number of passages: 78\n",
      "Title# 152: Antarctica, Number of passages: 44\n",
      "Title# 153: Eritrea, Number of passages: 40\n",
      "Title# 154: Uranium, Number of passages: 41\n",
      "Title# 155: Order_of_the_British_Empire, Number of passages: 13\n",
      "Title# 156: Circadian_rhythm, Number of passages: 22\n",
      "Title# 157: Elizabeth_II, Number of passages: 45\n",
      "Title# 158: Sexual_orientation, Number of passages: 53\n",
      "Title# 159: Dell, Number of passages: 45\n",
      "Title# 160: Capital_punishment_in_the_United_States, Number of passages: 47\n",
      "Title# 161: Age_of_Enlightenment, Number of passages: 83\n",
      "Title# 162: Nintendo_Entertainment_System, Number of passages: 44\n",
      "Title# 163: Athanasius_of_Alexandria, Number of passages: 36\n",
      "Title# 164: Seattle, Number of passages: 52\n",
      "Title# 165: Memory, Number of passages: 32\n",
      "Title# 166: Multiracial_American, Number of passages: 44\n",
      "Title# 167: Ashkenazi_Jews, Number of passages: 50\n",
      "Title# 168: Pharmaceutical_industry, Number of passages: 44\n",
      "Title# 169: Umayyad_Caliphate, Number of passages: 35\n",
      "Title# 170: Asphalt, Number of passages: 36\n",
      "Title# 171: Queen_Victoria, Number of passages: 42\n",
      "Title# 172: Freemasonry, Number of passages: 37\n",
      "Title# 173: Israel, Number of passages: 99\n",
      "Title# 174: Hellenistic_period, Number of passages: 94\n",
      "Title# 175: Bill_%26_Melinda_Gates_Foundation, Number of passages: 21\n",
      "Title# 176: Montevideo, Number of passages: 70\n",
      "Title# 177: Poultry, Number of passages: 31\n",
      "Title# 178: Dutch_language, Number of passages: 51\n",
      "Title# 179: Buckingham_Palace, Number of passages: 28\n",
      "Title# 180: Incandescent_light_bulb, Number of passages: 42\n",
      "Title# 181: Arsenal_F.C., Number of passages: 34\n",
      "Title# 182: Clothing, Number of passages: 21\n",
      "Title# 183: Chicago_Cubs, Number of passages: 63\n",
      "Title# 184: Korean_War, Number of passages: 75\n",
      "Title# 185: Copyright_infringement, Number of passages: 39\n",
      "Title# 186: Greece, Number of passages: 95\n",
      "Title# 187: Royal_Dutch_Shell, Number of passages: 25\n",
      "Title# 188: Mammal, Number of passages: 24\n",
      "Title# 189: East_India_Company, Number of passages: 31\n",
      "Title# 190: Hokkien, Number of passages: 21\n",
      "Title# 191: Professional_wrestling, Number of passages: 61\n",
      "Title# 192: Film_speed, Number of passages: 26\n",
      "Title# 193: Mexico_City, Number of passages: 87\n",
      "Title# 194: Napoleon, Number of passages: 89\n",
      "Title# 195: Germans, Number of passages: 32\n",
      "Title# 196: Southeast_Asia, Number of passages: 26\n",
      "Title# 197: Brigham_Young_University, Number of passages: 37\n",
      "Title# 198: Department_store, Number of passages: 40\n",
      "Title# 199: Intellectual_property, Number of passages: 21\n",
      "Number of passages: 9074\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of titles: {len(squad_train['data'])}\")\n",
    "\n",
    "num_titles = 200\n",
    "\n",
    "# for each title, get passages and all corresponding questions from SQuAD train set\n",
    "passages = []\n",
    "questions = []\n",
    "for i in range(num_titles):\n",
    "    print(f\"Title# {i}: {squad_train['data'][i]['title']}, Number of passages: {len(squad_train['data'][i]['paragraphs'])}\")\n",
    "    for p in squad_train['data'][i]['paragraphs']:\n",
    "        # we will append the title to each passage and question\n",
    "        passages.append(squad_train['data'][i]['title'] + \": \" + p['context'])\n",
    "        qs = []\n",
    "        for q in p['qas']:\n",
    "            q_appended = q\n",
    "            q_appended['question'] = squad_train['data'][i]['title'] + \": \" + q['question']\n",
    "            qs.append(q_appended)\n",
    "        questions.append(qs)       \n",
    "\n",
    "print(f\"Number of passages: {len(passages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9074/9074 [00:00<00:00, 13771.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF and creating inverted index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49432/49432 [00:00<00:00, 61837.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing TFIDF vector norms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9074/9074 [00:00<00:00, 13052.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# lets train our IR system on the SqUAD train set passages\n",
    "IR = IR_System(passages, BM_25=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  In what year was the iPod first introduced?, GOLD DOCUMENT: IPod: Though the iPod was released in 2001, its price and Mac-only compatibility caused sales to be relatively slow until 2004. The iPod line came from Apple's \"digital hub\" category, when the company began creating software for the growing market of personal digital devices. Digital cameras, camcorders and organizers had well-established mainstream markets, but the company found existing digital music players \"big and clunky or small and useless\" with user interfaces that were \"unbelievably awful,\" so Apple decided to develop its own. As ordered by CEO Steve Jobs, Apple's hardware engineering chief Jon Rubinstein assembled a team of engineers to design the iPod line, including hardware engineers Tony Fadell and Michael Dhuey, and design engineer Sir Jonathan Ive. Rubinstein had already discovered the Toshiba disk drive when meeting with an Apple supplier in Japan, and purchased the rights to it for Apple, and had also already worked out how the screen, battery, and other key elements would work. The aesthetic was inspired by the 1958 Braun T3 transistor radio designed by Dieter Rams, while the wheel based user interface was prompted by Bang & Olufsen's BeoCom 6000 telephone. The product (\"the Walkman of the twenty-first century\" ) was developed in less than one year and unveiled on October 23, 2001. Jobs announced it as a Mac-compatible product with a 5 GB hard drive that put \"1,000 songs in your pocket.\"\n",
      "SCORE: 0.3470460158861455, DOCUMENT: IPod: Originally, a FireWire connection to the host computer was used to update songs or recharge the battery. The battery could also be charged with a power adapter that was included with the first four generations.\n",
      "SCORE: 0.30568527248086524, DOCUMENT: IPod: On October 21, 2008, Apple reported that only 14.21% of total revenue for fiscal quarter 4 of year 2008 came from iPods. At the September 9, 2009 keynote presentation at the Apple Event, Phil Schiller announced total cumulative sales of iPods exceeded 220 million. The continual decline of iPod sales since 2009 has not been a surprising trend for the Apple corporation, as Apple CFO Peter Oppenheimer explained in June 2009: \"We expect our traditional MP3 players to decline over time as we cannibalize ourselves with the iPod Touch and the iPhone.\" Since 2009, the company's iPod sales have continually decreased every financial quarter and in 2013 a new model was not introduced onto the market.\n",
      "SCORE: 0.30328561993966097, DOCUMENT: IPod: The iTunes Store (introduced April 29, 2003) is an online media store run by Apple and accessed through iTunes. The store became the market leader soon after its launch and Apple announced the sale of videos through the store on October 12, 2005. Full-length movies became available on September 12, 2006.\n",
      "SCORE: 0.29858085282509006, DOCUMENT: IPod: Since October 2004, the iPod line has dominated digital music player sales in the United States, with over 90% of the market for hard drive-based players and over 70% of the market for all types of players. During the year from January 2004 to January 2005, the high rate of sales caused its U.S. market share to increase from 31% to 65% and in July 2005, this market share was measured at 74%. In January 2007 the iPod market share reached 72.7% according to Bloomberg Online.\n",
      "SCORE: 0.29473686194241927, DOCUMENT: IPod: In mid-2015, several new color schemes for all of the current iPod models were spotted in the latest version of iTunes, 12.2. Belgian website Belgium iPhone originally found the images when plugging in an iPod for the first time, and subsequent leaked photos were found by Pierre Dandumont.\n"
     ]
    }
   ],
   "source": [
    "# now let's get a query to test out our IR system\n",
    "passage_idx = 225 \n",
    "query = questions[passage_idx][0]['question']\n",
    "query = \"\".join(query.split(\":\")[1:])\n",
    "print(f\"QUERY: {query}, GOLD DOCUMENT: {passages[passage_idx]}\")\n",
    "\n",
    "best_docs = IR.retrieve_docs(query, topk=5)\n",
    "for doc, score in best_docs:\n",
    "    print(f\"SCORE: {score}, DOCUMENT: {doc}\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's measure the Top-5 accuracy of our trained IR system across all passages used in training and all corresponding questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(IR, passages, questions, topk=5, keeptitle=True, num_passages=1000):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    num_passages = min(num_passages, len(passages))\n",
    "    with tqdm(total=num_passages) as pbar:\n",
    "        for i in range(num_passages):\n",
    "            for q in questions[i]:\n",
    "                # only use questions which have an answer\n",
    "                if not q['is_impossible']:\n",
    "                    query = q['question']\n",
    "                    if not keeptitle:\n",
    "                        query = \" \".join(query.split(\":\")[1:])\n",
    "                    best_docs = IR.retrieve_docs(query, topk=topk)\n",
    "                    for doc, score in best_docs:\n",
    "                        if doc == passages[i]:\n",
    "                            num_correct += 1\n",
    "                            break\n",
    "                    num_total += 1\n",
    "                    accuracy = num_correct / num_total\n",
    "                    pbar.set_postfix(accuracy=accuracy)\n",
    "            pbar.update(1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:19<00:00,  1.79it/s, accuracy=0.781]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR system accuracy: 0.7813238770685579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(IR, passages, questions)\n",
    "print(f\"IR system accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with regular TFIDF on first 1000 passages is about 78%. Now let's try BM25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/1000 [00:27<16:47,  1.03s/it, accuracy=0.735]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIR system accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[153], line 13\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(IR, passages, questions, topk, keeptitle, num_passages)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keeptitle:\n\u001b[1;32m     12\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(query\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m---> 13\u001b[0m best_docs \u001b[38;5;241m=\u001b[39m \u001b[43mIR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m best_docs:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;241m==\u001b[39m passages[i]:\n",
      "Cell \u001b[0;32mIn[155], line 87\u001b[0m, in \u001b[0;36mIR_System.retrieve_docs\u001b[0;34m(self, query, topk)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m query_words:\n\u001b[0;32m---> 87\u001b[0m         scores[d] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTFIDF[(w,d)]\n\u001b[1;32m     88\u001b[0m     scores[d] \u001b[38;5;241m=\u001b[39m scores[d] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_tfidf_norms[d]        \n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#print(f\"scores: {scores}\")    \u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# return topk documents\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accuracy = compute_accuracy(IR, passages, questions)\n",
    "print(f\"IR system accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
