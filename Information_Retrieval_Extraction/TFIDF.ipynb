{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF Passage Retrieval\n",
    "\n",
    "A simple and efficient way of retreiving relevant documents (e.g. sentence or paragraph) from a document store is using `TFIDF`. Given a collection of documents, we can create TFIDF vectors for each document, which is a vector of TFIDF weights (one for each word from the vocabulary). \n",
    "\n",
    "Term-frequency for term $t$ in document $d$: \n",
    "\n",
    "$TF(t,d) = 1 + \\log_{10}(\\text{count}(t,d))$\n",
    "\n",
    "where $\\text{count}(t,d)$ is the frequency with which term $t$ appears in document $d$. (Note that we take the log to supress the range of count values).\n",
    "\n",
    "Inverse-document-frequency for term $t$:\n",
    "\n",
    "$IDF(t) = \\log_{10}(\\frac{N}{\\text{df}(t)})$\n",
    "\n",
    "where $N$ is the total number of documents in the collectino and $\\text{df}(t)$ is the number of documents in which term $t$ occurs. Then the TFIDF is given by the following product:\n",
    "\n",
    "$TFIDF(t,d) = TF(t,d) IDF(t)$\n",
    "\n",
    "\n",
    "Then given a query $q$ which is a sequence of terms, we can construct a TFIDF vector for this query. However, since queries are usually short and is likely to contain a single occurance of each unique term, we can simplify it's TFIDF vector by setting the TFIDF weight for each unique term to 1. Then we can compute a score for each document as the cosine similarity between the query vector and the corresponding document TFIDF vector $d$:\n",
    "\n",
    "$score(q,d) = \\frac{q \\cdot d}{|q| |d|}$\n",
    "\n",
    "Since $|q|$ is a fixed constant, we can ignore it because it will not affect the ranking of document scores. Then using our simplifying assumption of $q$ being a vector of binary weights, we have the following document score function:\n",
    "\n",
    "$score(q,d) = \\frac{\\sum_{t\\in q} TFIDF(t,d)}{\\sqrt{\\sum_{t\\in d} TFIDF(t,d)^2}}$\n",
    "\n",
    "where the square root term in the denominator is the norm of the document TFIDF vector. So the score for each document is just the TFIDF weights for the query terms which also appear in that document, normalized by the norm of that documents TFIDF vector.\n",
    "\n",
    "Now each word in the query will not occur in all documents, so we need to only consider documents that actually contain these query words instead of iterating over all documents in the collection. We can maintain an `inverted index` data structure which is a dictionary that maps each unique word to a list of tuples, each tuple containing a document and the TFIDF weight.\n",
    "\n",
    "e.g. `inverted_index = {'w1' : [(d1, TFIDF(w1,d1)), (d2, TFIDF(w1,d2),..)], 'w2': ...}`\n",
    "\n",
    "This data structure will allow us to compute and rank the document scores very efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test documents\n",
    "test_documents = [\"To be brief, I write for various reasons.\", \n",
    "                  \"I will confess that I have a fancy to be numbered among their honourable company.\",  \n",
    "                  \"Sir Henry Curtis, as everybody acquainted with him knows, is one of the most hospitable men on earth\", \n",
    "                  \"Everybody turned and stared politely at the curious-looking little lame man, and though his size was insignificant, he was quite worth staring at.\",\n",
    "                  \"Once it was a dense forest, now it's open level country cultivated here and there, but for the most part barren.\",\n",
    "                  \"Christian, the number of casualties from sickness has been very small indeed, and this although they frequently sleep in the trenches of newly-turned earth at all seasons of the year.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentence string into words, punctutaions removed\n",
    "def tokenize(sent):\n",
    "    return tokenizer.tokenize(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic TFIDF information rereival system\n",
    "class IR_System():\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.TFIDF, self.inverted_index, self.doc_tfidf_norms = self.create_inverted_index()\n",
    "        \n",
    "\n",
    "    def create_inverted_index(self):\n",
    "        N = len(self.documents)\n",
    "        TF = defaultdict(int)\n",
    "        TFIDF = defaultdict(float)\n",
    "        term_docs = defaultdict(set)\n",
    "        inverted_index = defaultdict(list)\n",
    "\n",
    "        # compute term frequency and document frequencies\n",
    "        for d, doc in enumerate(self.documents):\n",
    "            words = tokenize(doc)\n",
    "            for w in words:\n",
    "                TF[(w, d)] += 1\n",
    "                term_docs[w].add(d)\n",
    "\n",
    "        # create inverted index\n",
    "        for w, docs in term_docs.items():\n",
    "            for d in sorted(list(docs)):\n",
    "                tfidf = (1 + math.log10(TF[(w,d)])) * math.log10(N/len(docs))\n",
    "                inverted_index[w].append(d)\n",
    "                TFIDF[(w,d)] = tfidf\n",
    "\n",
    "        # compute document TFIDF vector norms\n",
    "        doc_tfidf_norms = [0] * N\n",
    "        for d, doc in enumerate(self.documents):\n",
    "            words = tokenize(doc)\n",
    "            for w in words:\n",
    "                doc_tfidf_norms[d] = doc_tfidf_norms[d] +  TFIDF[(w,d)]**2\n",
    "            doc_tfidf_norms[d] = math.sqrt(doc_tfidf_norms[d])\n",
    "\n",
    "        return TFIDF, inverted_index, doc_tfidf_norms  \n",
    "\n",
    "\n",
    "    def retrieve_docs(self, query, topk=1):\n",
    "        query_words = tokenizer.tokenize(query.lower())\n",
    "        #print(f\"query words: {query_words}\")\n",
    "        # get all documents which contain words from query\n",
    "        docs = []\n",
    "        for w in query_words:\n",
    "            docs.extend([d for d in self.inverted_index[w]])\n",
    "        #print(f\"docs: \")    \n",
    "        # score all these documents\n",
    "        scores = defaultdict(float)\n",
    "        for d in docs:\n",
    "            for w in query_words:\n",
    "                scores[d] += self.TFIDF[(w,d)]\n",
    "            scores[d] = scores[d] / self.doc_tfidf_norms[d]        \n",
    "        #print(f\"scores: {scores}\")    \n",
    "        # return topk documents\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        #print(f\"sorted scores: {sorted_scores}\")\n",
    "        best = sorted_scores[:topk]\n",
    "        topk_docs = []\n",
    "        for doc, score in best:\n",
    "            topk_docs.append((self.documents[doc], score))\n",
    "        return topk_docs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'to': [0, 1], 'be': [0, 1], 'brief': [0], 'i': [0, 1], 'write': [0], 'for': [0, 4], 'various': [0], 'reasons': [0], 'will': [1], 'confess': [1], 'that': [1], 'have': [1], 'a': [1, 4], 'fancy': [1], 'numbered': [1], 'among': [1], 'their': [1], 'honourable': [1], 'company': [1], 'sir': [2], 'henry': [2], 'curtis': [2], 'as': [2], 'everybody': [2, 3], 'acquainted': [2], 'with': [2], 'him': [2], 'knows': [2], 'is': [2], 'one': [2], 'of': [2, 5], 'the': [2, 3, 4, 5], 'most': [2, 4], 'hospitable': [2], 'men': [2], 'on': [2], 'earth': [2, 5], 'turned': [3, 5], 'and': [3, 4, 5], 'stared': [3], 'politely': [3], 'at': [3, 5], 'curious': [3], 'looking': [3], 'little': [3], 'lame': [3], 'man': [3], 'though': [3], 'his': [3], 'size': [3], 'was': [3, 4], 'insignificant': [3], 'he': [3], 'quite': [3], 'worth': [3], 'staring': [3], 'once': [4], 'it': [4], 'dense': [4], 'forest': [4], 'now': [4], 's': [4], 'open': [4], 'level': [4], 'country': [4], 'cultivated': [4], 'here': [4], 'there': [4], 'but': [4], 'part': [4], 'barren': [4], 'christian': [5], 'number': [5], 'casualties': [5], 'from': [5], 'sickness': [5], 'has': [5], 'been': [5], 'very': [5], 'small': [5], 'indeed': [5], 'this': [5], 'although': [5], 'they': [5], 'frequently': [5], 'sleep': [5], 'in': [5], 'trenches': [5], 'newly': [5], 'all': [5], 'seasons': [5], 'year': [5]})\n",
      "[1.825556419482995, 2.7402165171139874, 2.9687267790239704, 3.378932317549507, 3.399908383113122, 3.8964745251226014]\n"
     ]
    }
   ],
   "source": [
    "IR = IR_System(test_documents)\n",
    "#for w, docs in IR.inverted_index.items():\n",
    "#    print(f\"{w}: {[(d, IR.TFIDF[(w,d)]) for d in docs]}\")  \n",
    "print(IR.inverted_index)      \n",
    "print(IR.doc_tfidf_norms)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Once it was a dense forest, now it's open level country cultivated here and there, but for the most part barren.\",\n",
       "  0.592383863918572),\n",
       " ('I will confess that I have a fancy to be numbered among their honourable company.',\n",
       "  0.283974366815071)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IR.retrieve_docs(query=\"open country fancy\", topk=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
