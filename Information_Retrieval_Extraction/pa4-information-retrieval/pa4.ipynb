{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Programming Assignment 4: Information Retrieval\n",
    "\n",
    "In this assignment you will be improving upon a rather poorly-made information retrieval system. You will build an inverted index to quickly retrieve documents that match queries and then make it even better by using term-frequency inverse-document-frequency weighting and cosine similarity to compare queries to your data set. Your IR system will be evaluated for accuracy on the correct documents retrieved for different queries and the correctly computed tf-idf values and cosine similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "You will be using your IR system to find relevant documents among a collection of sixty short stories by Rider Haggard. The training data is located in the data/ directory under the subdirectory RiderHaggard/. Within this directory you will see yet another directory raw/. This contains the raw text files of the sixty short stories. The data/ directory also contains the files dev_queries.txt and dev_solutions.txt. We have provided these to you as a set of development queries and their expected answers to use as you begin implementing your IR system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task\n",
    "\n",
    "Improve upon the given IR system by implementing the following features:\n",
    "\n",
    "<b>Inverted Positional Index:</b> Implement an inverted index - a mapping from words to the documents in which they occur, as well as the positions in the documents for which they occur.\n",
    "\n",
    "<b>Boolean Retrieval:</b> Implement a Boolean retrieval system, in which you return the list of documents that contain all words in a query. (Yes, you only need to support conjunctions for this assignment.)\n",
    "\n",
    "<b>Phrase Query Retrieval:</b> Implement a system that returns the list of documents in which the full phrase appears, (ie. the words of the query appear next to each other, in the specified order). Note that at the time of retrieval, you will not have access to the original documents anymore (the documents would be turned into bag of words), so you'll have to utilize your inverted positional index to complete this part.\n",
    "\n",
    "<b>TF-IDF:</b> Compute and store the term-frequency inverse-document-frequency value for every word-document co-occurrence:\n",
    "\n",
    "<b>Cosine Similarity:</b> Implement cosine similarity in order to improve upon the ranked retrieval system, which currently retrieves documents based upon the Jaccard coefficient between the query and each document. Also note that when computing $w_{t, q}$ (i.e. the weight for the word ùë§ in the query) do not include the idf term. That is, $w_{t, q} = 1 + \\log_{10} \\text{tf}_{t, q}$.\n",
    "<b> The reference solution uses ltc.lnn weighting for computing cosine scores. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Your IR system will be evaluated on a development set of queries as well as a held-out set of queries. The queries are encoded in the file <b>dev_queries.txt</b> and are:\n",
    "\n",
    "- separation of church and state\n",
    "- white-robed priests\n",
    "- ancient underground city\n",
    "- native african queen\n",
    "- zulu king\n",
    "\n",
    "We test your system based on the five parts mentioned above: the inverted index (used both to get word positions and to get postings), boolean retrieval, phrase query retrieval, computing the correct tf-idf values, and implementing cosine similarity using the tf-idf values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check\n",
    "\n",
    "Before we do anything else, let's quickly check that you're running the correct\n",
    "version of Python and are in the right environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs124\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell complains, it means that you're using the wrong environment\n",
    "or Python version!\n",
    "\n",
    "If so, please exit this notebook, kill the notebook server with CTRL-C, and\n",
    "try running\n",
    "\n",
    "$ conda activate cs124\n",
    "\n",
    "then restarting your notebook server with\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "If that doesn't work, you should go back and follow the installation\n",
    "instructions in PA0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started!\n",
    "\n",
    "We will first start by importing some modules and setting up our IR system class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "from porter_stemmer import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem:\n",
    "    def __init__(self):\n",
    "        # For holding the data - initialized in read_data()\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        self.vocab = []\n",
    "        # For the text pre-processing.\n",
    "        self.alphanum = re.compile('[^a-zA-Z0-9]')\n",
    "        self.p = PorterStemmer()\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        uniq = set()\n",
    "        for doc in self.docs:\n",
    "            for word in doc:\n",
    "                uniq.add(word)\n",
    "        return uniq\n",
    "\n",
    "    def __read_raw_data(self, dirname):\n",
    "        print(\"Stemming Documents...\")\n",
    "\n",
    "        titles = []\n",
    "        docs = []\n",
    "        os.mkdir('%s/stemmed' % dirname)\n",
    "        title_pattern = re.compile('(.*) \\d+\\.txt')\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/raw' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = title_pattern.search(filename).group(1)\n",
    "            print(\"    Doc %d of %d: %s\" % (i + 1, len(filenames), title))\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/raw/%s' % (dirname, filename), 'r', encoding=\"utf-8\")\n",
    "            of = open('%s/stemmed/%s.txt' % (dirname, title), 'w',\n",
    "                      encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # make sure everything is lower case\n",
    "                line = line.lower()\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # remove non alphanumeric characters\n",
    "                line = [self.alphanum.sub('', xx) for xx in line]\n",
    "                # remove any words that are now empty\n",
    "                line = [xx for xx in line if xx != '']\n",
    "                # stem words\n",
    "                line = [self.p.stem(xx) for xx in line]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "                if len(line) > 0:\n",
    "                    of.write(\" \".join(line))\n",
    "                    of.write('\\n')\n",
    "            f.close()\n",
    "            of.close()\n",
    "            docs.append(contents)\n",
    "        return titles, docs\n",
    "\n",
    "    def __read_stemmed_data(self, dirname):\n",
    "        print(\"Already stemmed!\")\n",
    "        titles = []\n",
    "        docs = []\n",
    "\n",
    "        # make sure we're only getting the files we actually want\n",
    "        filenames = []\n",
    "        for filename in os.listdir('%s/stemmed' % dirname):\n",
    "            if filename.endswith(\".txt\") and not filename.startswith(\".\"):\n",
    "                filenames.append(filename)\n",
    "\n",
    "        if len(filenames) != 60:\n",
    "            msg = \"There are not 60 documents in ../data/RiderHaggard/stemmed/\\n\"\n",
    "            msg += \"Remove ../data/RiderHaggard/stemmed/ directory and re-run.\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        for i, filename in enumerate(filenames):\n",
    "            title = filename.split('.')[0]\n",
    "            titles.append(title)\n",
    "            contents = []\n",
    "            f = open('%s/stemmed/%s' % (dirname, filename), 'r',\n",
    "                     encoding=\"utf-8\")\n",
    "            for line in f:\n",
    "                # split on whitespace\n",
    "                line = [xx.strip() for xx in line.split()]\n",
    "                # add to the document's conents\n",
    "                contents.extend(line)\n",
    "            f.close()\n",
    "            docs.append(contents)\n",
    "\n",
    "        return titles, docs\n",
    "\n",
    "    def read_data(self, dirname):\n",
    "        \"\"\"\n",
    "        Given the location of the 'data' directory, reads in the documents to\n",
    "        be indexed.\n",
    "        \"\"\"\n",
    "        # NOTE: We cache stemmed documents for speed\n",
    "        #       (i.e. write to files in new 'stemmed/' dir).\n",
    "\n",
    "        print(\"Reading in documents...\")\n",
    "        # dict mapping file names to list of \"words\" (tokens)\n",
    "        filenames = os.listdir(dirname)\n",
    "        subdirs = os.listdir(dirname)\n",
    "        if 'stemmed' in subdirs:\n",
    "            titles, docs = self.__read_stemmed_data(dirname)\n",
    "        else:\n",
    "            titles, docs = self.__read_raw_data(dirname)\n",
    "\n",
    "        # Sort document alphabetically by title to ensure we have the proper\n",
    "        # document indices when referring to them.\n",
    "        ordering = [idx for idx, title in sorted(enumerate(titles),\n",
    "                                                 key=lambda xx: xx[1])]\n",
    "\n",
    "        self.titles = []\n",
    "        self.docs = []\n",
    "        numdocs = len(docs)\n",
    "        for d in range(numdocs):\n",
    "            self.titles.append(titles[ordering[d]])\n",
    "            self.docs.append(docs[ordering[d]])\n",
    "\n",
    "        # Get the vocabulary.\n",
    "        self.vocab = [xx for xx in self.get_uniq_words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will first build the inverted positional index (data structure that keeps track of the documents in which a particular word is contained, and the positions of that word in the document). The documents will have already been read in at this point. The following instance variables in the class are included in the starter code for you to use to build your inverted positional index: titles (a list of strings), docs (a list of lists of strings), and vocab (a list of strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def index(self):\n",
    "        \"\"\"\n",
    "        Build an index of the documents.\n",
    "        \"\"\"\n",
    "        print(\"Indexing...\")\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Create an inverted, positional index.\n",
    "        #       Granted this may not be a linked list as in a proper\n",
    "        #       implementation.\n",
    "        #       This index should allow easy access to both \n",
    "        #       1) the documents in which a particular word is contained, and \n",
    "        #       2) for every document, the positions of that word in the document \n",
    "        #       Some helpful instance variables:\n",
    "        #         * self.docs = List of documents\n",
    "        #         * self.titles = List of titles\n",
    "\n",
    "        inv_index = {}\n",
    "\n",
    "        # Generate inverted index here\n",
    "        for i in range(len(self.docs)):\n",
    "            doc = self.docs[i]\n",
    "            for j,word in enumerate(doc):\n",
    "                if word not in inv_index:\n",
    "                    inv_index[word] = {i:[j]}\n",
    "                else:\n",
    "                    if i not in inv_index[word]:\n",
    "                        inv_index[word][i] = [j]\n",
    "                    else:    \n",
    "                        inv_index[word][i].append(j)\n",
    "\n",
    "        self.inv_index = inv_index\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        # turn self.docs into a map from ID to bag of words\n",
    "        id_to_bag_of_words = {}\n",
    "        for d, doc in enumerate(self.docs):\n",
    "            bag_of_words = set(doc)\n",
    "            id_to_bag_of_words[d] = bag_of_words\n",
    "        self.docs = id_to_bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement get_word_positions. This method returns a list of integers that identifies the positions in the document doc in which the word is found.  This is basically just an API into your inverted index, but you must implement it in order for the index to be evaluated fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_word_positions(self, word, doc):\n",
    "        \"\"\"\n",
    "        Given a word and a document, use the inverted index to return\n",
    "        the positions of the specified word in the specified document.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of positions for a word in a document.\n",
    "\n",
    "        positions = []\n",
    "        if word in self.inv_index:\n",
    "            if doc in self.inv_index[word]:\n",
    "                positions = self.inv_index[word][doc]    \n",
    "\n",
    "        return positions\n",
    "        # ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add another method, get_posting, that returns a list of integers (document IDs) that identifies the documents in which the word is found. This is basically another API into your inverted index, but you must implement it in order to be evaluated fully.\n",
    "\n",
    "Keep in mind that the document IDs in each postings list to be sorted in order to perform the linear merge for boolean retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def get_posting(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this returns the list of document indices (sorted) in\n",
    "        which the word occurs.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: return the list of postings for a word.\n",
    "        posting = []\n",
    "\n",
    "        posting = sorted(list(self.inv_index[word].keys()))\n",
    "\n",
    "        return posting\n",
    "        # ------------------------------------------------------------------\n",
    "    \n",
    "    def get_posting_unstemmed(self, word):\n",
    "        \"\"\"\n",
    "        Given a word, this *stems* the word and then calls get_posting on the\n",
    "        stemmed word to get its postings list. You should *not* need to change\n",
    "        this function. It is needed for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_posting(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement Boolean retrieval, returning a list of document IDs corresponding to the documents in which all the words in query occur.\n",
    "\n",
    "Please implement the linear merge algorithm outlined in the videos/book (do not use built-in set intersection functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def boolean_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of a list of *stemmed* words, this returns\n",
    "        the list of documents in which *all* of those words occur (ie an AND\n",
    "        query).\n",
    "        Return an empty list if the query does not return any documents.\n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Boolean retrieval. You will want to use your\n",
    "        #       inverted index that you created in index().\n",
    "        # Right now this just returns all the possible documents!\n",
    "        docs = []\n",
    "        # get the first word in the query and it's posting list\n",
    "        first_word = query[0]\n",
    "        docs = self.get_posting(first_word)\n",
    "        #print(f\"first query word docs: {docs}\")\n",
    "        for word in query[1:]:\n",
    "            docs_next = self.get_posting(word)\n",
    "            # use linear merge to get the intersection between docs and docs_next\n",
    "            i = 0\n",
    "            j = 0\n",
    "            common = []\n",
    "            while True:\n",
    "                if docs[i] == docs_next[j]:\n",
    "                    common.append(docs[i])\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "                else:\n",
    "                    if docs[i] < docs_next[j]:\n",
    "                        i += 1    \n",
    "                    else:\n",
    "                        j += 1\n",
    "\n",
    "                if (i == len(docs)) or (j == len(docs_next)):\n",
    "                    break\n",
    "            docs = common        \n",
    "            #docs = list(set(docs) & set(docs_next))\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue to phase query retrieval. Our phrase_retrieve method will return a list of document IDs corresponding to the documents in which all the actual query phrase occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def phrase_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query in the form of an ordered list of *stemmed* words, this \n",
    "        returns the list of documents in which *all* of those words occur, and \n",
    "        in the specified order. \n",
    "        Return an empty list if the query does not return any documents. \n",
    "        \"\"\"\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement Phrase Query retrieval (ie. return the documents \n",
    "        #       that don't just contain the words, but contain them in the \n",
    "        #       correct order) You will want to use the inverted index \n",
    "        #       that you created in index(), and may also consider using\n",
    "        #       boolean_retrieve. \n",
    "        #       NOTE that you no longer have access to the original documents\n",
    "        #       in self.docs because it is now a map from doc IDs to set\n",
    "        #       of unique words in the original document.\n",
    "        # Right now this just returns all possible documents!\n",
    "        docs = []\n",
    "\n",
    "        # boolean retrieve documents which contain all words in query\n",
    "        docs_bool = self.boolean_retrieve(query)\n",
    "\n",
    "        # check which documents contain the query words in a contiguous span\n",
    "        for doc in docs_bool:\n",
    "            # get all positions of each query word\n",
    "            query_positions = [self.get_word_positions(word, doc) for word in query]\n",
    "            # for each position of the first query word, check for consecutive positions in the lists for the remiaining query words\n",
    "            for pos in query_positions[0]:\n",
    "                found = True\n",
    "                expected_pos = pos+1\n",
    "                for next_word_positions in query_positions[1:]:\n",
    "                    if expected_pos not in next_word_positions:\n",
    "                        found = False\n",
    "                        break\n",
    "                    else:\n",
    "                        expected_pos += 1\n",
    "                if found:\n",
    "                    docs.append(doc)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        return sorted(docs)  # sorted doesn't actually matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute and score the td-idf values. compute_tfidf and stores the tf-idf values for words and documents. For this you will probably want to be aware of the class variable vocab, which holds the list of all unique words, as well as the inverted index you created earlier.\n",
    "\n",
    "You must also implement get_tfidf to return the tf-idf weight for a particular word and document ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def compute_tfidf(self):\n",
    "        # -------------------------------------------------------------------\n",
    "        # TODO: Compute and store TF-IDF values for words and documents in self.tfidf.\n",
    "        #       Recall that you can make use of:\n",
    "        #         * self.vocab: a list of all distinct (stemmed) words\n",
    "        #         * self.docs: a list of lists, where the i-th document is\n",
    "        #                   self.docs[i] => ['word1', 'word2', ..., 'wordN']\n",
    "        #       NOTE that you probably do *not* want to store a value for every\n",
    "        #       word-document pair, but rather just for those pairs where a\n",
    "        #       word actually occurs in the document.\n",
    "        print(\"Calculating tf-idf...\")\n",
    "        self.tfidf = {}\n",
    "        \n",
    "        N = len(self.docs)\n",
    "\n",
    "        for word in self.inv_index.keys():\n",
    "            docs = self.inv_index[word]\n",
    "            df = len(docs)\n",
    "            for d, pos_list in docs.items():\n",
    "                tf = len(pos_list)\n",
    "                self.tfidf[(word,d)] = (1 + math.log10(tf)) * math.log10(N/df)\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "    def get_tfidf(self, word, document):\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Return the tf-idf weigthing for the given word (string) and\n",
    "        #       document index.\n",
    "        tfidf = 0.0\n",
    "\n",
    "        if (word, document) in self.tfidf:\n",
    "            tfidf = self.tfidf[(word, document)]\n",
    "        # ------------------------------------------------------------------\n",
    "        return tfidf\n",
    "\n",
    "    def get_tfidf_unstemmed(self, word, document):\n",
    "        \"\"\"\n",
    "        This function gets the TF-IDF of an *unstemmed* word in a document.\n",
    "        Stems the word and then calls get_tfidf. You should *not* need to\n",
    "        change this interface, but it is necessary for submission.\n",
    "        \"\"\"\n",
    "        word = self.p.stem(word)\n",
    "        return self.get_tfidf(word, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we will implement rank_retrieve. This function returns a priority queue of the top ranked documents for a given query. Right now it ranks documents according to their Jaccard similarity with the query, but you will replace this method of ranking with a ranking using the <b>cosine similarity</b> between the documents and query.\n",
    "    \n",
    "Remember to use ltc.lnn weighting, that is, ltc weighting for the document and lnn weighting for the query! This means that the query vector weights will be $1 + \\log_{10} \\text{tf}_{t, q}$ with no IDF term or normalization, but we normalize the document vector weights by the length of the document vector (square root of the sum of squares of the tf-idf weights). Finally, the cosine scores are the dot product of the query vector and the document vectors. Refer to this [handout](http://web.stanford.edu/class/cs124/lec/CS124_IR_Handout.pdf) or lecture slides for a more detailed explanation.\n",
    "    \n",
    "When we say normalize by \"document length\" or \"length of document\", we mean the length of the document vector, NOT the number of words in the actual text document. So, when you‚Äôre computing cosine similarity between the query and document, the document vector is the tf.idf document weights for all terms in the vocabulary. The document length would be the L2 norm of the document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def rank_retrieve(self, query):\n",
    "        \"\"\"\n",
    "        Given a query (a list of words), return a rank-ordered list of\n",
    "        documents (by ID) and score for the query.\n",
    "        \"\"\"\n",
    "        scores = [0.0 for xx in range(len(self.titles))]\n",
    "        # ------------------------------------------------------------------\n",
    "        # TODO: Implement cosine similarity between a document and a list of\n",
    "        #       query words.\n",
    "\n",
    "        # Right now, this code simply gets the score by taking the Jaccard\n",
    "        # similarity between the query and every document.\n",
    "        words_in_query = set()\n",
    "        \n",
    "        for word in query:\n",
    "            words_in_query.add(word)\n",
    "        \"\"\"\n",
    "        for d, words_in_doc in self.docs.items():\n",
    "            scores[d] = len(words_in_query.intersection(words_in_doc)) \\\n",
    "                        / float(len(words_in_query.union(words_in_doc)))\n",
    "        \"\"\"\n",
    "\n",
    "        # compute tf for query word \n",
    "        tf_query = defaultdict(int)\n",
    "        for word in query:\n",
    "            tf_query[word] =  tf_query[word] + 1\n",
    "        # compute scores for each document\n",
    "        for d in range(len(self.docs)):\n",
    "            # compute dot product between query vector and doument vector\n",
    "            for word, tf in tf_query.items():\n",
    "                if (word, d) in self.tfidf:\n",
    "                   scores[d] = scores[d] + (1+math.log10(tf)) * self.tfidf[(word, d)]\n",
    "            # normalize by length of document vector\n",
    "            norm = 0\n",
    "            for word in self.docs[d]:\n",
    "                norm += self.tfidf[(word, d)]**2\n",
    "            norm = math.sqrt(norm)                        \n",
    "            scores[d] /= norm\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "\n",
    "        ranking = [idx for idx, sim in sorted(enumerate(scores),\n",
    "                                              key=lambda xx: xx[1],\n",
    "                                              reverse=True)]\n",
    "        results = []\n",
    "        for i in range(10):\n",
    "            results.append((ranking[i], scores[ranking[i]]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also add a few methods that given a string, will process and then return the list of matching documents for the different methods you have implemented. You do not need to add any additional code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem): \n",
    "    def process_query(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a query string, process it and return the list of lowercase,\n",
    "        alphanumeric, stemmed words in the string.\n",
    "        \"\"\"\n",
    "        # make sure everything is lower case\n",
    "        query = query_str.lower()\n",
    "        # split on whitespace\n",
    "        query = query.split()\n",
    "        # remove non alphanumeric characters\n",
    "        query = [self.alphanum.sub('', xx) for xx in query]\n",
    "        # stem words\n",
    "        query = [self.p.stem(xx) for xx in query]\n",
    "        return query\n",
    "\n",
    "    def query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by boolean_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.boolean_retrieve(query)\n",
    "\n",
    "    def phrase_query_retrieve(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of matching documents\n",
    "        found by phrase_retrieve().\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.phrase_retrieve(query)\n",
    "\n",
    "    def query_rank(self, query_str):\n",
    "        \"\"\"\n",
    "        Given a string, process and then return the list of the top matching\n",
    "        documents, rank-ordered.\n",
    "        \"\"\"\n",
    "        query = self.process_query(query_str)\n",
    "        return self.rank_retrieve(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "You can use the run_query function to test your code on a specific query. \n",
    "\n",
    "Note that the first time to run the run_query function, it will create a directory named stemmed/ in ../data/RiderHaggard/. This is meant to be a simple cache for the raw text documents. Later runs will be much faster after the first run. However, this means that if something happens during this first run and it does not get through processing all the documents, you may be left with an incomplete set of documents in ../data/RiderHaggard/stemmed/. If this happens, simply remove the stemmed/ directory and re-run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(irsys):\n",
    "    print(\"===== Running tests =====\")\n",
    "\n",
    "    ff = open('./data/dev_queries.txt')\n",
    "    questions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "    ff = open('./data/dev_solutions.txt')\n",
    "    solutions = [xx.strip() for xx in ff.readlines()]\n",
    "    ff.close()\n",
    "\n",
    "    epsilon = 1e-4\n",
    "    for part in range(6):\n",
    "        points = 0\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        prob = questions[part]\n",
    "        soln = json.loads(solutions[part])\n",
    "\n",
    "        if part == 0:  # inverted index test\n",
    "            print(\"Inverted Index Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            print(\"queries: \", queries)\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_word_positions(word, doc)\n",
    "                if sorted(guess) == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        if part == 1:  # get postings test\n",
    "            print(\"Get Postings Test\")\n",
    "            words = prob.split(\", \")\n",
    "            for i, word in enumerate(words):\n",
    "                num_total += 1\n",
    "                posting = irsys.get_posting_unstemmed(word)\n",
    "                if posting == soln[i]:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 2:  # boolean retrieval test\n",
    "            print(\"Boolean Retrieval Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            print(\"queries: \", queries)\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 3:  # phrase query test\n",
    "            print(\"Phrase Query Retrieval\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.phrase_query_retrieve(query)\n",
    "                if set(guess) == set(soln[i]):\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 4:  # tfidf test\n",
    "            print(\"TF-IDF Test\")\n",
    "            queries = prob.split(\"; \")\n",
    "            queries = [xx.split(\", \") for xx in queries]\n",
    "            queries = [(xx[0], int(xx[1])) for xx in queries]\n",
    "            for i, (word, doc) in enumerate(queries):\n",
    "                num_total += 1\n",
    "                guess = irsys.get_tfidf_unstemmed(word, doc)\n",
    "                if guess >= float(soln[i]) - epsilon and \\\n",
    "                        guess <= float(soln[i]) + epsilon:\n",
    "                    num_correct += 1\n",
    "\n",
    "        elif part == 5:  # cosine similarity test\n",
    "            print(\"Cosine Similarity Test\")\n",
    "            queries = prob.split(\", \")\n",
    "            for i, query in enumerate(queries):\n",
    "                num_total += 1\n",
    "                ranked = irsys.query_rank(query)\n",
    "                top_rank = ranked[0]\n",
    "                if top_rank[0] == soln[i][0]:\n",
    "                    if top_rank[1] >= float(soln[i][1]) - epsilon and \\\n",
    "                            top_rank[1] <= float(soln[i][1]) + epsilon:\n",
    "                        num_correct += 1\n",
    "\n",
    "        feedback = \"%d/%d Correct. Accuracy: %f\" % \\\n",
    "                   (num_correct, num_total, float(num_correct) / num_total)\n",
    "\n",
    "        if part == 1:\n",
    "            if num_correct == num_total:\n",
    "                points = 2\n",
    "            elif num_correct >= 0.5 * num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        elif part == 2:\n",
    "            if num_correct == num_total:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "        else:\n",
    "            if num_correct == num_total:\n",
    "                points = 3\n",
    "            elif num_correct > 0.75 * num_total:\n",
    "                points = 2\n",
    "            elif num_correct > 0:\n",
    "                points = 1\n",
    "            else:\n",
    "                points = 0\n",
    "\n",
    "        print(\"    Score: %d Feedback: %s\" % (points, feedback))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "===== Running tests =====\n",
      "Inverted Index Test\n",
      "queries:  [('separ', 30), ('priest', 25), ('underground', 18), ('zulu', 43), ('queen', 22)]\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Get Postings Test\n",
      "    Score: 2 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Boolean Retrieval Test\n",
      "queries:  ['separation of church and state', 'white-robed priests', 'ancient underground city', 'native african queen', 'zulu king']\n",
      "    Score: 1 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Phrase Query Retrieval\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "TF-IDF Test\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
      "Cosine Similarity Test\n",
      "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to run the tests\n",
    "irsys = IRSystem()\n",
    "irsys.read_data('./data/RiderHaggard')\n",
    "irsys.index()\n",
    "irsys.compute_tfidf()\n",
    "\n",
    "run_tests(irsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in documents...\n",
      "Already stemmed!\n",
      "Indexing...\n",
      "Calculating tf-idf...\n",
      "Best matching documents to 'My very own query':\n",
      "Long Odds: 2.240478e-03\n",
      "Hunter Quatermain's Story: 2.103787e-03\n",
      "The Tale of Three Lions: 1.574803e-03\n",
      "The Mahatma and the Hare: 1.283697e-03\n",
      "Black Heart and White Heart: 1.139818e-03\n",
      "Elissa: 1.117943e-03\n",
      "Moon of Israel: 1.059883e-03\n",
      "Morning Star: 9.930487e-04\n",
      "Maiwa's Revenge: 9.526834e-04\n",
      "Doctor Therne: 9.261403e-04\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    irsys = IRSystem()\n",
    "    irsys.read_data('./data/RiderHaggard')\n",
    "    irsys.index()\n",
    "    irsys.compute_tfidf()\n",
    "    \n",
    "    print(\"Best matching documents to '%s':\" % query)\n",
    "    results = irsys.query_rank(query)\n",
    "    for docId, score in results:\n",
    "        print(\"%s: %e\" % (irsys.titles[docId], score))\n",
    "        \n",
    "## Example query run where \"My very own query\" is your query.\n",
    "run_query(\"My very own query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval Systems and Search Engines\n",
    "\n",
    "Safiya Umoja Noble‚Äôs <a href=\"https://nyupress.org/9781479837243/algorithms-of-oppression/\">Algorithms of Oppression</a> (2018) provides insight into how search engines and information retrieval algorithms can exhibit substantial racist and sexist biases. Noble demonstrates how prejudice against black women is embedded into search engine ranked results. These biases are apparent in both Google search‚Äôs autosuggestions and the first page of Google results. In this assignment, we have explored numerous features that are built into information retrieval systems, like Google Search. \n",
    "\n",
    "How could the algorithms you built in this assignment contribute to enforcing real-world biases? \n",
    "\n",
    "How can we reduce data discrimination and algorithmic bias that perpetuate gender and racial inequalities in search results and IR system?\n",
    "\n",
    "Please provide a short response to each of these questions (1-2 paragraphs per question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRSystem(IRSystem):\n",
    "    def algorithmic_bias_in_IR_systems(self):\n",
    "        # TODO: Place your response into the response string below\n",
    "        response = \"\"\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're ready to submit, you can run the cell below to prepare and zip\n",
    "up your solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa4.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa4.ipynb deps/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running on Google Colab, see the README for instructions on\n",
    "how to submit.\n",
    "\n",
    "__Best of luck!__\n",
    "\n",
    "__Some reminders for submission:__\n",
    "* If you have any extra files required for your implementation to work, make\n",
    " sure they are in a `deps/` folder on the same level as `pa4.ipynb` and\n",
    " include that folder in your submission zip file.\n",
    " * Make sure you didn't accidentally change the name of your notebook file,\n",
    " (it should be `pa4.ipynb`) as that is required for the autograder to work.\n",
    "* Go to Gradescope (gradescope.com), find the PA4 IR assignment and\n",
    "upload your zip file (`submission.zip`) as your solution.\n",
    "* Wait for the autograder to run (it should only take a minute or so) and check\n",
    "that your submission was graded successfully! If the autograder fails, or you\n",
    "get an unexpected score it may be a sign that your zip file was incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
