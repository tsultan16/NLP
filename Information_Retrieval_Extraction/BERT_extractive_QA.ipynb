{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning BERT for `Extractive Question Answering` \n",
    "\n",
    "We will finetune a BERT model on the task of extractive QA, which involves taking a `factoid question` and a `context passage` of text and `labeling a span` of text from that passage which contains the `answer`. We can frame this as a `classification task`. First we concatenate the question and context passage pair, seperated by a `[SEP]` token. Then we compute the BERT encoding for this sequence. Then we apply a linear transform to each output token's encoding vector to compute a scalar score. By passing the scores from all tokens through a softmax, we obtain a `probability distribution` over tokens in the sequence, which we can interpret as the probability of a token being the start of the span. We actually will compute two separate linear transforms of all tokens and pass both sets of scores through a softmax to get two probability distributions over tokens, one for `start of span` and one for `end of span`. \n",
    "\n",
    "We will train this model on the SQuAD v1 dataset which contains passages with multiple questions and answer span pairs. We will use the cross entropy loss at the softmax output. To make predictions, we can simply just add up the scores of the `ith` token being the start and the `jth` token being the end for all i and j>i, then declare the (i,j) with the highest score as the predicted span.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data from file and then set up pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passages(squad, num_titles=None):\n",
    "    if num_titles is None:\n",
    "        num_titles = len(squad['data'])\n",
    "    # for each title, get passages and all corresponding questions from SQuAD train set\n",
    "    passages = []\n",
    "    questions = []\n",
    "    num_questions = 0\n",
    "    j = 0\n",
    "    for i in range(num_titles):\n",
    "        #print(f\"Title# {i}: {squad['data'][i]['title']}, Number of passages: {len(squad['data'][i]['paragraphs'])}\")\n",
    "        for p in squad['data'][i]['paragraphs']:\n",
    "            passages.append(p['context'])\n",
    "            for q in p['qas']:\n",
    "                if not q['is_impossible']:\n",
    "                    questions.append((q,j))    \n",
    "                    num_questions += 1\n",
    "            j += 1\n",
    "    print(f\"Number of passages: {len(passages)}\")\n",
    "    print(f\"Number of questions: {num_questions}\")\n",
    "    return passages, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 312\n",
      "Number of questions: 2282\n",
      "Number of passages: 173\n",
      "Number of questions: 705\n"
     ]
    }
   ],
   "source": [
    "passages_train, questions_train = get_passages(squad_train, num_titles=5)\n",
    "passages_val, questions_val = get_passages(squad_dev, num_titles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max passage length: 2132, Avg passage length: 728.1698717948718\n"
     ]
    }
   ],
   "source": [
    "passage_lengths = [len(p) for p in passages_train]\n",
    "print(f\"Max passage length: {max(passage_lengths)}, Avg passage length: {sum(passage_lengths)/len(passage_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the context passages are very long (over 700 words on average) and won't fit into our BERT model (which can only take upto 512 tokens per sequence). So we will instead take a shorter fixed size context window for each question.  \n",
    "\n",
    "Since we will use WordPiece tokenization, we also need to be careful about converting the character positions of the start and end of the span to subwork token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the first album Beyoncé released as a solo artist?\n",
      "Answer span: Dangerously in Love\n"
     ]
    }
   ],
   "source": [
    "q_idx = 10\n",
    "q = questions_train[q_idx][0]\n",
    "passage_idx = questions_train[q_idx][1]\n",
    "answer_start_pos = q['answers'][0]['answer_start']\n",
    "answer_end_pos = answer_start_pos + len(q['answers'][0]['text'])\n",
    "context = passages_train[passage_idx]\n",
    "\n",
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special tokens with their integer id:\n",
      "{'[UNK]': 100, '[SEP]': 102, '[PAD]': 0, '[CLS]': 101, '[MASK]': 103}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSpecial tokens with their integer id:\")\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "special_tokens_to_ids = {t:tokenizer.convert_tokens_to_ids(t) for t in special_tokens}\n",
    "print(special_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window:  songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awar\n",
      "Answer window: Dangerously in Love\n",
      "Answer window trimmed: Dangerously in Love\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n",
      "Decoded subword token span: dangerously in love\n",
      "Decoded sentence pair: [CLS] record producer and actress. born and raised in houston, texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of r & b girl - group destiny's child. managed by her father, mathew knowles, the group became one of the world's best - selling girl groups of all time. their hiatus saw the release of beyonce's debut album, dangerously in love ( 2003 ), which established her as a solo artist worldwide, earned five grammy [SEP] what was the first album beyonce released as a solo artist? [SEP]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Method 1: Create window directly on the context string, before tokenization.\n",
    "\"\"\"\n",
    "\n",
    "window_size_chars = 500\n",
    "# pick a random context window around the answer (try to keep at least 40% of the characters in window on the left side of the answer)\n",
    "answer_middle_pos = int((answer_start_pos+answer_end_pos)/2) \n",
    "a = max(0,answer_end_pos-window_size_chars)\n",
    "b = max(0, answer_start_pos - 0.4*window_size_chars)\n",
    "random_window_start_pos = random.randint(a, b)\n",
    "#window_start_pos = max(0,answer_middle_pos-window_size_chars)\n",
    "#window_end_pos = answer_middle_pos+window_size_chars\n",
    "window_start_pos = random_window_start_pos\n",
    "window_end_pos = window_start_pos+window_size_chars\n",
    "\n",
    "context_window = context[window_start_pos:window_end_pos]\n",
    "print(\"Context window: \", context_window)\n",
    "\n",
    "answer_start_pos_window = answer_start_pos - window_start_pos\n",
    "answer_end_pos_window = answer_start_pos_window + len(q['answers'][0]['text'])\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window: {answer_window}\")\n",
    "\n",
    "# Trim off stray partial words at the beginning and end\n",
    "context_window_words = context_window.split()\n",
    "# only trim if the first stray word does not overlap with the answer span\n",
    "if answer_start_pos_window > len(context_window_words[0]):\n",
    "    context_window = ' '.join(context_window_words[1:-1])\n",
    "    left_trim_length = len(context_window_words[0]) + 1 # add 1 for the white space between stary partial first word and next word\n",
    "    answer_start_pos_window = answer_start_pos_window - left_trim_length\n",
    "    answer_end_pos_window = answer_end_pos_window - left_trim_length\n",
    "\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window trimmed: {answer_window}\")\n",
    "\n",
    "\n",
    "# encode the passage\n",
    "context_encoded = tokenizer.encode_plus((context_window, q['question']), add_special_tokens=True, return_offsets_mapping=True)\n",
    "print(context_encoded.keys())\n",
    "# convert character positions from original sentence to subword token positions\n",
    "start_pos_enc = context_encoded.char_to_token(answer_start_pos_window)\n",
    "end_pos_enc = context_encoded.char_to_token(answer_end_pos_window-1)\n",
    "# get the corresponding subword token span\n",
    "answer_span_encoded = context_encoded['input_ids'][start_pos_enc:end_pos_enc+1]\n",
    "# decode the span to check if it matches original answer span\n",
    "print(f\"Decoded subword token span: {tokenizer.decode(answer_span_encoded)}\")\n",
    "print(f\"Decoded sentence pair: {tokenizer.decode(context_encoded['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the first album Beyoncé released as a solo artist?\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Answer span: Dangerously in Love\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword tokens: ['beyonce', 'gi', '##selle', 'knowles', '-', 'carter', '(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/', 'bee', '-', 'yo', '##n', '-', 'say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl', '-', 'group', 'destiny', \"'\", 's', 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'\", 's', 'best', '-', 'selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyonce', \"'\", 's', 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number', '-', 'one', 'singles', '\"', 'crazy', 'in', 'love', '\"', 'and', '\"', 'baby', 'boy', '\"', '.']\n",
      "Answer start char: 505, Answer end char: 524\n",
      "Answer start token: 123, Answer end token: 125\n",
      "start min: 116, start max: 123\n",
      "Random window start: 122, window end: 132\n",
      "window tokens: [',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established']\n",
      "window answer start token: 1, window answer end token: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "    Method 2: Tokenize first, then create context window around answer span. (Cleaner than method 1)\n",
    "\"\"\"\n",
    "\n",
    "# tokenize the context passage, get offset mapping\n",
    "encoding = tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "input_ids = encoding['input_ids']\n",
    "#offset_mapping = encoding['offset_mapping']\n",
    "print(f\"Subword tokens: {tokenizer.convert_ids_to_tokens(input_ids)}\")\n",
    "#print(f\"Offset mapping: {offset_mapping}\")\n",
    "\n",
    "# answer span start and end character positions\n",
    "answer_start_char = q['answers'][0]['answer_start']\n",
    "answer_end_char = answer_start_char + len(q['answers'][0]['text'])\n",
    "print(f\"Answer start char: {answer_start_char}, Answer end char: {answer_end_char}\")    \n",
    "\n",
    "# convert char positions to token positions\n",
    "answer_start_token = encoding.char_to_token(answer_start_char)\n",
    "answer_end_token = encoding.char_to_token(answer_end_char-1)\n",
    "print(f\"Answer start token: {answer_start_token}, Answer end token: {answer_end_token}\")    \n",
    "\n",
    "# now create a window around the answer span, pick the window start position randomly\n",
    "window_size_tokens = 10\n",
    "\n",
    "# range of legal starting positions\n",
    "start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "start_max = answer_start_token\n",
    "print(f\"start min: {start_min}, start max: {start_max}\")\n",
    "\n",
    "window_start = random.randint(start_min, start_max)\n",
    "window_end = window_start + window_size_tokens\n",
    "print(f\"Random window start: {window_start}, window end: {window_end}\")\n",
    "\n",
    "# select window of tokens\n",
    "window_tokens = input_ids[window_start:window_end]\n",
    "print(f\"window tokens: {tokenizer.convert_ids_to_tokens(window_tokens)}\")\n",
    "\n",
    "# offset the answer span token positions by window start position\n",
    "answer_start_token_window = answer_start_token - window_start\n",
    "answer_end_token_window = answer_end_token - window_start\n",
    "print(f\"window answer start token: {answer_start_token_window}, window answer end token: {answer_end_token_window}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'was', 'the', 'first', 'album', 'beyonce', 'released', 'as', 'a', 'solo', 'artist', '?', '[SEP]', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "question_encoding = tokenizer.encode_plus(q['question'], add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "question_idx = question_encoding['input_ids']        \n",
    "input_idx = [special_tokens_to_ids[\"[CLS]\"]] + question_idx + [special_tokens_to_ids[\"[SEP]\"]] + window_tokens + [special_tokens_to_ids[\"[SEP]\"]]   \n",
    "print(tokenizer.convert_ids_to_tokens(input_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's creating a pytorch dataset that handles the tokenization and context creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, passages, questions, max_length=256, block_size=20):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the question and context passage\n",
    "        q = self.questions[idx][0]\n",
    "        passage_idx = self.questions[idx][1]\n",
    "        question = q['question']\n",
    "        context = self.passages[passage_idx]\n",
    "        # tokenize the context passage\n",
    "        context_encoding = tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        context_idx = context_encoding['input_ids']\n",
    "        # tokenize the question\n",
    "        question_encoding = tokenizer.encode_plus(q['question'], add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        question_idx = question_encoding['input_ids']\n",
    "\n",
    "        # get answer span start and end character positions, for multiple answers, we will only use the first answer\n",
    "        first_answer_idx = 0\n",
    "        answer_start_char = q['answers'][first_answer_idx]['answer_start']\n",
    "        answer_end_char = answer_start_char + len(q['answers'][first_answer_idx]['text'])\n",
    "        # convert char positions to token positions\n",
    "        answer_start_token = context_encoding.char_to_token(answer_start_char)\n",
    "        answer_end_token = context_encoding.char_to_token(answer_end_char-1)\n",
    "\n",
    "        # select a window size so that the input sequence is not longer than block size\n",
    "        window_size_tokens = self.block_size - len(question_idx) - 3 # 3 special tokens ([CLS], [SEP], [SEP])\n",
    "        # now create a window around the answer span, pick the window start position randomly\n",
    "        window_start = random.randint(max(0, answer_end_token - window_size_tokens + 1), answer_start_token)\n",
    "        window_end = window_start + window_size_tokens\n",
    "        \n",
    "        # select window of tokens\n",
    "        window_tokens = context_idx[window_start:window_end]\n",
    "        # offset the answer span token positions by window start position\n",
    "        answer_start_token_window = answer_start_token - window_start\n",
    "        answer_end_token_window = answer_end_token - window_start\n",
    "        # concatenate the question and context, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id] + question_idx + [self.tokenizer.sep_token_id] + window_tokens + [self.tokenizer.sep_token_id]\n",
    "        # make sure the input sequence is not longer than max_length\n",
    "        if len(input_idx) > self.max_length:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.max_length}!\")\n",
    "\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id]*(self.max_length-len(input_idx))\n",
    "        # offset the answer span token positions again by the length of the question and the two special tokens ([CLS] and [SEP])\n",
    "        answer_start_token_window = answer_start_token_window + len(question_idx) + 2\n",
    "        answer_end_token_window = answer_end_token_window + len(question_idx) + 2\n",
    "        # create attention mask\n",
    "        attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        start_pos_enc = torch.tensor(answer_start_token_window)\n",
    "        end_pos_enc = torch.tensor(answer_end_token_window)\n",
    "        return input_idx, start_pos_enc, end_pos_enc, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small fraction of the full dataset\n",
    "train_dataset = SquadDataset(passages_train, questions_train, block_size=128)\n",
    "val_dataset = SquadDataset(passages_val, questions_val, block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input idx: ['[CLS]', 'what', 'did', 'beyonce', 'and', 'jay', 'z', 'name', 'their', 'daughter', '?', '[SEP]', 'a', 'daughter', ',', 'blue', 'ivy', 'carter', ',', 'at', 'len', '##ox', 'hill', 'hospital', 'in', 'new', 'york', 'under', 'heavy', 'security', '.', 'two', 'days', 'later', ',', 'jay', 'z', 'released', '\"', 'glory', '\"', ',', 'a', 'song', 'dedicated', 'to', 'their', 'child', ',', 'on', 'his', 'website', 'life', '##and', '##time', '##s', '.', 'com', '.', 'the', 'song', 'detailed', 'the', 'couple', \"'\", 's', 'pregnancy', 'struggles', ',', 'including', 'a', 'mis', '##carriage', 'beyonce', 'suffered', 'before', 'becoming', 'pregnant', 'with', 'blue', 'ivy', '.', 'blue', 'ivy', \"'\", 's', 'cries', 'are', 'included', 'at', 'the', 'end', 'of', 'the', 'song', ',', 'and', 'she', 'was', 'officially', 'credited', 'as', '\"', 'b', '.', 'i', '.', 'c', '.', '\"', 'on', 'it', '.', 'at', 'two', 'days', 'old', ',', 'she', 'became', 'the', 'youngest', 'person', 'ever', 'to', 'appear', 'on', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Answer span: ['blue', 'ivy', 'carter']\n"
     ]
    }
   ],
   "source": [
    "idx = 356\n",
    "input_idx, start_pos_enc, end_pos_enc, attn_mask = train_dataset[idx]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_idx)\n",
    "print(f\"Input idx: {input_tokens}\")\n",
    "print(f\"Answer span: {input_tokens[start_pos_enc:end_pos_enc+1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTExtractiveQA(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)      \n",
    "        # define two classifier heads, one for predicting start of span and another for end of span \n",
    "        self.classifier_head_start_span = torch.nn.Linear(hidden_size, 1)\n",
    "        self.classifier_head_end_span = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, labels_start, labels_end, attn_mask):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        # compute logits/scores over tokens for each of the classifier heads\n",
    "        logits_start = self.classifier_head_start_span(bert_output).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "        logits_end = self.classifier_head_end_span(bert_output).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits_start, labels_start) + F.cross_entropy(logits_end, labels_end) \n",
    "\n",
    "        return logits_start, logits_end, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets_start, targets_end, attn_mask = batch\n",
    "            # move batch to device\n",
    "            inputs, targets_start, targets_end, attn_mask = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits_start, logits_end, loss = model(inputs, targets_start, targets_end, attn_mask)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += ((y_pred_start.eq(targets_start.view(-1)) + y_pred_end.eq(targets_end.view(-1))) == 2).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets_start, targets_end, attn_mask = batch\n",
    "            inputs, targets_start, targets_end, attn_mask = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device)\n",
    "            logits_start, logits_end, loss = model(inputs, targets_start, targets_end, attn_mask)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += ((y_pred_start.eq(targets_start.view(-1)) + y_pred_end.eq(targets_end.view(-1))) == 2).sum().item()            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'qa_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('qa_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model with and without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 4353.38 MB\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "B = 64\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-3\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True) #, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True) #, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTExtractiveQA(finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256]) torch.Size([64]) torch.Size([64]) torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets_start, targets_end, attn_mask = next(iter(train_dataloader))\n",
    "print(inputs.shape, targets_start.shape, targets_end.shape, attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 4.595, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000:  17%|█▋        | 6/36 [00:03<00:15,  1.88it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=2, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=1, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
