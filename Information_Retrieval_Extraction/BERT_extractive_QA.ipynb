{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning BERT for `Extractive Question Answering` \n",
    "\n",
    "We will finetune a BERT model on the task of extractive QA, which involves taking a `factoid question` and a `context passage` of text and `labeling a span` of text from that passage which contains the `answer`. We can frame this as a `classification task`. First we concatenate the question and context passage pair, seperated by a `[SEP]` token. Then we compute the BERT encoding for this sequence. Then we apply a linear transform to each output token's encoding vector to compute a scalar score. By passing the scores from all tokens through a softmax, we obtain a `probability distribution` over tokens in the sequence, which we can interpret as the probability of a token being the start of the span. We actually will compute two separate linear transforms of all tokens and pass both sets of scores through a softmax to get two probability distributions over tokens, one for `start of span` and one for `end of span`. \n",
    "\n",
    "We will train this model on the SQuAD v1 dataset which contains passages with multiple questions and answer span pairs. We will use the cross entropy loss at the softmax output. To make predictions, we can simply just add up the scores of the `ith` token being the start and the `jth` token being the end for all i and j>i, then declare the (i,j) with the highest score as the predicted span.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data from file and then set up pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passages(squad, num_titles=None):\n",
    "    if num_titles is None:\n",
    "        num_titles = len(squad['data'])\n",
    "    # for each title, get passages and all corresponding questions from SQuAD train set\n",
    "    passages = []\n",
    "    questions = []\n",
    "    num_questions = 0\n",
    "    for i in range(num_titles):\n",
    "        #print(f\"Title# {i}: {squad['data'][i]['title']}, Number of passages: {len(squad['data'][i]['paragraphs'])}\")\n",
    "        for p in squad['data'][i]['paragraphs']:\n",
    "            # we will append the title to each passage and question\n",
    "            passages.append(p['context'])\n",
    "            questions.append([q for q in p['qas']])    \n",
    "            num_questions += len(p['qas'])\n",
    "    print(f\"Number of passages: {len(passages)}\")\n",
    "    print(f\"Number of questions: {num_questions}\")\n",
    "    return passages, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 312\n",
      "Number of questions: 2442\n",
      "Number of passages: 173\n",
      "Number of questions: 1407\n"
     ]
    }
   ],
   "source": [
    "passages_train, questions_train = get_passages(squad_train, num_titles=5)\n",
    "passages_val, questions_val = get_passages(squad_dev, num_titles=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max passage length: 2132, Avg passage length: 728.1698717948718\n"
     ]
    }
   ],
   "source": [
    "passage_lengths = [len(p) for p in passages_train]\n",
    "print(f\"Max passage length: {max(passage_lengths)}, Avg passage length: {sum(passage_lengths)/len(passage_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the context passages are very long (over 700 words on average) and won't fit into our BERT model (which can only take upto 512 tokens per sequence). So we will instead take a shorter fixed size context window for each question.  \n",
    "\n",
    "Since we will use WordPiece tokenization, we also need to be careful about converting the character positions of the start and end of the span to subwork token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many awards was Beyonce nominated for at the 52nd Grammy Awards?\n",
      "Answer span: ten\n"
     ]
    }
   ],
   "source": [
    "q = questions_train[16][0]\n",
    "answer_start_pos = q['answers'][0]['answer_start']\n",
    "answer_end_pos = answer_start_pos + len(q['answers'][0]['text'])\n",
    "context = passages_train[16]\n",
    "\n",
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special tokens with their integer id:\n",
      "{'[UNK]': 100, '[SEP]': 102, '[PAD]': 0, '[CLS]': 101, '[MASK]': 103}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSpecial tokens with their integer id:\")\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "special_tokens_to_ids = {t:tokenizer.convert_tokens_to_ids(t) for t in special_tokens}\n",
    "print(special_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window:  At the 52nd Annual Grammy Awards, Beyoncé received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for \"Halo\", and Song of the Year for \"Single Ladies (Put a Ring on It)\", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyoncé was featured on Lady Gaga's single \"Telephone\" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyoncé and Gaga,\n",
      "Answer window: ten\n",
      "Answer window trimmed: ten\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n",
      "Decoded subword token span: ten\n",
      "Decoded sentence pair: [CLS] the 52nd annual grammy awards, beyonce received ten nominations, including album of the year for i am... sasha fierce, record of the year for \" halo \", and song of the year for \" single ladies ( put a ring on it ) \", among others. she tied with lauryn hill for most grammy nominations in a single year by a female artist. in 2010, beyonce was featured on lady gaga's single \" telephone \" and its music video. the song topped the us pop songs chart, becoming the sixth number - one for both beyonce and [SEP] how many awards was beyonce nominated for at the 52nd grammy awards? [SEP]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Method 1: Create window directly on the context string, before tokenization.\n",
    "\"\"\"\n",
    "\n",
    "window_size_chars = 500\n",
    "# pick a random context window around the answer (try to keep at least 40% of the characters in window on the left side of the answer)\n",
    "answer_middle_pos = int((answer_start_pos+answer_end_pos)/2) \n",
    "a = max(0,answer_end_pos-window_size_chars)\n",
    "b = max(0, answer_start_pos - 0.4*window_size_chars)\n",
    "random_window_start_pos = random.randint(a, b)\n",
    "#window_start_pos = max(0,answer_middle_pos-window_size_chars)\n",
    "#window_end_pos = answer_middle_pos+window_size_chars\n",
    "window_start_pos = random_window_start_pos\n",
    "window_end_pos = window_start_pos+window_size_chars\n",
    "\n",
    "context_window = context[window_start_pos:window_end_pos]\n",
    "print(\"Context window: \", context_window)\n",
    "\n",
    "answer_start_pos_window = answer_start_pos - window_start_pos\n",
    "answer_end_pos_window = answer_start_pos_window + len(q['answers'][0]['text'])\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window: {answer_window}\")\n",
    "\n",
    "# Trim off stray partial words at the beginning and end\n",
    "context_window_words = context_window.split()\n",
    "# only trim if the first stray word does not overlap with the answer span\n",
    "if answer_start_pos_window > len(context_window_words[0]):\n",
    "    context_window = ' '.join(context_window_words[1:-1])\n",
    "    left_trim_length = len(context_window_words[0]) + 1 # add 1 for the white space between stary partial first word and next word\n",
    "    answer_start_pos_window = answer_start_pos_window - left_trim_length\n",
    "    answer_end_pos_window = answer_end_pos_window - left_trim_length\n",
    "\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window trimmed: {answer_window}\")\n",
    "\n",
    "\n",
    "# encode the passage\n",
    "context_encoded = tokenizer.encode_plus((context_window, q['question']), add_special_tokens=True, return_offsets_mapping=True)\n",
    "print(context_encoded.keys())\n",
    "# convert character positions from original sentence to subword token positions\n",
    "start_pos_enc = context_encoded.char_to_token(answer_start_pos_window)\n",
    "end_pos_enc = context_encoded.char_to_token(answer_end_pos_window-1)\n",
    "# get the corresponding subword token span\n",
    "answer_span_encoded = context_encoded['input_ids'][start_pos_enc:end_pos_enc+1]\n",
    "# decode the span to check if it matches original answer span\n",
    "print(f\"Decoded subword token span: {tokenizer.decode(answer_span_encoded)}\")\n",
    "print(f\"Decoded sentence pair: {tokenizer.decode(context_encoded['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many awards was Beyonce nominated for at the 52nd Grammy Awards?\n",
      "Context: At the 52nd Annual Grammy Awards, Beyoncé received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for \"Halo\", and Song of the Year for \"Single Ladies (Put a Ring on It)\", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyoncé was featured on Lady Gaga's single \"Telephone\" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyoncé and Gaga, tying them with Mariah Carey for most number-ones since the Nielsen Top 40 airplay chart launched in 1992. \"Telephone\" received a Grammy Award nomination for Best Pop Collaboration with Vocals.\n",
      "Answer span: ten\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword tokens: ['at', 'the', '52nd', 'annual', 'grammy', 'awards', ',', 'beyonce', 'received', 'ten', 'nominations', ',', 'including', 'album', 'of', 'the', 'year', 'for', 'i', 'am', '.', '.', '.', 'sasha', 'fierce', ',', 'record', 'of', 'the', 'year', 'for', '\"', 'halo', '\"', ',', 'and', 'song', 'of', 'the', 'year', 'for', '\"', 'single', 'ladies', '(', 'put', 'a', 'ring', 'on', 'it', ')', '\"', ',', 'among', 'others', '.', 'she', 'tied', 'with', 'lau', '##ryn', 'hill', 'for', 'most', 'grammy', 'nominations', 'in', 'a', 'single', 'year', 'by', 'a', 'female', 'artist', '.', 'in', '2010', ',', 'beyonce', 'was', 'featured', 'on', 'lady', 'gaga', \"'\", 's', 'single', '\"', 'telephone', '\"', 'and', 'its', 'music', 'video', '.', 'the', 'song', 'topped', 'the', 'us', 'pop', 'songs', 'chart', ',', 'becoming', 'the', 'sixth', 'number', '-', 'one', 'for', 'both', 'beyonce', 'and', 'gaga', ',', 'tying', 'them', 'with', 'maria', '##h', 'carey', 'for', 'most', 'number', '-', 'ones', 'since', 'the', 'nielsen', 'top', '40', 'airplay', 'chart', 'launched', 'in', '1992', '.', '\"', 'telephone', '\"', 'received', 'a', 'grammy', 'award', 'nomination', 'for', 'best', 'pop', 'collaboration', 'with', 'vocals', '.']\n",
      "Offset mapping: [(0, 2), (3, 6), (7, 11), (12, 18), (19, 25), (26, 32), (32, 33), (34, 41), (42, 50), (51, 54), (55, 66), (66, 67), (68, 77), (78, 83), (84, 86), (87, 90), (91, 95), (96, 99), (100, 101), (102, 104), (104, 105), (105, 106), (106, 107), (108, 113), (114, 120), (120, 121), (122, 128), (129, 131), (132, 135), (136, 140), (141, 144), (145, 146), (146, 150), (150, 151), (151, 152), (153, 156), (157, 161), (162, 164), (165, 168), (169, 173), (174, 177), (178, 179), (179, 185), (186, 192), (193, 194), (194, 197), (198, 199), (200, 204), (205, 207), (208, 210), (210, 211), (211, 212), (212, 213), (214, 219), (220, 226), (226, 227), (228, 231), (232, 236), (237, 241), (242, 245), (245, 248), (249, 253), (254, 257), (258, 262), (263, 269), (270, 281), (282, 284), (285, 286), (287, 293), (294, 298), (299, 301), (302, 303), (304, 310), (311, 317), (317, 318), (319, 321), (322, 326), (326, 327), (328, 335), (336, 339), (340, 348), (349, 351), (352, 356), (357, 361), (361, 362), (362, 363), (364, 370), (371, 372), (372, 381), (381, 382), (383, 386), (387, 390), (391, 396), (397, 402), (402, 403), (404, 407), (408, 412), (413, 419), (420, 423), (424, 426), (427, 430), (431, 436), (437, 442), (442, 443), (444, 452), (453, 456), (457, 462), (463, 469), (469, 470), (470, 473), (474, 477), (478, 482), (483, 490), (491, 494), (495, 499), (499, 500), (501, 506), (507, 511), (512, 516), (517, 522), (522, 523), (524, 529), (530, 533), (534, 538), (539, 545), (545, 546), (546, 550), (551, 556), (557, 560), (561, 568), (569, 572), (573, 575), (576, 583), (584, 589), (590, 598), (599, 601), (602, 606), (606, 607), (608, 609), (609, 618), (618, 619), (620, 628), (629, 630), (631, 637), (638, 643), (644, 654), (655, 658), (659, 663), (664, 667), (668, 681), (682, 686), (687, 693), (693, 694)]\n",
      "Answer start char: 51, Answer end char: 54\n",
      "Answer start token: 9, Answer end token: 9\n",
      "start min: 8, start max: 9\n",
      "Random window start: 8, window end: 10\n",
      "window tokens: ['received', 'ten']\n",
      "window answer start token: 1, window answer end token: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "    Method 2: Tokenize first, then create context window around answer span.\n",
    "\"\"\"\n",
    "\n",
    "# tokenize the context passage, get offset mapping\n",
    "encoding = tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=True)\n",
    "input_ids = encoding['input_ids']\n",
    "offset_mapping = encoding['offset_mapping']\n",
    "print(f\"Subword tokens: {tokenizer.convert_ids_to_tokens(input_ids)}\")\n",
    "print(f\"Offset mapping: {offset_mapping}\")\n",
    "\n",
    "# answer span start and end character positions\n",
    "answer_start_char = q['answers'][0]['answer_start']\n",
    "answer_end_char = answer_start_pos + len(q['answers'][0]['text'])\n",
    "print(f\"Answer start char: {answer_start_char}, Answer end char: {answer_end_char}\")    \n",
    "\n",
    "# convert char positions to token positions\n",
    "answer_start_token = encoding.char_to_token(answer_start_char)\n",
    "answer_end_token = encoding.char_to_token(answer_end_char-1)\n",
    "print(f\"Answer start token: {answer_start_token}, Answer end token: {answer_end_token}\")    \n",
    "\n",
    "# now create a window around the answer span, pick the window start position randomly\n",
    "window_size_tokens = 2\n",
    "\n",
    "# range of legal starting positions\n",
    "start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "start_max = answer_end_token\n",
    "print(f\"start min: {start_min}, start max: {start_max}\")\n",
    "\n",
    "window_start = random.randint(start_min, start_max)\n",
    "window_end = window_start + window_size_tokens\n",
    "print(f\"Random window start: {window_start}, window end: {window_end}\")\n",
    "\n",
    "# select window of tokens\n",
    "window_tokens = input_ids[window_start:window_end]\n",
    "print(f\"window tokens: {tokenizer.convert_ids_to_tokens(window_tokens)}\")\n",
    "\n",
    "# offset the answer span token positions by window start position\n",
    "answer_start_token_window = answer_start_token - window_start\n",
    "answer_end_token_window = answer_end_token - window_start\n",
    "print(f\"window answer start token: {answer_start_token_window}, window answer end token: {answer_end_token_window}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create question, trimmed context window and answer span instances. Note: some quesitions may contain empty answers list, those will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating instances\n",
    "def get_instances(passages, questions, window_size_chars=400):\n",
    "    triplets = []\n",
    "    for i, context in enumerate(passages):\n",
    "        for q in questions[i]:\n",
    "            for a in q['answers']:\n",
    "                # get start and end character positions of answer span \n",
    "                answer_start_pos = a['answer_start']\n",
    "                answer_end_pos = answer_start_pos + len(a['text'])\n",
    "                # get windowed context\n",
    "                a = max(0,answer_end_pos-window_size_chars)\n",
    "                b = max(0, answer_start_pos - 0.4*window_size_chars)\n",
    "                window_start_pos = random.randint(a, b)\n",
    "                window_end_pos = window_start_pos+window_size_chars\n",
    "                context_window = context[window_start_pos:window_end_pos]\n",
    "                # offset the answer span positions\n",
    "                answer_start_pos_window = answer_start_pos - window_start_pos\n",
    "                answer_end_pos_window = answer_start_pos_window + len(q['answers'][0]['text'])\n",
    "                # Trim off stray partial words at the beginning and end of window\n",
    "                context_window_words = context_window.split()\n",
    "                # only trim if the first stray word does not overlap with the answer span\n",
    "                \"\"\"\n",
    "                if answer_start_pos_window > len(context_window_words[0]):\n",
    "                    context_window = ' '.join(context_window_words[1:-1])\n",
    "                    left_trim_length = len(context_window_words[0]) + 1 # add 1 for the white space between stary partial first word and next word\n",
    "                    answer_start_pos_window = answer_start_pos_window - left_trim_length\n",
    "                    answer_end_pos_window = answer_end_pos_window - left_trim_length\n",
    "                \"\"\"    \n",
    "                if (answer_start_pos_window < 0) or (answer_end_pos_window < 0) or (answer_end_pos_window < answer_start_pos_window):\n",
    "                    print(f\"Context: {context}\")\n",
    "                    print(f\"Question: {q}\")\n",
    "                    print(f\"answer_start_pos: {answer_start_pos}, answer_end_pos: {answer_end_pos}\")    \n",
    "                    print(f\"context char length: {len(context)}, window start pos: {window_start_pos}, window end pos: {window_end_pos}\")\n",
    "                    print(f\"answer start pos window: {answer_start_pos_window}, answer end pos window: {answer_end_pos_window}\")    \n",
    "                    print(f\"Context trimmed: {context_window}\")\n",
    "                    raise Exception(f\"Trimmed answer span is negative! start_pos: {answer_start_pos_window}, end_pos: {answer_end_pos_window}\")\n",
    "                \n",
    "                triplets.append(((context_window, q['question']), answer_start_pos_window, answer_end_pos_window))\n",
    "    return triplets            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_train = get_instances(passages_train, questions_train)\n",
    "triplets_val = get_instances(passages_val, questions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((\"d performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. T\",\n",
       "  'How many Grammy awards has Beyoncé won?'),\n",
       " 326,\n",
       " 328)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets_train[37]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, data, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the data instance\n",
    "        sentence_pair = self.data[idx][0]\n",
    "        start_pos_char = self.data[idx][1]\n",
    "        end_pos_char = self.data[idx][2]\n",
    "        # encode the sentences\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            sentence_pair,\n",
    "            add_special_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",  # Pad to the maximum length\n",
    "            max_length=self.max_length  # Set the maximum length\n",
    "        )\n",
    "        # get the token indices and attention mask\n",
    "        input_idx = encoded['input_ids']\n",
    "        attn_mask = encoded['attention_mask']\n",
    "        # convert start and end character positions to subword token positions\n",
    "        start_pos_enc = encoded.char_to_token(start_pos_char)\n",
    "        end_pos_enc = encoded.char_to_token(end_pos_char-1)\n",
    "\n",
    "        \n",
    "        if (start_pos_enc is None) or (end_pos_enc is None):\n",
    "            print(f\"start pos char: {start_pos_char}, end pos char: {end_pos_char}\")\n",
    "            print(f\"decoded sentence pair: {self.tokenizer.decode(input_idx)}\")\n",
    "            print(f\"answer span: {sentence_pair[0][start_pos_char:end_pos_char]}\")\n",
    "            raise Exception(f\"Start or end position is None! start_pos: {start_pos_enc}, end_pos: {end_pos_enc}\")\n",
    "        if len(input_idx) != self.max_length:\n",
    "            raise Exception(f\"Encoded input sequence length {len(input_idx)} is not equal to max_length {self.max_length}!\")\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        start_pos_enc = torch.tensor(start_pos_enc)\n",
    "        end_pos_enc = torch.tensor(end_pos_enc)\n",
    "        return input_idx, start_pos_enc, end_pos_enc, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(triplets_train)\n",
    "val_dataset = SquadDataset(triplets_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTExtractiveQA(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)      \n",
    "        # define two classifier heads, one for predicting start of span and another for end of span \n",
    "        self.classifier_head_start_span = torch.nn.Linear(hidden_size, 1)\n",
    "        self.classifier_head_end_span = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, labels_start, labels_end, attn_mask):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        # compute logits/scores over tokens for each of the classifier heads\n",
    "        logits_start = self.classifier_head_start_span(bert_output).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "        logits_end = self.classifier_head_end_span(bert_output).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits_start, labels_start) + F.cross_entropy(logits_end, labels_end) \n",
    "\n",
    "        return logits_start, logits_end, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets_start, targets_end, attn_mask = batch\n",
    "            # move batch to device\n",
    "            inputs, targets_start, targets_end, attn_mask = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits_start, logits_end, loss = model(inputs, targets_start, targets_end, attn_mask)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += ((y_pred_start.eq(targets_start.view(-1)) + y_pred_end.eq(targets_end.view(-1))) == 2).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets_start, targets_end, attn_mask = batch\n",
    "            inputs, targets_start, targets_end, attn_mask = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device)\n",
    "            logits_start, logits_end, loss = model(inputs, targets_start, targets_end, attn_mask)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += ((y_pred_start.eq(targets_start.view(-1)) + y_pred_end.eq(targets_end.view(-1))) == 2).sum().item()            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'qa_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('qa_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model with and without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 6661.90 MB\n"
     ]
    }
   ],
   "source": [
    "block_size = 256\n",
    "B = 64\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-3\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True) #, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True) #, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTExtractiveQA(finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256]) torch.Size([64]) torch.Size([64]) torch.Size([64, 256])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets_start, targets_end, attn_mask = next(iter(train_dataloader))\n",
    "print(inputs.shape, targets_start.shape, targets_end.shape, attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 6.158, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 1357/1357 [11:35<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start pos char: 351, end pos char: 403\n",
      "decoded sentence pair: [CLS] earlier they surrendered to the mongols, the higher they were placed, the more the held out, the lower they were ranked. the northern chinese were ranked higher and southern chinese were ranked lower because southern china withstood and fought to the last before caving in. major commerce during this era gave rise to favorable conditions for private southern chinese manufacturers and merchants. [SEP] who did the yuan's increase in commerce help? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "answer span: southern Chinese manufacturers and merchants.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Start or end position is None! start_pos: 66, end_pos: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[61], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, scheduler, device, num_epochs, val_every, save_every, log_metrics)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_every \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39mval_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;66;03m# compute validation loss\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m         val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, EMA Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m .3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_every \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[61], line 89\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(model, val_dataloader, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     88\u001b[0m num_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 89\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[33], line 34\u001b[0m, in \u001b[0;36mSquadDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoded sentence pair: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(input_idx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer span: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence_pair[\u001b[38;5;241m0\u001b[39m][start_pos_char:end_pos_char]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart or end position is None! start_pos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_pos_enc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, end_pos: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_pos_enc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_idx) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded input sequence length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_idx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not equal to max_length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Start or end position is None! start_pos: 66, end_pos: None"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=1, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=1, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
