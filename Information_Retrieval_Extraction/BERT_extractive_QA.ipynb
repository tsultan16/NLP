{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning BERT for `Extractive Question Answering` \n",
    "\n",
    "We will finetune a BERT model on the task of extractive QA, which involves taking a `factoid question` and a `context passage` of text and `labeling a span` of text from that passage which contains the `answer`. We can frame this as a `classification task`. First we concatenate the question and context passage pair, seperated by a `[SEP]` token. Then we compute the BERT encoding for this sequence. Then we apply a linear transform to each output token's encoding vector to compute a scalar score. By passing the scores from all tokens through a softmax, we obtain a `probability distribution` over tokens in the sequence, which we can interpret as the probability of a token being the start of the span. We actually will compute two separate linear transforms of all tokens and pass both sets of scores through a softmax to get two probability distributions over tokens, one for `start of span` and one for `end of span`. \n",
    "\n",
    "We will train this model on the SQuAD v1 dataset which contains passages with multiple questions and answer span pairs. We will use the cross entropy loss at the softmax output. To make predictions, we can simply just add up the scores of the `ith` token being the start and the `jth` token being the end for all i and j>i, then declare the (i,j) with the highest score as the predicted span.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login()\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data from file and then set up pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passages(squad, num_titles=None):\n",
    "    if num_titles is None:\n",
    "        num_titles = len(squad['data'])\n",
    "    # for each title, get passages and all corresponding questions from SQuAD train set\n",
    "    passages = []\n",
    "    questions = []\n",
    "    num_questions = 0\n",
    "    j = 0\n",
    "    for i in range(num_titles):\n",
    "        #print(f\"Title# {i}: {squad['data'][i]['title']}, Number of passages: {len(squad['data'][i]['paragraphs'])}\")\n",
    "        for p in squad['data'][i]['paragraphs']:\n",
    "            passages.append(p['context'])\n",
    "            for q in p['qas']:\n",
    "                if not q['is_impossible']:\n",
    "                    questions.append((q,j))    \n",
    "                    num_questions += 1\n",
    "            j += 1\n",
    "    print(f\"Number of passages: {len(passages)}\")\n",
    "    print(f\"Number of questions: {num_questions}\")\n",
    "    return passages, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 19035\n",
      "Number of questions: 86821\n",
      "Number of passages: 1204\n",
      "Number of questions: 5928\n"
     ]
    }
   ],
   "source": [
    "passages_train, questions_train = get_passages(squad_train)\n",
    "passages_val, questions_val = get_passages(squad_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max passage length: 3706, Avg passage length: 735.5478854741266\n"
     ]
    }
   ],
   "source": [
    "passage_lengths = [len(p) for p in passages_train]\n",
    "print(f\"Max passage length: {max(passage_lengths)}, Avg passage length: {sum(passage_lengths)/len(passage_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the context passages are very long (over 700 words on average) and won't fit into our BERT model (which can only take upto 512 tokens per sequence). So we will instead take a shorter fixed maximum length context window for each question.  \n",
    "\n",
    "Since we will use WordPiece tokenization, we also need to be careful about converting the character positions of the start and end of the span to subwork token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the first album Beyoncé released as a solo artist?\n",
      "Answer span: Dangerously in Love\n"
     ]
    }
   ],
   "source": [
    "q_idx = 10\n",
    "q = questions_train[q_idx][0]\n",
    "passage_idx = questions_train[q_idx][1]\n",
    "answer_start_pos = q['answers'][0]['answer_start']\n",
    "answer_end_pos = answer_start_pos + len(q['answers'][0]['text'])\n",
    "context = passages_train[passage_idx]\n",
    "\n",
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Special tokens with their integer id:\n",
      "{'[UNK]': 100, '[SEP]': 102, '[PAD]': 0, '[CLS]': 101, '[MASK]': 103}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSpecial tokens with their integer id:\")\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "special_tokens_to_ids = {t:tokenizer.convert_tokens_to_ids(t) for t in special_tokens}\n",
    "print(special_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window:  rmed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Bo\n",
      "Answer window: Dangerously in Love\n",
      "Answer window trimmed: Dangerously in Love\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n",
      "Decoded subword token span: dangerously in love\n",
      "Decoded sentence pair: [CLS] in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of r & b girl - group destiny's child. managed by her father, mathew knowles, the group became one of the world's best - selling girl groups of all time. their hiatus saw the release of beyonce's debut album, dangerously in love ( 2003 ), which established her as a solo artist worldwide, earned five grammy awards and featured the billboard hot 100 number - one singles \" crazy in love \" and \" baby [SEP] what was the first album beyonce released as a solo artist? [SEP]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Method 1: Create window directly on the context string, before tokenization.\n",
    "\"\"\"\n",
    "\n",
    "window_size_chars = 500\n",
    "# pick a random context window around the answer (try to keep at least 40% of the characters in window on the left side of the answer)\n",
    "answer_middle_pos = int((answer_start_pos+answer_end_pos)/2) \n",
    "a = max(0,answer_end_pos-window_size_chars)\n",
    "b = max(0, answer_start_pos - 0.4*window_size_chars)\n",
    "random_window_start_pos = random.randint(a, b)\n",
    "#window_start_pos = max(0,answer_middle_pos-window_size_chars)\n",
    "#window_end_pos = answer_middle_pos+window_size_chars\n",
    "window_start_pos = random_window_start_pos\n",
    "window_end_pos = window_start_pos+window_size_chars\n",
    "\n",
    "context_window = context[window_start_pos:window_end_pos]\n",
    "print(\"Context window: \", context_window)\n",
    "\n",
    "answer_start_pos_window = answer_start_pos - window_start_pos\n",
    "answer_end_pos_window = answer_start_pos_window + len(q['answers'][0]['text'])\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window: {answer_window}\")\n",
    "\n",
    "# Trim off stray partial words at the beginning and end\n",
    "context_window_words = context_window.split()\n",
    "# only trim if the first stray word does not overlap with the answer span\n",
    "if answer_start_pos_window > len(context_window_words[0]):\n",
    "    context_window = ' '.join(context_window_words[1:-1])\n",
    "    left_trim_length = len(context_window_words[0]) + 1 # add 1 for the white space between stary partial first word and next word\n",
    "    answer_start_pos_window = answer_start_pos_window - left_trim_length\n",
    "    answer_end_pos_window = answer_end_pos_window - left_trim_length\n",
    "\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window trimmed: {answer_window}\")\n",
    "\n",
    "\n",
    "# encode the passage\n",
    "context_encoded = tokenizer.encode_plus((context_window, q['question']), add_special_tokens=True, return_offsets_mapping=True)\n",
    "print(context_encoded.keys())\n",
    "# convert character positions from original sentence to subword token positions\n",
    "start_pos_enc = context_encoded.char_to_token(answer_start_pos_window)\n",
    "end_pos_enc = context_encoded.char_to_token(answer_end_pos_window-1)\n",
    "# get the corresponding subword token span\n",
    "answer_span_encoded = context_encoded['input_ids'][start_pos_enc:end_pos_enc+1]\n",
    "# decode the span to check if it matches original answer span\n",
    "print(f\"Decoded subword token span: {tokenizer.decode(answer_span_encoded)}\")\n",
    "print(f\"Decoded sentence pair: {tokenizer.decode(context_encoded['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was the first album Beyoncé released as a solo artist?\n",
      "Context: Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "Answer span: Dangerously in Love\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subword tokens: ['beyonce', 'gi', '##selle', 'knowles', '-', 'carter', '(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/', 'bee', '-', 'yo', '##n', '-', 'say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl', '-', 'group', 'destiny', \"'\", 's', 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'\", 's', 'best', '-', 'selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyonce', \"'\", 's', 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number', '-', 'one', 'singles', '\"', 'crazy', 'in', 'love', '\"', 'and', '\"', 'baby', 'boy', '\"', '.']\n",
      "Answer start char: 505, Answer end char: 524\n",
      "Answer start token: 123, Answer end token: 125\n",
      "start min: 116, start max: 123\n",
      "Random window start: 123, window end: 133\n",
      "window tokens: ['dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her']\n",
      "window answer start token: 0, window answer end token: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  \n",
    "    Method 2: Tokenize first, then create context window around answer span. (Cleaner than method 1). We also add some randomness to the window selection, this will also give a regularization effect during training time as the model will see different context windows for the same question.\n",
    "\"\"\"\n",
    "\n",
    "# tokenize the context passage, get offset mapping\n",
    "encoding = tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "input_ids = encoding['input_ids']\n",
    "#offset_mapping = encoding['offset_mapping']\n",
    "print(f\"Subword tokens: {tokenizer.convert_ids_to_tokens(input_ids)}\")\n",
    "#print(f\"Offset mapping: {offset_mapping}\")\n",
    "\n",
    "# answer span start and end character positions\n",
    "answer_start_char = q['answers'][0]['answer_start']\n",
    "answer_end_char = answer_start_char + len(q['answers'][0]['text'])\n",
    "print(f\"Answer start char: {answer_start_char}, Answer end char: {answer_end_char}\")    \n",
    "\n",
    "# convert char positions to token positions\n",
    "answer_start_token = encoding.char_to_token(answer_start_char)\n",
    "answer_end_token = encoding.char_to_token(answer_end_char-1)\n",
    "print(f\"Answer start token: {answer_start_token}, Answer end token: {answer_end_token}\")    \n",
    "\n",
    "# now create a window around the answer span, pick the window start position randomly\n",
    "window_size_tokens = 10\n",
    "\n",
    "# range of legal starting positions\n",
    "start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "start_max = answer_start_token\n",
    "print(f\"start min: {start_min}, start max: {start_max}\")\n",
    "\n",
    "window_start = random.randint(start_min, start_max)\n",
    "window_end = window_start + window_size_tokens\n",
    "print(f\"Random window start: {window_start}, window end: {window_end}\")\n",
    "\n",
    "# select window of tokens\n",
    "window_tokens = input_ids[window_start:window_end]\n",
    "print(f\"window tokens: {tokenizer.convert_ids_to_tokens(window_tokens)}\")\n",
    "\n",
    "# offset the answer span token positions by window start position\n",
    "answer_start_token_window = answer_start_token - window_start\n",
    "answer_end_token_window = answer_end_token - window_start\n",
    "print(f\"window answer start token: {answer_start_token_window}, window answer end token: {answer_end_token_window}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'what', 'was', 'the', 'first', 'album', 'beyonce', 'released', 'as', 'a', 'solo', 'artist', '?', '[SEP]', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "question_encoding = tokenizer.encode_plus(q['question'], add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "question_idx = question_encoding['input_ids']        \n",
    "input_idx = [special_tokens_to_ids[\"[CLS]\"]] + question_idx + [special_tokens_to_ids[\"[SEP]\"]] + window_tokens + [special_tokens_to_ids[\"[SEP]\"]]   \n",
    "print(tokenizer.convert_ids_to_tokens(input_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's creating a pytorch dataset that handles the tokenization and context creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need this because huggingface tokenizer is not thread safe when using it inside __getitem__ instead of dataloader collatefunction\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  #\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, passages, questions, block_size=128):\n",
    "        self.passages = passages\n",
    "        self.questions = questions\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the question and context passage\n",
    "        q = self.questions[idx][0]\n",
    "        passage_idx = self.questions[idx][1]\n",
    "        question = q['question']\n",
    "        context = self.passages[passage_idx]\n",
    "        # tokenize the context passage\n",
    "        context_encoding = tokenizer.encode_plus(context, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        context_idx = context_encoding['input_ids']\n",
    "        # tokenize the question\n",
    "        question_encoding = tokenizer.encode_plus(question, add_special_tokens=False, return_offsets_mapping=False, return_attention_mask=False, return_token_type_ids=False)\n",
    "        question_idx = question_encoding['input_ids']\n",
    "\n",
    "        # get answer span start and end character positions, for multiple answers, we will only use the first answer\n",
    "        first_answer_idx = 0\n",
    "        answer_start_char = q['answers'][first_answer_idx]['answer_start']\n",
    "        answer_end_char = answer_start_char + len(q['answers'][first_answer_idx]['text'])\n",
    "        # convert char positions to token positions\n",
    "        answer_start_token = context_encoding.char_to_token(answer_start_char)\n",
    "        answer_end_token = context_encoding.char_to_token(answer_end_char-1)\n",
    "\n",
    "        # select a window size so that the input sequence is not longer than block size\n",
    "        window_size_tokens = self.block_size - len(question_idx) - 3 # 3 special tokens ([CLS], [SEP], [SEP])\n",
    "        # now create a window around the answer span, pick the window start position randomly\n",
    "        window_start_min = max(0, answer_end_token - window_size_tokens + 1)\n",
    "        window_start_max = min(answer_start_token, max(0,len(context_idx) - window_size_tokens)) # we want to make the window as large as possible, but not go over the end of the context\n",
    "        window_start = random.randint(window_start_min, window_start_max)\n",
    "        window_end = window_start + window_size_tokens\n",
    "        \n",
    "        # select window of tokens\n",
    "        window_tokens = context_idx[window_start:window_end]\n",
    "        # offset the answer span token positions by window start position\n",
    "        answer_start_token_window = answer_start_token - window_start\n",
    "        answer_end_token_window = answer_end_token - window_start\n",
    "        # concatenate the question and context, add special tokens and padding\n",
    "        input_idx = [self.tokenizer.cls_token_id] + question_idx + [self.tokenizer.sep_token_id] + window_tokens + [self.tokenizer.sep_token_id]\n",
    "        # make sure the input sequence is not longer than max_length\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise Exception(f\"Input sequence length {len(input_idx)} is longer than max_length {self.block_size}!\")\n",
    "\n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id]*(self.block_size-len(input_idx))\n",
    "        # offset the answer span token positions again by the length of the question and the two special tokens ([CLS] and [SEP])\n",
    "        answer_start_token_window = answer_start_token_window + len(question_idx) + 2\n",
    "        answer_end_token_window = answer_end_token_window + len(question_idx) + 2\n",
    "        # create attention mask\n",
    "        attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "        # create segment ids\n",
    "        segment_idx = [0]*(len(question_idx)+2) + [1]*(self.block_size-len(question_idx)-2)\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        segment_idx = torch.tensor(segment_idx)\n",
    "        start_pos_enc = torch.tensor(answer_start_token_window)\n",
    "        end_pos_enc = torch.tensor(answer_end_token_window)\n",
    "        return input_idx, start_pos_enc, end_pos_enc, attn_mask, segment_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(passages_train, questions_train, block_size=256)\n",
    "val_dataset = SquadDataset(passages_val, questions_val, block_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input idx: ['[CLS]', 'as', 'of', 'april', '2014', ',', 'how', 'many', 'albums', 'have', 'jay', 'z', 'and', 'beyonce', 'sold', 'together', '?', '[SEP]', 'beyonce', 'is', 'believed', 'to', 'have', 'first', 'started', 'a', 'relationship', 'with', 'jay', 'z', 'after', 'a', 'collaboration', 'on', '\"', \"'\", '03', 'bonnie', '&', 'clyde', '\"', ',', 'which', 'appeared', 'on', 'his', 'seventh', 'album', 'the', 'blue', '##print', '2', ':', 'the', 'gift', '&', 'the', 'curse', '(', '2002', ')', '.', 'beyonce', 'appeared', 'as', 'jay', 'z', \"'\", 's', 'girlfriend', 'in', 'the', 'music', 'video', 'for', 'the', 'song', ',', 'which', 'would', 'further', 'fuel', 'speculation', 'of', 'their', 'relationship', '.', 'on', 'april', '4', ',', '2008', ',', 'beyonce', 'and', 'jay', 'z', 'were', 'married', 'without', 'publicity', '.', 'as', 'of', 'april', '2014', ',', 'the', 'couple', 'have', 'sold', 'a', 'combined', '300', 'million', 'records', 'together', '.', 'the', 'couple', 'are', 'known', 'for', 'their', 'private', 'relationship', ',', 'although', 'they', 'have', 'appeared', 'to', 'become', 'more', 'relaxed', 'in', 'recent', 'years', '.', 'beyonce', 'suffered', 'a', 'mis', '##carriage', 'in', '2010', 'or', '2011', ',', 'describing', 'it', 'as', '\"', 'the', 'sad', '##des', '##t', 'thing', '\"', 'she', 'had', 'ever', 'endured', '.', 'she', 'returned', 'to', 'the', 'studio', 'and', 'wrote', 'music', 'in', 'order', 'to', 'cope', 'with', 'the', 'loss', '.', 'in', 'april', '2011', ',', 'beyonce', 'and', 'jay', 'z', 'traveled', 'to', 'paris', 'in', 'order', 'to', 'shoot', 'the', 'album', 'cover', 'for', 'her', '4', ',', 'and', 'unexpectedly', 'became', 'pregnant', 'in', 'paris', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Answer span: ['300', 'million']\n"
     ]
    }
   ],
   "source": [
    "idx = 323\n",
    "input_idx, start_pos_enc, end_pos_enc, attn_mask, segment_idx = train_dataset[idx]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_idx)\n",
    "print(f\"Input idx: {input_tokens}\")\n",
    "print(f\"Answer span: {input_tokens[start_pos_enc:end_pos_enc+1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTExtractiveQA(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)      \n",
    "        # define two classifier heads, one for predicting start of span and another for end of span \n",
    "        self.classifier_head_start_span = torch.nn.Linear(hidden_size, 1)\n",
    "        self.classifier_head_end_span = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, attn_mask, segment_idx, labels_start=None, labels_end=None):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask, token_type_ids=segment_idx)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        # compute logits/scores over tokens for each of the classifier heads\n",
    "        logits_start = self.classifier_head_start_span(bert_output).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "        logits_end = self.classifier_head_end_span(bert_output).squeeze(-1)  # shape: (batch_size, sequence_length)\n",
    "        if labels_start is None or labels_end is None:\n",
    "            return logits_start, logits_end \n",
    "        \n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits_start, labels_start) + F.cross_entropy(logits_end, labels_end) \n",
    "        return logits_start, logits_end, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc_start = 0\n",
    "    train_acc_end = 0\n",
    "    val_loss = 0\n",
    "    val_acc_start = 0\n",
    "    val_acc_end = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct_start = 0\n",
    "        num_correct_end = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets_start, targets_end, attn_mask, segment_idx = batch\n",
    "            # move batch to device\n",
    "            inputs, targets_start, targets_end, attn_mask, segment_idx = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device), segment_idx.to(device)\n",
    "            # forward pass\n",
    "            logits_start, logits_end, loss = model(inputs, attn_mask, segment_idx, targets_start, targets_end)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct_start += (y_pred_start.eq(targets_start.view(-1))).sum().item()            \n",
    "            num_correct_end += (y_pred_end.eq(targets_end.view(-1))).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc_start = num_correct_start / num_total        \n",
    "            train_acc_end = num_correct_end / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy start: {train_acc_start: .3f}, Train Accuracy end: {train_acc_end: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy start: {val_acc_start: .3f}, Val Accuracy end: {val_acc_end: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc_start, val_acc_end = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy start: {train_acc_start: .3f}, Train Accuracy end: {train_acc_end: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy start: {val_acc_start: .3f}, Val Accuracy end: {val_acc_end: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "# predict answer span for a given input\n",
    "def predict(model, input_idx, attn_mask, segment_idx, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_idx, attn_mask, segment_idx = input_idx.to(device), attn_mask.to(device), segment_idx.to(device)\n",
    "        logits_start, logits_end = model(input_idx.unsqueeze(0), attn_mask.unsqueeze(0), segment_idx.unsqueeze(0))\n",
    "        logits_start = logits_start.view(-1) # shape (seq_len,)\n",
    "        logits_end = logits_end.view(-1) # shape (seq_len,)\n",
    "        # find the highest scoring span\n",
    "        max_score = -float('inf')\n",
    "        best_span = None\n",
    "        for i in range(len(logits_start)):\n",
    "            for j in range(i, len(logits_end)):\n",
    "                score = logits_start[i] + logits_end[j]\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    best_span = (i,j)\n",
    "                    \n",
    "    model.train()\n",
    "    return best_span, max_score.item() \n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct_start = 0\n",
    "        num_correct_end = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets_start, targets_end, attn_mask, segment_idx = batch\n",
    "            inputs, targets_start, targets_end, attn_mask, segment_idx = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device), segment_idx.to(device)\n",
    "            logits_start, logits_end, loss = model(inputs, attn_mask, segment_idx, targets_start, targets_end)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct_start += (y_pred_start.eq(targets_start.view(-1))).sum().item()            \n",
    "            num_correct_end += (y_pred_end.eq(targets_end.view(-1))).sum().item()             \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy_start = num_correct_start / num_total\n",
    "    val_accuracy_end = num_correct_end / num_total\n",
    "    return val_loss, val_accuracy_start, val_accuracy_end\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'qa_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('qa_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 2772.11 MB\n"
     ]
    }
   ],
   "source": [
    "block_size = 256\n",
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-6\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTExtractiveQA(finetune=True).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256]) torch.Size([16]) torch.Size([16]) torch.Size([16, 256]) torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "inputs, targets_start, targets_end, attn_mask, segment_idx = next(iter(train_dataloader))\n",
    "print(inputs.shape, targets_start.shape, targets_end.shape, attn_mask.shape, segment_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/5427 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (622 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=5, save_every=5, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (733 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (644 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.140028715133667, 0.6993927125506073, 0.7199730094466936)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #0\n",
      "\n",
      "Question: \tin what field were double and triple expansion engines common?\n",
      "Context: \tefficiency. these stages were called expansions, with double and triple expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating ( piston ) steam engines, with shipping in the 20th - century relying upon the steam turbine.\n",
      "Actual Answer span:    \tshipping\n",
      "Predicted Answer span: \tshipping\n",
      "Example #1\n",
      "\n",
      "Question: \tstephen eildmann cites the oldest known example of civil disobedience in what part of the bible?\n",
      "Context: \tbook of exodus, where shiphrah and puah refused a direct order of pharaoh but misrepresented how they did it. ( exodus 1 : 15 - 19 )\n",
      "Actual Answer span:    \t( exodus 1 : 15 - 19 )\n",
      "Predicted Answer span: \texodus\n",
      "Example #2\n",
      "\n",
      "Question: \twhat are the phagocytes that are located in tissues in contact with the external environment called?\n",
      "Context: \tdendritic cells ( dc ) are phagocytes in tissues that are in contact with the external environment ; therefore, they are located mainly in the skin, nose, lungs, stomach, and intestines. they are named for their resemblance to neuronal dendrites, as both have many spine - like projections, but dendritic cells are in no way connected to the nervous system. dendritic cells serve as a link between the bodily tissues and the innate and adaptive immune systems, as they present antigens\n",
      "Actual Answer span:    \tdendritic cells\n",
      "Predicted Answer span: \tdendritic cells\n",
      "Example #3\n",
      "\n",
      "Question: \twhat typically involves mass production of similar items without a designated purchaser?\n",
      "Context: \tof constructing a building or infrastructure. construction differs from manufacturing in that manufacturing typically involves mass production of similar items without a designated purchaser, while construction typically takes place on location for a known client. construction as an industry comprises six to nine percent of the gross domestic product of developed countries. construction starts with planning, [ citation needed ] design, and financing and continues until the project is built and ready for use.\n",
      "Actual Answer span:    \tmanufacturing\n",
      "Predicted Answer span: \tmanufacturing\n",
      "Example #4\n",
      "\n",
      "Question: \twhere did the huguenots land in new york originally?\n",
      "Context: \tthe north shore of long island sound, seemed to be the great location of the huguenots in new york. it is said that they landed on the coastline peninsula of davenports neck called \" bauffet's point \" after traveling from england where they had previously taken refuge on account of religious persecution, four years before the revocation of the edict of nantes. they purchased from john pell, lord of pelham manor, a tract of land consisting of six thousand one hundred acres with the help of jacob leisler. it was named new rochelle after\n",
      "Actual Answer span:    \t\" bauffet's point \"\n",
      "Predicted Answer span: \tthe north shore of long island sound\n",
      "Example #5\n",
      "\n",
      "Question: \tthe prospect of what event compelled the protection of german private schools?\n",
      "Context: \tunusual protection of private schools was implemented to protect these schools from a second gleichschaltung or similar event in the future. still, they are less common than in many other countries. overall, between 1992 and 2008 the percent of pupils in such schools in germany increased from 6. 1 % to 7. 8 % ( including rise from 0. 5 % to 6. 1 % in the former gdr ). percent of students in private high schools reached 11. 1 %.\n",
      "Actual Answer span:    \tsecond gleichschaltung\n",
      "Predicted Answer span: \ta second gleichschaltung\n",
      "Example #6\n",
      "\n",
      "Question: \twho did bskyb team up with because it was not part of the consortium?\n",
      "Context: \tfree - to - air replacement, freeview, in which it holds an equal stake with the bbc, itv, channel 4 and national grid wireless. prior to october 2005, three bskyb channels were available on this platform : sky news, sky three, and sky sports news. initially bskyb provided sky travel to the service. however, this was replaced by sky three on 31 october 2005, which was itself later re - branded as'pick tv'in 2011.\n",
      "Actual Answer span:    \tfreeview\n",
      "Predicted Answer span: \tthe bbc, itv, channel 4 and national grid wireless\n",
      "Example #7\n",
      "\n",
      "Question: \twhat stature did pharmacists have in the pre - heian imperial court?\n",
      "Context: \tacist assistants — were assigned status superior to all others in health - related fields such as physicians and acupuncturists. in the imperial household, the pharmacist was even ranked above the two personal physicians of the emperor.\n",
      "Actual Answer span:    \tstatus superior to all others in health - related fields such as physicians and acupuncturists\n",
      "Predicted Answer span: \tstatus superior to all others in health - related fields such as physicians and acupuncturists. in the imperial household, the pharmacist was even ranked above the two personal physicians of the emperor\n",
      "Example #8\n",
      "\n",
      "Question: \tthe speed of the killing response of the human immune system is a product of what process?\n",
      "Context: \trecognition signal triggers a rapid killing response. the speed of the response is a result of signal amplification that occurs following sequential proteolytic activation of complement molecules, which are also proteases. after complement proteins initially bind to the microbe, they activate their protease activity, which in turn activates other complement proteases, and so on. this produces a catalytic cascade that amplifies the initial signal by controlled positive feedback. the cascade results in the production of peptides that attract immune cells, increase vascular\n",
      "Actual Answer span:    \tsignal amplification\n",
      "Predicted Answer span: \tsignal amplification\n",
      "Example #9\n",
      "\n",
      "Question: \thow long did western europe control cyprus?\n",
      "Context: \tof the island, which would be under western european domination for the following 380 years. although not part of a planned operation, the conquest had much more permanent results than initially expected.\n",
      "Actual Answer span:    \t380 years\n",
      "Predicted Answer span: \t380 years\n"
     ]
    }
   ],
   "source": [
    "# let's test span prediction on some randomly selected exampes from the validation set\n",
    "for i in range(10):\n",
    "    print(f\"Example #{i}\")\n",
    "    idx = random.randint(0,len(val_dataset)-1)\n",
    "    input_idx, start_pos_enc, end_pos_enc, attn_mask, segment_idx = val_dataset[idx] \n",
    "    input_string = tokenizer.decode(input_idx[1:-1])\n",
    "    sentences = input_string.split('[SEP]')\n",
    "    question = sentences[0].strip()\n",
    "    context = sentences[1].strip()\n",
    "    print(f\"\\nQuestion: \\t{question}\")\n",
    "    print(f\"Context: \\t{context}\")\n",
    "    input_tokens = tokenizer.convert_ids_to_tokens(input_idx)\n",
    "    #print(f\"Input tokens: {input_tokens}\")\n",
    "    (start_pos_pred, end_pos_pred), score = predict(model, input_idx, attn_mask, segment_idx, device=DEVICE)\n",
    "    #print(f\"\\nGold span pos: {(start_pos_enc.item(), end_pos_enc.item())}\")\n",
    "    #print(f\"Predicted span pos: {(start_pos_pred, end_pos_pred)}\")\n",
    "    gold_span = tokenizer.decode(input_idx[start_pos_enc:end_pos_enc+1])\n",
    "    predicted_span = tokenizer.decode(input_idx[start_pos_pred:end_pos_pred+1])\n",
    "    print(f\"Actual Answer span:    \\t{gold_span}\")\n",
    "    print(f\"Predicted Answer span: \\t{predicted_span}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
