{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning BERT for `Extractive Question Answering` \n",
    "\n",
    "We will finetune a BERT model on the task of extractive QA, which involves taking a `factoid question` and a `context passage` of text and `labeling a span` of text from that passage which contains the `answer`. We can frame this as a `classification task`. First we concatenate the question and context passage pair, seperated by a `[SEP]` token. Then we compute the BERT encoding for this sequence. Then we apply a linear transform to each output token's encoding vector to compute a scalar score. By passing the scores from all tokens through a softmax, we obtain a `probability distribution` over tokens in the sequence, which we can interpret as the probability of a token being the start of the span. We actually will compute two separate linear transforms of all tokens and pass both sets of scores through a softmax to get two probability distributions over tokens, one for `start of span` and one for `end of span`. \n",
    "\n",
    "We will train this model on the SQuAD v1 dataset which contains passages with multiple questions and answer span pairs. We will use the cross entropy loss at the softmax output. To make predictions, we can simply just add up the scores of the `ith` token being the start and the `jth` token being the end for all i and j>i, then declare the (i,j) with the highest score as the predicted span.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import json\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data from file and then set up pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the train and dev JSON documents\n",
    "with open(\"train.json\", \"r\") as train_file:\n",
    "    squad_train = json.load(train_file)         \n",
    "with open(\"dev.json\", \"r\") as dev_file:\n",
    "    squad_dev = json.load(dev_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passages(squad, num_titles=None):\n",
    "    if num_titles is None:\n",
    "        num_titles = len(squad['data'])\n",
    "    # for each title, get passages and all corresponding questions from SQuAD train set\n",
    "    passages = []\n",
    "    questions = []\n",
    "    num_questions = 0\n",
    "    for i in range(num_titles):\n",
    "        #print(f\"Title# {i}: {squad['data'][i]['title']}, Number of passages: {len(squad['data'][i]['paragraphs'])}\")\n",
    "        for p in squad['data'][i]['paragraphs']:\n",
    "            # we will append the title to each passage and question\n",
    "            passages.append(p['context'])\n",
    "            questions.append([q for q in p['qas']])    \n",
    "            num_questions += len(p['qas'])\n",
    "    print(f\"Number of passages: {len(passages)}\")\n",
    "    print(f\"Number of questions: {num_questions}\")\n",
    "    return passages, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of passages: 19035\n",
      "Number of questions: 130319\n",
      "Number of passages: 1204\n",
      "Number of questions: 11873\n"
     ]
    }
   ],
   "source": [
    "passages_train, questions_train = get_passages(squad_train)\n",
    "passages_val, questions_val = get_passages(squad_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max passage length: 3706, Avg passage length: 735.5478854741266\n"
     ]
    }
   ],
   "source": [
    "passage_lengths = [len(p) for p in passages_train]\n",
    "print(f\"Max passage length: {max(passage_lengths)}, Avg passage length: {sum(passage_lengths)/len(passage_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the context passaged are very long (over 700 words on average) and won't fit into our BERT model (which can only take upto 512 tokens per sequence). So we will instead take a shorter fixed size context window for each question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many awards was Beyonce nominated for at the 52nd Grammy Awards?\n",
      "Answer span: ten\n"
     ]
    }
   ],
   "source": [
    "q = questions_train[16][0]\n",
    "answer_start_pos = q['answers'][0]['answer_start']\n",
    "answer_end_pos = answer_start_pos + len(q['answers'][0]['text'])\n",
    "context = passages_train[16]\n",
    "\n",
    "print(f\"Question: {q['question']}\")\n",
    "print(f\"Answer span: {context[answer_start_pos:answer_end_pos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context window:  At the 52nd Annual Grammy Awards, Beyoncé received ten nominations, including Album of the Year for I Am... Sasha Fierce, Record of the Year for \"Halo\", and Song of the Year for \"Single Ladies (Put a Ring on It)\", among others. She tied with Lauryn Hill for most Grammy nominations in a single year by a female artist. In 2010, Beyoncé was featured on Lady Gaga's single \"Telephone\" and its music video. The song topped the US Pop Songs chart, becoming the sixth number-one for both Beyoncé and Gaga,\n",
      "Answer window: ten\n",
      "Answer window trimmed: ten\n"
     ]
    }
   ],
   "source": [
    "window_size_chars = 500\n",
    "# pick a random context window around the answer (try to keep at least 40% of the characters in window on the left side of the answer)\n",
    "answer_middle_pos = int((answer_start_pos+answer_end_pos)/2) \n",
    "a = max(0,answer_end_pos-window_size_chars)\n",
    "b = max(0, answer_start_pos - 0.4*window_size_chars)\n",
    "random_window_start_pos = random.randint(a, b)\n",
    "#window_start_pos = max(0,answer_middle_pos-window_size_chars)\n",
    "#window_end_pos = answer_middle_pos+window_size_chars\n",
    "window_start_pos = random_window_start_pos\n",
    "window_end_pos = window_start_pos+window_size_chars\n",
    "\n",
    "context_window = context[window_start_pos:window_end_pos]\n",
    "print(\"Context window: \", context_window)\n",
    "\n",
    "answer_start_pos_window = answer_start_pos - window_start_pos\n",
    "answer_end_pos_window = answer_start_pos_window + len(q['answers'][0]['text'])\n",
    "answer_window = context_window[answer_start_pos_window:answer_end_pos_window]\n",
    "print(f\"Answer window: {answer_window}\")\n",
    "\n",
    "# Trim off stray partial words at the beginning and end\n",
    "context_window_words = context_window.split()\n",
    "# only trim if the first stray word does not overlap with the answer span\n",
    "if answer_start_pos_window > len(context_window_words[0]):\n",
    "    context_window = ' '.join(context_window_words[1:-1])\n",
    "    left_trim_length = len(context_window_words[0]) + 1 # add 1 for the white space between stary partial first word and next word\n",
    "    start_pos_window = answer_start_pos_window - left_trim_length\n",
    "    end_pos_window = answer_end_pos_window - left_trim_length\n",
    "\n",
    "start_pos_window = answer_start_pos_window - left_trim_length\n",
    "end_pos_window = answer_end_pos_window - left_trim_length\n",
    "answer_window = context_window[start_pos_window:end_pos_window]\n",
    "print(f\"Answer window trimmed: {answer_window}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will use WordPiece tokenization, we need to be careful about converting the character positions of the start and end of the span to subwork token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n",
      "Decoded subword token span: ten\n",
      "Decoded sentence pair: [CLS] the 52nd annual grammy awards, beyonce received ten nominations, including album of the year for i am... sasha fierce, record of the year for \" halo \", and song of the year for \" single ladies ( put a ring on it ) \", among others. she tied with lauryn hill for most grammy nominations in a single year by a female artist. in 2010, beyonce was featured on lady gaga's single \" telephone \" and its music video. the song topped the us pop songs chart, becoming the sixth number - one for both beyonce and [SEP] how many awards was beyonce nominated for at the 52nd grammy awards? [SEP]\n"
     ]
    }
   ],
   "source": [
    "# encode the passage\n",
    "context_encoded = tokenizer.encode_plus((context_window, q['question']), add_special_tokens=True, return_offsets_mapping=True)\n",
    "print(context_encoded.keys())\n",
    "# convert character positions from original sentence to subword token positions\n",
    "start_pos_enc = context_encoded.char_to_token(start_pos_window)\n",
    "end_pos_enc = context_encoded.char_to_token(end_pos_window-1)\n",
    "# get the corresponding subword token span\n",
    "answer_span_encoded = context_encoded['input_ids'][start_pos_enc:end_pos_enc+1]\n",
    "# decode the span to check if it matches original answer span\n",
    "print(f\"Decoded subword token span: {tokenizer.decode(answer_span_encoded)}\")\n",
    "print(f\"Decoded sentence pair: {tokenizer.decode(context_encoded['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create question, trimmed context window and answer span instances. Note: some quesitions may contain empty answers list, those will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating instances\n",
    "def get_instances(passages, questions, window_size_chars=800):\n",
    "    triplets = []\n",
    "    for i, context in enumerate(passages):\n",
    "        for q in questions[i]:\n",
    "            for a in q['answers']:\n",
    "                # get start and end character positions of answer span \n",
    "                answer_start_pos = a['answer_start']\n",
    "                answer_end_pos = answer_start_pos + len(a['text'])\n",
    "                # get windowed context\n",
    "                a = max(0,answer_end_pos-window_size_chars)\n",
    "                b = max(0, answer_start_pos - 0.4*window_size_chars)\n",
    "                window_start_pos = random.randint(a, b)\n",
    "                window_end_pos = window_start_pos+window_size_chars\n",
    "                context_window = context[window_start_pos:window_end_pos]\n",
    "                # offset the answer span positions\n",
    "                answer_start_pos_window = answer_start_pos - window_start_pos\n",
    "                answer_end_pos_window = answer_start_pos_window + len(q['answers'][0]['text'])\n",
    "                # Trim off stray partial words at the beginning and end of window\n",
    "                context_window_words = context_window.split()\n",
    "                # only trim if the first stray word does not overlap with the answer span\n",
    "                if answer_start_pos_window > len(context_window_words[0]):\n",
    "                    context_window = ' '.join(context_window_words[1:-1])\n",
    "                    left_trim_length = len(context_window_words[0]) + 1 # add 1 for the white space between stary partial first word and next word\n",
    "                    start_pos_window = answer_start_pos_window - left_trim_length\n",
    "                    end_pos_window = answer_end_pos_window - left_trim_length\n",
    "                \n",
    "                if start_pos_window < 0 or end_pos_window < 0:\n",
    "                    print(f\"Context: {context}\")\n",
    "                    print(f\"Question: {q}\")\n",
    "                    print(f\"answer_start_pos: {answer_start_pos}, answer_end_pos: {answer_end_pos}\")    \n",
    "                    print(f\"context char length: {len(context)}, window start pos: {window_start_pos}, window end pos: {window_end_pos}\")\n",
    "                    print(f\"answer start pos window: {answer_start_pos_window}, answer end pos window: {answer_end_pos_window}\")    \n",
    "                    print(f\"Context trimmed: {context_window}\")\n",
    "                    raise Exception(f\"Trimmed answer span is negative! start_pos: {start_pos_window}, end_pos: {end_pos_window}\")\n",
    "                \n",
    "                triplets.append(((context_window, q['question']), start_pos_window, end_pos_window))\n",
    "    return triplets            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_train = get_instances(passages_train, questions_train)\n",
    "triplets_val = get_instances(passages_val, questions_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, data, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the data instance\n",
    "        sentence_pair = self.data[idx][0]\n",
    "        start_pos_char = self.data[idx][1]\n",
    "        end_pos_char = self.data[idx][2]\n",
    "        # encode the sentences\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            sentence_pair,\n",
    "            add_special_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",  # Pad to the maximum length\n",
    "            max_length=self.max_length  # Set the maximum length\n",
    "        )\n",
    "        # get the token indices and attention mask\n",
    "        input_idx = encoded['input_ids']\n",
    "        attn_mask = encoded['attention_mask']\n",
    "        # convert start and end character positions to subword token positions\n",
    "        start_pos_enc = encoded.char_to_token(start_pos_char)\n",
    "        end_pos_enc = encoded.char_to_token(end_pos_char-1)\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        attn_mask = torch.tensor(attn_mask)\n",
    "        start_pos_enc = torch.tensor(start_pos_enc)\n",
    "        end_pos_enc = torch.tensor(end_pos_enc)\n",
    "        return input_idx, start_pos_enc, end_pos_enc, attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(triplets_train)\n",
    "val_dataset = SquadDataset(triplets_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now define the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTExtractiveQA(torch.nn.Module):\n",
    "    def __init__(self, block_size, hidden_size=768, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)      \n",
    "        # define two classifier heads, one for predicting start of span and another for end of span \n",
    "        num_classes = block_size\n",
    "        self.classifier_head_start_span = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.classifier_head_end_span = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, labels_start, labels_end, attn_mask):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        # compute logits/scores over tokens for each of the classifier heads\n",
    "        logits_start = self.classifier_head_start_span(bert_output)  # shape: (batch_size, sequence_length)\n",
    "        logits_end = self.classifier_head_end_span(bert_output)  # shape: (batch_size, sequence_length)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits_start, labels_start) + F.cross_entropy(logits_end, labels_end) \n",
    "\n",
    "        return logits_start, logits_end, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets_start, targets_end, attn_mask = batch\n",
    "            # move batch to device\n",
    "            inputs, targets_start, targets_end, attn_mask = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits_start, logits_end, loss = model(inputs, targets_start, targets_end, attn_mask)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += ((y_pred_start.eq(targets_start.view(-1)) + y_pred_end.eq(targets_end.view(-1))) == 2).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets_start, targets_end, attn_mask = batch\n",
    "            inputs, targets_start, targets_end, attn_mask = inputs.to(device), targets_start.to(device), targets_end.to(device), attn_mask.to(device)\n",
    "            logits_start, logits_end, loss = model(inputs, targets_start, targets_end, attn_mask)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred_start = logits_start.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            y_pred_end = logits_end.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += ((y_pred_start.eq(targets_start.view(-1)) + y_pred_end.eq(targets_end.view(-1))) == 2).sum().item()            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'qa_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('qa_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model with and without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.875968 M\n",
      "RAM used: 2869.18 MB\n"
     ]
    }
   ],
   "source": [
    "block_size = 256\n",
    "B = 64\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-3\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTExtractiveQA(block_size, finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1357 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epochs:   0%|          | 0/1357 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OverflowError",
     "evalue": "Caught OverflowError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_246840/3506256130.py\", line 27, in __getitem__\n    start_pos_enc = encoded.char_to_token(start_pos_char)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 594, in char_to_token\n    return self._encodings[batch_index].char_to_token(char_index, sequence_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOverflowError: can't convert negative int to unsigned\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[14], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_dataloader, val_dataloader, scheduler, device, num_epochs, val_every, save_every, log_metrics)\u001b[0m\n\u001b[1;32m     42\u001b[0m num_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     43\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# move batch to device\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOverflowError\u001b[0m: Caught OverflowError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_246840/3506256130.py\", line 27, in __getitem__\n    start_pos_enc = encoded.char_to_token(start_pos_char)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tanzid/miniconda3/envs/torch_clone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\", line 594, in char_to_token\n    return self._encodings[batch_index].char_to_token(char_index, sequence_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nOverflowError: can't convert negative int to unsigned\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=1, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
