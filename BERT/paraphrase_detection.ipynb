{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning BERT for Paraphrase Detection\n",
    "\n",
    "Similar to how we finetuned a BERT model to perform sentiment analysis on a single sentence, we can also perform paraphrase detection on a pair of sentences. We feed in an input sequence containing a contatenation of both sentences seperated by the `[SEP]` token. Then we can perform binary classification using the encoded `[CLS]` token exactly like how we did sentiment classification.\n",
    "\n",
    "We will use the Quora dataset for this task which contains questin pairs with binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_quora(split=\"train\"):\n",
    "    if split == \"test\":\n",
    "        filename = \"data/quora-test-student.csv\"    \n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for record in csv.DictReader(f, delimiter='\\t'):\n",
    "                sent1 = record['sentence1'].lower().strip()\n",
    "                sent2 = record['sentence2'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                data.append((sent1,sent2,sent_id))\n",
    "        return data          \n",
    "    else:\n",
    "        if split == \"train\":\n",
    "            filename = \"data/quora-train.csv\"\n",
    "        elif split== \"dev\":\n",
    "            filename = \"data/quora-dev.csv\"   \n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(filename, 'r') as f:\n",
    "            i = 1\n",
    "            for record in csv.DictReader(f, delimiter='\\t'):\n",
    "                sent1 = record['sentence1'].lower().strip()\n",
    "                sent2 = record['sentence2'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                label = record['is_duplicate'].strip()\n",
    "                if (sent1==\"\") or (sent2==\"\") or (label==\"\"):\n",
    "                    continue\n",
    "                label = int(float(label))                \n",
    "                data.append(((sent1,sent2),label,sent_id))\n",
    "                labels.append(label)\n",
    "        label_distribution = Counter(labels)        \n",
    "        return data, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 141497\n",
      "Train Label distribution: Counter({0: 89224, 1: 52273})\n",
      "Number of dev examples: 20212\n",
      "Dev Label distribution: Counter({0: 12627, 1: 7585})\n"
     ]
    }
   ],
   "source": [
    "quora_train, train_label_distribution = load_data_quora(split=\"train\")\n",
    "quora_dev, dev_label_distribution = load_data_quora(split=\"dev\")\n",
    "\n",
    "print(f\"Number of training examples: {len(quora_train)}\")\n",
    "print(f\"Train Label distribution: {train_label_distribution}\")\n",
    "print(f\"Number of dev examples: {len(quora_dev)}\")\n",
    "print(f\"Dev Label distribution: {dev_label_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "    def __init__(self, data, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    # collate function for padding the sentences to the same length and creating attention masks\n",
    "    def collate_fn(self, batch):\n",
    "        sent_pairs = [x[0] for x in batch]\n",
    "        labels = [x[1] for x in batch]\n",
    "        encoded = self.tokenizer.batch_encode_plus(sent_pairs, add_special_tokens=True, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        input_idx = encoded['input_ids']\n",
    "        attn_mask = encoded['attention_mask']   \n",
    "        #token_type_idx = encoded['token_type_ids'] # don't need this since we only have one sentence\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_idx, labels, attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTParaphraseDetector(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_classes=2, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head\n",
    "        self.classifier_head = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, labels, attn_mask):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask)\n",
    "        # extract the `[CLS]` encoding (first element of the sequence)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        cls_encoding = bert_output[:, 0] # shape: (batch_size, hidden_size)\n",
    "        # apply dropout \n",
    "        cls_encoding = self.dropout(cls_encoding) \n",
    "        # compute classifier logits\n",
    "        logits = self.classifier_head(cls_encoding)  # shape: (batch_size, num_classes)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets, attn_mask = batch\n",
    "            # move batch to device\n",
    "            inputs, targets, attn_mask = inputs.to(device), targets.to(device), attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets, attn_mask)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += y_pred.eq(targets.view(-1)).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets, attn_mask = batch\n",
    "            inputs, targets, attn_mask = inputs.to(device), targets.to(device), attn_mask.to(device)\n",
    "            logits, loss = model(inputs, targets, attn_mask)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += y_pred.eq(targets.view(-1)).sum().item()            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'paraphrase_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('paraphrase_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, let's train the model wihout finetuning the BERT base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.483778 M\n",
      "RAM used: 1133.46 MB\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "B = 64\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 5e-3\n",
    "\n",
    "train_dataset = QuoraDataset(quora_train, max_length=max_length)\n",
    "val_dataset = QuoraDataset(quora_dev, max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTParaphraseDetector(finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128]) torch.Size([64]) torch.Size([64, 128])\n"
     ]
    }
   ],
   "source": [
    "input_idx, labels, attn_mask = next(iter(train_dataloader))\n",
    "print(input_idx.shape, labels.shape, attn_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 0.565, Train Accuracy:  0.641, Val Loss:  0.000, Val Accuracy:  0.000:   1%|▏         | 28/2211 [00:06<08:19,  4.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.590, Train Accuracy:  0.654, Val Loss:  0.000, Val Accuracy:  0.000:   3%|▎         | 62/2211 [00:14<08:12,  4.36it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.533, Train Accuracy:  0.682, Val Loss:  0.000, Val Accuracy:  0.000:   8%|▊         | 182/2211 [00:42<07:48,  4.33it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.553, Train Accuracy:  0.689, Val Loss:  0.000, Val Accuracy:  0.000:  10%|█         | 229/2211 [00:53<07:38,  4.32it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.547, Train Accuracy:  0.691, Val Loss:  0.000, Val Accuracy:  0.000:  14%|█▎        | 301/2211 [01:09<07:22,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.557, Train Accuracy:  0.697, Val Loss:  0.000, Val Accuracy:  0.000:  18%|█▊        | 388/2211 [01:29<07:01,  4.32it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.551, Train Accuracy:  0.699, Val Loss:  0.000, Val Accuracy:  0.000:  21%|██        | 466/2211 [01:48<06:45,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.556, Train Accuracy:  0.699, Val Loss:  0.000, Val Accuracy:  0.000:  21%|██▏       | 471/2211 [01:49<06:46,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.546, Train Accuracy:  0.700, Val Loss:  0.000, Val Accuracy:  0.000:  22%|██▏       | 489/2211 [01:53<06:40,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.571, Train Accuracy:  0.700, Val Loss:  0.000, Val Accuracy:  0.000:  25%|██▍       | 544/2211 [02:06<06:27,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.530, Train Accuracy:  0.701, Val Loss:  0.000, Val Accuracy:  0.000:  25%|██▌       | 555/2211 [02:08<06:24,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.532, Train Accuracy:  0.702, Val Loss:  0.000, Val Accuracy:  0.000:  26%|██▌       | 577/2211 [02:14<06:19,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.537, Train Accuracy:  0.703, Val Loss:  0.000, Val Accuracy:  0.000:  29%|██▉       | 651/2211 [02:31<06:02,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.549, Train Accuracy:  0.705, Val Loss:  0.000, Val Accuracy:  0.000:  31%|███▏      | 695/2211 [02:41<05:51,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.524, Train Accuracy:  0.705, Val Loss:  0.000, Val Accuracy:  0.000:  33%|███▎      | 720/2211 [02:47<05:46,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.542, Train Accuracy:  0.705, Val Loss:  0.000, Val Accuracy:  0.000:  34%|███▍      | 747/2211 [02:53<05:39,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.559, Train Accuracy:  0.705, Val Loss:  0.000, Val Accuracy:  0.000:  38%|███▊      | 851/2211 [03:17<05:17,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.532, Train Accuracy:  0.705, Val Loss:  0.000, Val Accuracy:  0.000:  40%|████      | 886/2211 [03:25<05:07,  4.31it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.542, Train Accuracy:  0.706, Val Loss:  0.000, Val Accuracy:  0.000:  46%|████▌     | 1011/2211 [03:54<04:39,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.560, Train Accuracy:  0.707, Val Loss:  0.000, Val Accuracy:  0.000:  47%|████▋     | 1037/2211 [04:00<04:33,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.550, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  52%|█████▏    | 1141/2211 [04:25<04:09,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.524, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  57%|█████▋    | 1258/2211 [04:52<03:41,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.553, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  59%|█████▉    | 1300/2211 [05:02<03:31,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.525, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  60%|██████    | 1329/2211 [05:08<03:24,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.557, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  61%|██████    | 1350/2211 [05:13<03:20,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.542, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  65%|██████▍   | 1428/2211 [05:31<03:02,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.549, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  66%|██████▋   | 1465/2211 [05:40<02:53,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.526, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  69%|██████▉   | 1525/2211 [05:54<02:39,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.520, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  69%|██████▉   | 1530/2211 [05:55<02:38,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.521, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  69%|██████▉   | 1533/2211 [05:56<02:37,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.547, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  73%|███████▎  | 1610/2211 [06:14<02:19,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.523, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  74%|███████▍  | 1636/2211 [06:20<02:13,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.541, Train Accuracy:  0.708, Val Loss:  0.000, Val Accuracy:  0.000:  85%|████████▍ | 1876/2211 [07:16<01:17,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.523, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  87%|████████▋ | 1914/2211 [07:24<01:08,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.552, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  87%|████████▋ | 1918/2211 [07:25<01:08,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.552, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  87%|████████▋ | 1930/2211 [07:28<01:05,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.538, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  89%|████████▉ | 1966/2211 [07:37<00:56,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.555, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  91%|█████████ | 2004/2211 [07:45<00:48,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.527, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  93%|█████████▎| 2055/2211 [07:57<00:36,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.551, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  95%|█████████▍| 2091/2211 [08:06<00:27,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.555, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000:  99%|█████████▊| 2181/2211 [08:27<00:06,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 1, EMA Train Loss: 0.547, Train Accuracy:  0.709, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 2211/2211 [08:34<00:00,  4.30it/s]\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.550, Train Accuracy:  0.737, Val Loss:  0.496, Val Accuracy:  0.744:   1%|          | 12/2211 [00:02<08:32,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.562, Train Accuracy:  0.723, Val Loss:  0.496, Val Accuracy:  0.744:   3%|▎         | 64/2211 [00:14<08:19,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.568, Train Accuracy:  0.719, Val Loss:  0.496, Val Accuracy:  0.744:   5%|▍         | 107/2211 [00:24<08:10,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.624, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  13%|█▎        | 298/2211 [01:09<07:25,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.600, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  14%|█▎        | 304/2211 [01:10<07:24,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.554, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  17%|█▋        | 365/2211 [01:25<07:10,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.534, Train Accuracy:  0.712, Val Loss:  0.496, Val Accuracy:  0.744:  17%|█▋        | 381/2211 [01:28<07:06,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.537, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  19%|█▉        | 422/2211 [01:38<06:57,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.567, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  23%|██▎       | 512/2211 [01:59<06:35,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.556, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  28%|██▊       | 619/2211 [02:24<06:19,  4.20it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.574, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  28%|██▊       | 628/2211 [02:26<06:24,  4.12it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.563, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  33%|███▎      | 720/2211 [02:48<05:48,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.582, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  35%|███▍      | 773/2211 [03:00<05:35,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.551, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  38%|███▊      | 842/2211 [03:16<05:19,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.566, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  42%|████▏     | 920/2211 [03:35<05:00,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.541, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  47%|████▋     | 1048/2211 [04:04<04:31,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.543, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  49%|████▉     | 1084/2211 [04:13<04:23,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.560, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  50%|████▉     | 1099/2211 [04:16<04:19,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.574, Train Accuracy:  0.712, Val Loss:  0.496, Val Accuracy:  0.744:  53%|█████▎    | 1171/2211 [04:33<04:02,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.565, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  57%|█████▋    | 1258/2211 [04:53<03:41,  4.30it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.541, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  58%|█████▊    | 1279/2211 [04:58<03:37,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.538, Train Accuracy:  0.710, Val Loss:  0.496, Val Accuracy:  0.744:  59%|█████▉    | 1303/2211 [05:04<03:31,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.546, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  61%|██████▏   | 1355/2211 [05:16<03:19,  4.28it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.536, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  63%|██████▎   | 1391/2211 [05:24<03:11,  4.29it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Epoch 2, EMA Train Loss: 0.535, Train Accuracy:  0.711, Val Loss:  0.496, Val Accuracy:  0.744:  63%|██████▎   | 1398/2211 [05:26<03:09,  4.29it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=3, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
