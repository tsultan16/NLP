{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning a BERT Model for Sentiment Classification\n",
    "\n",
    "We will take the original `BERT` model trained on the masked language modeling task and `finetune` it for `sentiment classification` on the Stanford Sentiment Tree and CFIMDB datasets. The `pretrained` BERT model takes in an input sequence of integer tokens and outputs a corresponding sequence of contextualized encoded vectors. A special `[CLS]` token is appended at the start of the input sequence and the coressponding encoded output vector of this token represents a `pooled representation` of the entire sequence. This pooled representation vector can then be used by a feedforward network to perform a sentence classification task. All parameters in this combined model (consisting of the BERT and the feedforward classifier) can be trained together to optimize the model for the sentence classification task, this process is called `finetuning`, because it involves adapting the pretrained parameters of BERT for this specialized task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the WordPiece tokenizer and the pre-trained BERT provided by the Hugginface transformers library. First, lets try out the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  Yay, I'm excited to try out this BERT model from Huggingface!\n",
      "Subword tokens:  ['ya', '##y', ',', 'i', \"'\", 'm', 'excited', 'to', 'try', 'out', 'this', 'bert', 'model', 'from', 'hugging', '##face', '!']\n",
      "Encoded sentence:  [8038, 2100, 1010, 1045, 1005, 1049, 7568, 2000, 3046, 2041, 2023, 14324, 2944, 2013, 17662, 12172, 999]\n",
      "Idx back to tokens:  ['ya', '##y', ',', 'i', \"'\", 'm', 'excited', 'to', 'try', 'out', 'this', 'bert', 'model', 'from', 'hugging', '##face', '!']\n",
      "Decoded sentence:  yay, i'm excited to try out this bert model from huggingface!\n",
      "\n",
      "Special tokens with their integer id:\n",
      "[PAD]  <-->  0\n",
      "[CLS]  <-->  101\n",
      "[SEP]  <-->  102\n",
      "[MASK]  <-->  103\n",
      "[UNK]  <-->  100\n"
     ]
    }
   ],
   "source": [
    "# load the prettrained WordPiece tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# use it on a test sentence\n",
    "sentence = \"Yay, I'm excited to try out this BERT model from Huggingface!\"\n",
    "tokens_subwords = tokenizer.tokenize(sentence)\n",
    "tokens_idx = tokenizer.encode(sentence)\n",
    "idx_to_tokens = tokenizer.convert_ids_to_tokens(tokens_idx)\n",
    "decoded_sentence = tokenizer.decode(tokens_idx)\n",
    "print(\"Original sentence: \", sentence)\n",
    "print(\"Subword tokens: \", tokens_subwords)\n",
    "print(\"Encoded sentence: \", tokens_idx)\n",
    "print(\"Idx back to tokens: \", idx_to_tokens)\n",
    "print(\"Decoded sentence: \", decoded_sentence)\n",
    "\n",
    "# let's also take a look at all the special tokens\n",
    "print(\"\\nSpecial tokens with their integer id:\")\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for t in special_tokens:\n",
    "    print(t,\" <--> \" ,tokenizer.convert_tokens_to_ids(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that since we're using the `uncased` version of the tokenizer, everything becomes lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
