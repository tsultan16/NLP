{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning a BERT Model for Sentiment Classification\n",
    "\n",
    "We will take the original `BERT` model trained on the masked language modeling task and `finetune` it for `sentiment classification` on the Stanford Sentiment Tree and CFIMDB datasets. The `pretrained` BERT model takes in an input sequence of integer tokens and outputs a corresponding sequence of contextualized encoded vectors (768 dimensional). A special `[CLS]` token is appended at the start of the input sequence and the coressponding encoded output vector of this token represents a `pooled representation` of the entire sequence. This pooled representation vector can then be used by a feedforward network to perform a sentence classification task. All parameters in this combined model (consisting of the BERT and the feedforward classifier) can be trained together to optimize the model for the sentence classification task, this process is called `finetuning`, because it involves adapting the pretrained parameters of BERT for this specialized task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the WordPiece tokenizer and the pre-trained BERT provided by the Hugginface transformers library. First, lets try out the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  Yay, I'm excited to try out this BERT model from Huggingface!\n",
      "Subword tokens:  ['ya', '##y', ',', 'i', \"'\", 'm', 'excited', 'to', 'try', 'out', 'this', 'bert', 'model', 'from', 'hugging', '##face', '!']\n",
      "Encoded sentence:  [101, 8038, 2100, 1010, 1045, 1005, 1049, 7568, 2000, 3046, 2041, 2023, 14324, 2944, 2013, 17662, 12172, 999, 102]\n",
      "Idx back to tokens:  ['[CLS]', 'ya', '##y', ',', 'i', \"'\", 'm', 'excited', 'to', 'try', 'out', 'this', 'bert', 'model', 'from', 'hugging', '##face', '!', '[SEP]']\n",
      "Decoded sentence:  [CLS] yay, i'm excited to try out this bert model from huggingface! [SEP]\n",
      "\n",
      "Special tokens with their integer id:\n",
      "[UNK]  <-->  100\n",
      "[SEP]  <-->  102\n",
      "[PAD]  <-->  0\n",
      "[CLS]  <-->  101\n",
      "[MASK]  <-->  103\n"
     ]
    }
   ],
   "source": [
    "# load the prettrained WordPiece tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# use it on a test sentence\n",
    "sentence = \"Yay, I'm excited to try out this BERT model from Huggingface!\"\n",
    "tokens_subwords = tokenizer.tokenize(sentence)\n",
    "tokens_idx = tokenizer.encode(sentence)\n",
    "idx_to_tokens = tokenizer.convert_ids_to_tokens(tokens_idx)\n",
    "decoded_sentence = tokenizer.decode(tokens_idx)\n",
    "print(\"Original sentence: \", sentence)\n",
    "print(\"Subword tokens: \", tokens_subwords)\n",
    "print(\"Encoded sentence: \", tokens_idx)\n",
    "print(\"Idx back to tokens: \", idx_to_tokens)\n",
    "print(\"Decoded sentence: \", decoded_sentence)\n",
    "\n",
    "# let's also take a look at all the special tokens\n",
    "print(\"\\nSpecial tokens with their integer id:\")\n",
    "special_tokens = tokenizer.all_special_tokens\n",
    "for t in special_tokens:\n",
    "    print(t,\" <--> \" ,tokenizer.convert_tokens_to_ids(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that since we're using the `uncased` version of the tokenizer, everything becomes lowercase.\n",
    "\n",
    "Now let's load the SST dataset from file and package it inside a pytroch dataset class. Each data instance is a sentence-sentiment value pair, there are 5 different sentiment labels: NEGATIVE (0), SOMEWHAT NEGATIVE (1), NEUTRAL (2), SOMEWHAT POSITIVE (3), POSITIVE (4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_sst(split=\"train\"):\n",
    "    if split == \"test\":\n",
    "        filename = \"data/ids-sst-test-student.csv\"    \n",
    "        data = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for record in csv.DictReader(f, delimiter='\\t'):\n",
    "                sent = record['sentence'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                data.append((sent,sent_id))\n",
    "        return data          \n",
    "    else:\n",
    "        if split == \"train\":\n",
    "            filename = \"data/ids-sst-train.csv\"\n",
    "        elif split== \"dev\":\n",
    "            filename = \"data/ids-sst-dev.csv\"   \n",
    "        data = []\n",
    "        labels = []\n",
    "        with open(filename, 'r') as f:\n",
    "            for record in csv.DictReader(f, delimiter='\\t'):\n",
    "                sent = record['sentence'].lower().strip()\n",
    "                sent_id = record['id'].lower().strip()\n",
    "                label = int(record['sentiment'].strip())\n",
    "                data.append((sent,label,sent_id))\n",
    "                labels.append(label)\n",
    "        label_distribution = Counter(labels)        \n",
    "        return data, label_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8544\n",
      "Train Label distribution: Counter({3: 2322, 1: 2218, 2: 1624, 4: 1288, 0: 1092})\n",
      "Number of dev examples: 1101\n",
      "Dev Label distribution: Counter({1: 289, 3: 279, 2: 229, 4: 165, 0: 139})\n"
     ]
    }
   ],
   "source": [
    "sst_train, train_label_distribution = load_data_sst(split=\"train\")\n",
    "sst_dev, dev_label_distribution = load_data_sst(split=\"dev\")\n",
    "\n",
    "print(f\"Number of training examples: {len(sst_train)}\")\n",
    "print(f\"Train Label distribution: {train_label_distribution}\")\n",
    "print(f\"Number of dev examples: {len(sst_dev)}\")\n",
    "print(f\"Dev Label distribution: {dev_label_distribution}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, data, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    # collate function for padding the sentences to the same length and creating attention masks\n",
    "    def collate_fn(self, batch):\n",
    "        sents = [x[0] for x in batch]\n",
    "        labels = [x[1] for x in batch]\n",
    "        encoded = self.tokenizer.batch_encode_plus(sents, add_special_tokens=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        input_idx = encoded['input_ids']\n",
    "        attn_mask = encoded['attention_mask']   \n",
    "        #token_type_idx = encoded['token_type_ids'] # don't need this since we only have one sentence\n",
    "        labels = torch.tensor(labels)\n",
    "        return input_idx, labels, attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we define our sentiment classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_classes=5, dropout_rate=0.1, finetune=False):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head\n",
    "        self.classifier_head = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            if finetune:\n",
    "                # make all parameters of BERT model trainable if we're finetuning\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                # freeze all parameters of BERT model if we're not finetuning\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_idx, labels, attn_mask):\n",
    "        # compute BERT encodings\n",
    "        bert_output = self.bert(input_idx, attention_mask=attn_mask)\n",
    "        # extract the `[CLS]` encoding (first element of the sequence)\n",
    "        bert_output = bert_output.last_hidden_state # shape: (batch_size, sequence_length, hidden_size)\n",
    "        cls_encoding = bert_output[:, 0] # shape: (batch_size, hidden_size)\n",
    "        # apply dropout \n",
    "        cls_encoding = self.dropout(cls_encoding) \n",
    "        # compute classifier logits\n",
    "        logits = self.classifier_head(cls_encoding)  # shape: (batch_size, num_classes)\n",
    "        # compute loss\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets, attn_mask = batch\n",
    "            # move batch to device\n",
    "            inputs, targets, attn_mask = inputs.to(device), targets.to(device), attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets, attn_mask)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = inputs.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += y_pred.eq(targets.view(-1)).sum().item()            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets, attn_mask = batch\n",
    "            inputs, targets, attn_mask = inputs.to(device), targets.to(device), attn_mask.to(device)\n",
    "            logits, loss = model(inputs, targets, attn_mask)\n",
    "            B, _ = inputs.shape\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) # shape (B,)\n",
    "            num_correct += y_pred.eq(targets.view(-1)).sum().item()            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'sentiment_classifier_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('sentiment_classifier_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 109.486085 M\n",
      "RAM used: 1584.21 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "max_length = 128\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 5e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataset = SSTDataset(sst_train, max_length=max_length)\n",
    "val_dataset = SSTDataset(sst_dev, max_length=max_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=train_dataset.collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2, collate_fn=val_dataset.collate_fn)\n",
    "\n",
    "# model with finetuning disabled\n",
    "model = BERTSentimentClassifier(finetune=False).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 1.378, Train Accuracy:  0.380, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 267/267 [00:29<00:00,  9.15it/s]\n",
      "Epoch 2, EMA Train Loss: 1.303, Train Accuracy:  0.433, Val Loss:  1.349, Val Accuracy:  0.412: 100%|██████████| 267/267 [00:29<00:00,  9.08it/s]\n",
      "Epoch 3, EMA Train Loss: 1.240, Train Accuracy:  0.452, Val Loss:  1.296, Val Accuracy:  0.431: 100%|██████████| 267/267 [00:29<00:00,  8.97it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=10, save_every=50, val_every=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
