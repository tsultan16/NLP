{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple Question Answering Using a Transformer Decoder**\n",
    "\n",
    "We will explore how to train a character-level tranformer language model for a simple question answering task. Given a question of the form `Where was [X] born?` where `[X]` is the name of a public figure, the model will be trained to predict the output `[Y]` which is the name of the birthplace of that person. Our goal is to first pretrain the model on a wikipedia corpus (next character prediction) from which it is expected to acquire knowledge of persons and their birthplaces. Then we finetune the model with supervised training on `(x,y)` sequence pairs of the following form:\n",
    "\n",
    "`x: Where was Albert Einstein born?%Germany%□□□□□□□□□□□□□□`\n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "where `x` is the input sequence and `y` is the predicted output sequence and `□` is a special padding token. This is a simple next character prediction task, however we do not want the model to predict the question itself, only the answer, which is why in the output sequence, we replace all characters from the question with the padding token and only have the model predict the characters from the answer. \n",
    "\n",
    "i.e. instead of\n",
    "\n",
    "`y: here was Albert Einstein born?%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "we use \n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "We've also used a special token `%` to mark the beginning and end of the span containing the answer.\n",
    "\n",
    "**The idea is that by training the model on this task, it can learn to answer a question by retreiving information pertaining to the answer from it's pretrained knowledge.** After training, we can test this idea by giving the model an input sequence which does not contain an answer, i.e. after the start of answer token `%`, we fill the rest of the sequence with padding tokens: \n",
    "\n",
    "`x: Where was Enrico Fermi born?%□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "Then if the predicted output sequence contains the right answer, then it will support our idea. We also make sure that person names which were not in the training set will be used during testing.\n",
    "\n",
    "\n",
    "We will first train the model on the finetuning task without pretraining it and then look at the difference in performance with and without pretraining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = \"<MASK>\"\n",
    "pad_token = \"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['\\n', ' ', '!', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<MASK>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '£', '\\xad', 'Á', 'Å', 'É', 'Ó', 'Ö', 'Ø', 'Ü', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ë', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ø', 'ü', 'ý', 'ă', 'ą', 'ć', 'Č', 'č', 'ě', 'ğ', 'ī', 'İ', 'ı', 'ł', 'ń', 'ō', 'Ő', 'ő', 'œ', 'ř', 'ś', 'ş', 'Š', 'š', 'ť', 'ū', 'Ż', 'ż', 'Ž', 'ž', 'ș', 'Γ', 'Μ', 'ά', 'έ', 'α', 'γ', 'η', 'ι', 'κ', 'ν', 'ο', 'ρ', 'ς', 'τ', 'υ', 'ω', 'ώ', 'Ј', 'А', 'В', 'Г', 'И', 'К', 'П', 'Р', 'С', 'а', 'б', 'в', 'г', 'д', 'е', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'р', 'с', 'т', 'ц', 'ч', 'ь', 'я', 'ћ', 'א', 'ג', 'ה', 'ו', 'ז', 'ח', 'י', 'כ', 'ל', 'ם', 'מ', 'נ', 'ס', 'ץ', 'ר', 'ש', 'ا', 'ب', 'ت', 'ح', 'خ', 'د', 'ر', 'ش', 'ط', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'ی', 'क', 'द', 'प', 'ल', 'ा', 'ी', 'ौ', '\\u200e', '†', '−', 'ァ', 'ィ', 'イ', 'テ', 'ニ', 'フ', 'マ', 'レ', '・', 'ー', '一', '久', '佳', '前', '剛', '口', '忠', '木', '本', '田', '蛮', '西', '里', '野']\n"
     ]
    }
   ],
   "source": [
    "# first get the character vocabulry from the pretraining dataset\n",
    "with open(\"birth_place_data/wiki.txt\", 'r') as file:\n",
    "    pretrain_text = file.read()\n",
    "\n",
    "vocab = sorted(list(set(pretrain_text)) + [mask_token, pad_token])\n",
    "print(f\"vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameBirthplaceDataset(Dataset):\n",
    "    def __init__(self, vocab, mask_token, pad_token, block_size=128, split=\"train\"):\n",
    "        self.vocab= vocab\n",
    "        self.ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "        self.mask_token = mask_token \n",
    "        self.pad_token = pad_token\n",
    "        self.block_size = block_size\n",
    "        if split == \"train\":\n",
    "            data_filename=\"birth_place_data/birth_places_train.tsv\"\n",
    "        elif split == \"dev\":\n",
    "            data_filename=\"birth_place_data/birth_places_dev.tsv\"\n",
    "    \n",
    "        self.data = self.read_data(data_filename)\n",
    "         \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r') as f: \n",
    "            lines = f.readlines()\n",
    "        return lines    \n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        return self.ctoi[self.pad_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        question, answer = line.strip().split('\\t') \n",
    "        question, answer = list(question), list(answer)\n",
    "        x = question + [self.mask_token] + answer + [self.mask_token] \n",
    "        x = x + (self.block_size-len(x)) * [self.pad_token] \n",
    "        y = [self.pad_token] * (len(question) -1) + x[len(question):]\n",
    "        x = x[:-1] \n",
    "\n",
    "        x = torch.tensor([self.ctoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.ctoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameBirthplaceDataset(vocab, mask_token, pad_token)\n",
    "dev_data = NameBirthplaceDataset(vocab, mask_token, pad_token, split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was Khatchig Mouradian born?<MASK>Lebanon<MASK><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><MASK>Lebanon<MASK><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "x, y = train_data[0]\n",
    "\n",
    "def decode_token_indices(x):\n",
    "    return \"\".join([vocab[i] for i in x])\n",
    "\n",
    "x_decoded = decode_token_indices(x)\n",
    "y_decoded = decode_token_indices(y)\n",
    "print(x_decoded)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the transformer question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerQA(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, blocks_size, pad_token_index, embedding_dim=16, feedforward_dim=64, num_heads=1, num_layers=1, dropout_rate=0.1, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = blocks_size\n",
    "        self.pad_token_index = pad_token_index\n",
    "        # embedding layers\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        c = 0.01        \n",
    "        torch.nn.init.uniform_(self.emb.weight, -c, c)\n",
    "\n",
    "        # static positional encoding (max length set to 1024)\n",
    "        self.pos_emb = torch.zeros(size=(1024, embedding_dim), device=device)\n",
    "        for pos in range(1024):\n",
    "            for i in range(0, embedding_dim, 2):\n",
    "                self.pos_emb[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embedding_dim)))\n",
    "                if i+1 < embedding_dim:\n",
    "                    self.pos_emb[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1))/embedding_dim)))\n",
    "\n",
    "        # transformer decoder\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=feedforward_dim, activation='gelu', dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        # tie the output layer weights with the embedding layer weights\n",
    "        self.output_layer.weight = self.emb.weight\n",
    "\n",
    "\n",
    "    def create_causal_mask(self, input):\n",
    "        _, L, _ = input.shape\n",
    "        # create an L x L matrix with ones on and below diagonal\n",
    "        mask = torch.tril(torch.ones(size=(L,L), device=input.device))\n",
    "        # create mask in which the positions where there is a zero is filled with -infinity \n",
    "        mask = mask.masked_fill((mask==0), float(\"-inf\"))\n",
    "        return mask\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y=None):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # add positional embedding\n",
    "        x = x + self.pos_emb[:x.shape[1]] # shape: (B,L,D)\n",
    "        # pass through transformer decoder layers\n",
    "        mask = self.create_causal_mask(x)\n",
    "        x = self.transformer_decoder(x, x, tgt_mask=mask) # shape: (B,L,D)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,vocab_size)\n",
    "\n",
    "        if y==None:\n",
    "            return x\n",
    "\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,vocab_size)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y, ignore_index=self.pad_token_index)\n",
    "        return x, loss\n",
    "    \n",
    "    \"\"\"    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, subword2idx, block_size, temperature=1.0, topk=None, start_token=\"<s>\", end_token=\"</s>\", max_len=30, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        # generate one token at a time\n",
    "        x = torch.full(size=(1,1), fill_value=subword2idx[start_token], dtype=torch.long, device=device)\n",
    "        tokens = [x.item()]\n",
    "        for _ in range(max_len):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-block_size:]\n",
    "            logits = self.forward(x) # shape: (1,L,V)\n",
    "            # rescale the logits with the temperature\n",
    "            logits = logits / temperature\n",
    "            if topk is not None:\n",
    "                topk_logits, idx = torch.sort(logits[0,-1,:], descending=True)\n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(topk_logits, dim=-1) # shape: (V,)\n",
    "                next_word_idx = idx[torch.multinomial(p, num_samples=1)]\n",
    "            else:             \n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(logits[:,-1,:], dim=-1) # shape: (V,)\n",
    "                next_word_idx = torch.multinomial(p, num_samples=1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_word_idx.view(1,1)), dim=1)\n",
    "            tokens.append(next_word_idx.item())\n",
    "\n",
    "        self.train()\n",
    "        return tokens\n",
    "        \"\"\"\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    pp = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        if epoch%val_every == 0:\n",
    "            # compute validation loss\n",
    "            val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'qa_model_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('qa_model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 0.042432 M\n",
      "RAM used: 1014.29 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "D = 32\n",
    "vocab_size = len(vocab)\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "learning_rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(dev_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "\n",
    "model = TransformerQA(vocab_size, blocks_size=128, pad_token_index=train_data.pad_token_index, embedding_dim=D, feedforward_dim=4*D, num_heads=num_heads, num_layers=num_layers, dropout_rate=0.2, device=DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 5.287, Train Accuracy:  0.004, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 63/63 [00:00<00:00, 74.51it/s]\n",
      "Epoch 2, EMA Train Loss: 4.953, Train Accuracy:  0.004, Val Loss:  5.225, Val Accuracy:  0.005: 100%|██████████| 63/63 [00:00<00:00, 100.37it/s]\n",
      "Epoch 3, EMA Train Loss: 4.636, Train Accuracy:  0.006, Val Loss:  4.886, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 96.28it/s] \n",
      "Epoch 4, EMA Train Loss: 4.359, Train Accuracy:  0.006, Val Loss:  4.577, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 96.20it/s] \n",
      "Epoch 5, EMA Train Loss: 4.128, Train Accuracy:  0.006, Val Loss:  4.306, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 98.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, EMA Train Loss: 3.915, Train Accuracy:  0.006, Val Loss:  4.072, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 96.22it/s] \n",
      "Epoch 7, EMA Train Loss: 3.743, Train Accuracy:  0.006, Val Loss:  3.877, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 90.98it/s]\n",
      "Epoch 8, EMA Train Loss: 3.612, Train Accuracy:  0.006, Val Loss:  3.712, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 98.26it/s] \n",
      "Epoch 9, EMA Train Loss: 3.499, Train Accuracy:  0.010, Val Loss:  3.582, Val Accuracy:  0.006: 100%|██████████| 63/63 [00:00<00:00, 101.65it/s]\n",
      "Epoch 10, EMA Train Loss: 3.418, Train Accuracy:  0.016, Val Loss:  3.476, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 100.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, EMA Train Loss: 3.339, Train Accuracy:  0.016, Val Loss:  3.395, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 102.74it/s]\n",
      "Epoch 12, EMA Train Loss: 3.289, Train Accuracy:  0.016, Val Loss:  3.328, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 103.51it/s]\n",
      "Epoch 13, EMA Train Loss: 3.256, Train Accuracy:  0.016, Val Loss:  3.276, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 101.11it/s]\n",
      "Epoch 14, EMA Train Loss: 3.224, Train Accuracy:  0.016, Val Loss:  3.235, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 99.93it/s] \n",
      "Epoch 15, EMA Train Loss: 3.189, Train Accuracy:  0.016, Val Loss:  3.205, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 98.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, EMA Train Loss: 3.162, Train Accuracy:  0.016, Val Loss:  3.180, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 96.04it/s]\n",
      "Epoch 17, EMA Train Loss: 3.167, Train Accuracy:  0.016, Val Loss:  3.164, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 100.58it/s]\n",
      "Epoch 18, EMA Train Loss: 3.139, Train Accuracy:  0.016, Val Loss:  3.149, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 102.35it/s]\n",
      "Epoch 19, EMA Train Loss: 3.144, Train Accuracy:  0.016, Val Loss:  3.137, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 103.06it/s]\n",
      "Epoch 20, EMA Train Loss: 3.128, Train Accuracy:  0.016, Val Loss:  3.129, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 103.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, EMA Train Loss: 3.124, Train Accuracy:  0.016, Val Loss:  3.121, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 97.56it/s] \n",
      "Epoch 22, EMA Train Loss: 3.119, Train Accuracy:  0.016, Val Loss:  3.117, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 99.86it/s] \n",
      "Epoch 23, EMA Train Loss: 3.101, Train Accuracy:  0.016, Val Loss:  3.113, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 100.22it/s]\n",
      "Epoch 24, EMA Train Loss: 3.098, Train Accuracy:  0.016, Val Loss:  3.103, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 103.70it/s]\n",
      "Epoch 25, EMA Train Loss: 3.105, Train Accuracy:  0.016, Val Loss:  3.098, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 101.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, EMA Train Loss: 3.062, Train Accuracy:  0.016, Val Loss:  3.090, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 96.06it/s] \n",
      "Epoch 27, EMA Train Loss: 3.031, Train Accuracy:  0.016, Val Loss:  3.046, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 96.23it/s] \n",
      "Epoch 28, EMA Train Loss: 3.004, Train Accuracy:  0.016, Val Loss:  3.013, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 101.89it/s]\n",
      "Epoch 29, EMA Train Loss: 2.970, Train Accuracy:  0.016, Val Loss:  2.978, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 101.43it/s]\n",
      "Epoch 30, EMA Train Loss: 2.947, Train Accuracy:  0.016, Val Loss:  2.945, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 99.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31, EMA Train Loss: 2.906, Train Accuracy:  0.016, Val Loss:  2.917, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 96.65it/s] \n",
      "Epoch 32, EMA Train Loss: 2.854, Train Accuracy:  0.016, Val Loss:  2.872, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 101.28it/s]\n",
      "Epoch 33, EMA Train Loss: 2.828, Train Accuracy:  0.017, Val Loss:  2.838, Val Accuracy:  0.016: 100%|██████████| 63/63 [00:00<00:00, 97.00it/s] \n",
      "Epoch 34, EMA Train Loss: 2.776, Train Accuracy:  0.017, Val Loss:  2.784, Val Accuracy:  0.017: 100%|██████████| 63/63 [00:00<00:00, 96.24it/s]\n",
      "Epoch 35, EMA Train Loss: 2.740, Train Accuracy:  0.018, Val Loss:  2.741, Val Accuracy:  0.018: 100%|██████████| 63/63 [00:00<00:00, 90.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36, EMA Train Loss: 2.690, Train Accuracy:  0.018, Val Loss:  2.701, Val Accuracy:  0.018: 100%|██████████| 63/63 [00:00<00:00, 90.26it/s]\n",
      "Epoch 37, EMA Train Loss: 2.656, Train Accuracy:  0.019, Val Loss:  2.659, Val Accuracy:  0.019: 100%|██████████| 63/63 [00:00<00:00, 93.26it/s]\n",
      "Epoch 38, EMA Train Loss: 2.631, Train Accuracy:  0.019, Val Loss:  2.624, Val Accuracy:  0.019: 100%|██████████| 63/63 [00:00<00:00, 94.33it/s]\n",
      "Epoch 39, EMA Train Loss: 2.594, Train Accuracy:  0.020, Val Loss:  2.587, Val Accuracy:  0.020: 100%|██████████| 63/63 [00:00<00:00, 80.54it/s]\n",
      "Epoch 40, EMA Train Loss: 2.574, Train Accuracy:  0.020, Val Loss:  2.558, Val Accuracy:  0.021: 100%|██████████| 63/63 [00:00<00:00, 92.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41, EMA Train Loss: 2.528, Train Accuracy:  0.021, Val Loss:  2.529, Val Accuracy:  0.021: 100%|██████████| 63/63 [00:00<00:00, 99.02it/s] \n",
      "Epoch 42, EMA Train Loss: 2.507, Train Accuracy:  0.021, Val Loss:  2.498, Val Accuracy:  0.021: 100%|██████████| 63/63 [00:00<00:00, 102.75it/s]\n",
      "Epoch 43, EMA Train Loss: 2.485, Train Accuracy:  0.022, Val Loss:  2.466, Val Accuracy:  0.022: 100%|██████████| 63/63 [00:00<00:00, 95.63it/s]\n",
      "Epoch 44, EMA Train Loss: 2.468, Train Accuracy:  0.022, Val Loss:  2.439, Val Accuracy:  0.023: 100%|██████████| 63/63 [00:00<00:00, 95.32it/s] \n",
      "Epoch 45, EMA Train Loss: 2.437, Train Accuracy:  0.022, Val Loss:  2.410, Val Accuracy:  0.023: 100%|██████████| 63/63 [00:00<00:00, 95.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46, EMA Train Loss: 2.413, Train Accuracy:  0.023, Val Loss:  2.376, Val Accuracy:  0.024: 100%|██████████| 63/63 [00:00<00:00, 98.44it/s] \n",
      "Epoch 47, EMA Train Loss: 2.390, Train Accuracy:  0.023, Val Loss:  2.353, Val Accuracy:  0.024: 100%|██████████| 63/63 [00:00<00:00, 94.52it/s] \n",
      "Epoch 48, EMA Train Loss: 2.382, Train Accuracy:  0.023, Val Loss:  2.330, Val Accuracy:  0.024: 100%|██████████| 63/63 [00:00<00:00, 91.05it/s]\n",
      "Epoch 49, EMA Train Loss: 2.354, Train Accuracy:  0.023, Val Loss:  2.311, Val Accuracy:  0.025: 100%|██████████| 63/63 [00:00<00:00, 98.41it/s] \n",
      "Epoch 50, EMA Train Loss: 2.330, Train Accuracy:  0.023, Val Loss:  2.290, Val Accuracy:  0.025: 100%|██████████| 63/63 [00:00<00:00, 98.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=50, save_every=5, val_every=1) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
