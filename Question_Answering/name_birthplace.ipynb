{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple Question Answering Using a Transformer Decoder**\n",
    "\n",
    "We will explore how to train a character-level tranformer language model for a simple question answering task. Given a question of the form `Where was [X] born?` where `[X]` is the name of a public figure, the model will be trained to predict the output `[Y]` which is the name of the birthplace of that person. Our goal is to first pretrain the model on a wikipedia corpus (next character prediction) from which it is expected to acquire knowledge of persons and their birthplaces. Then we finetune the model with supervised training on `(x,y)` sequence pairs of the following form:\n",
    "\n",
    "`x: Where was Albert Einstein born?%Germany%□□□□□□□□□□□□□□`\n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "where `x` is the input sequence and `y` is the predicted output sequence and `□` is a special padding token. This is a simple next character prediction task, however we do not want the model to predict the question itself, only the answer, which is why in the output sequence, we replace all characters from the question with the padding token and only have the model predict the characters from the answer. \n",
    "\n",
    "i.e. instead of\n",
    "\n",
    "`y: here was Albert Einstein born?%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "we use \n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "We've also used a special token `%` to mark the beginning and end of the span containing the answer.\n",
    "\n",
    "**The idea is that by training the model on this task, it can learn to answer a question by retreiving information pertaining to the answer from it's pretrained knowledge.** After training, we can test this idea by giving the model an input sequence which does not contain an answer, i.e. after the start of answer token `%`, we fill the rest of the sequence with padding tokens: \n",
    "\n",
    "`x: Where was Enrico Fermi born?%□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "Then if the predicted output sequence contains the right answer, then it will support our idea. We also make sure that person names which were not in the training set will be used during testing.\n",
    "\n",
    "\n",
    "We will first train the model on the finetuning task without pretraining it and then look at the difference in performance with and without pretraining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask token: ‒\n",
      " token: □\n"
     ]
    }
   ],
   "source": [
    "mask_token = u\"\\u2012\"\n",
    "pad_token = u\"\\u25A1\"\n",
    "\n",
    "print(f\"mask token: {mask_token}\")\n",
    "print(f\" token: {pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['\\n', ' ', '!', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '£', '\\xad', 'Á', 'Å', 'É', 'Ó', 'Ö', 'Ø', 'Ü', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ë', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ø', 'ü', 'ý', 'ă', 'ą', 'ć', 'Č', 'č', 'ě', 'ğ', 'ī', 'İ', 'ı', 'ł', 'ń', 'ō', 'Ő', 'ő', 'œ', 'ř', 'ś', 'ş', 'Š', 'š', 'ť', 'ū', 'Ż', 'ż', 'Ž', 'ž', 'ș', 'Γ', 'Μ', 'ά', 'έ', 'α', 'γ', 'η', 'ι', 'κ', 'ν', 'ο', 'ρ', 'ς', 'τ', 'υ', 'ω', 'ώ', 'Ј', 'А', 'В', 'Г', 'И', 'К', 'П', 'Р', 'С', 'а', 'б', 'в', 'г', 'д', 'е', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'р', 'с', 'т', 'ц', 'ч', 'ь', 'я', 'ћ', 'א', 'ג', 'ה', 'ו', 'ז', 'ח', 'י', 'כ', 'ל', 'ם', 'מ', 'נ', 'ס', 'ץ', 'ר', 'ש', 'ا', 'ب', 'ت', 'ح', 'خ', 'د', 'ر', 'ش', 'ط', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'ی', 'क', 'द', 'प', 'ल', 'ा', 'ी', 'ौ', '\\u200e', '†', '−', 'ァ', 'ィ', 'イ', 'テ', 'ニ', 'フ', 'マ', 'レ', '・', 'ー', '一', '久', '佳', '前', '剛', '口', '忠', '木', '本', '田', '蛮', '西', '里', '野', '‒', '□']\n"
     ]
    }
   ],
   "source": [
    "# first get the character vocabulry from the pretraining dataset\n",
    "with open(\"birth_place_data/wiki.txt\", 'r') as file:\n",
    "    pretrain_text = file.read()\n",
    "\n",
    "vocab = sorted(list(set(pretrain_text)))\n",
    "assert mask_token not in vocab, \"mask token should not be in the vocabulary\"\n",
    "assert pad_token not in vocab, \"pad token should not be in the vocabulary\"\n",
    "vocab = vocab + [mask_token, pad_token]\n",
    "print(f\"vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameBirthplaceDataset(Dataset):\n",
    "    def __init__(self, vocab, mask_token, pad_token, block_size=128, split=\"train\"):\n",
    "        self.vocab= vocab\n",
    "        self.ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "        self.mask_token = mask_token \n",
    "        self.pad_token = pad_token\n",
    "        self.block_size = block_size\n",
    "        if split == \"train\":\n",
    "            data_filename=\"birth_place_data/birth_places_train.tsv\"\n",
    "        elif split == \"dev\":\n",
    "            data_filename=\"birth_place_data/birth_places_dev.tsv\"\n",
    "    \n",
    "        self.data = self.read_data(data_filename)\n",
    "         \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r') as f: \n",
    "            lines = f.readlines()\n",
    "        return lines    \n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        return self.ctoi[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def mask_token_index(self):\n",
    "        return self.ctoi[self.mask_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        question, answer = line.strip().split('\\t') \n",
    "        question, answer = list(question), list(answer)\n",
    "        x = question + [self.mask_token] + answer + [self.mask_token] \n",
    "        x = x + (self.block_size-len(x)) * [self.pad_token] \n",
    "        y = [self.pad_token] * (len(question) -1) + x[len(question):]\n",
    "        x = x[:-1] \n",
    "\n",
    "        x = torch.tensor([self.ctoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.ctoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameBirthplaceDataset(vocab, mask_token, pad_token)\n",
    "dev_data = NameBirthplaceDataset(vocab, mask_token, pad_token, split=\"dev\")\n",
    "\n",
    "pad_token_index = train_data.pad_token_index\n",
    "mask_token_index = train_data.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was Khatchig Mouradian born?‒Lebanon‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Lebanon‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "x, y = train_data[0]\n",
    "\n",
    "def decode_token_indices(x):\n",
    "    return \"\".join([vocab[i] for i in x])\n",
    "\n",
    "x_decoded = decode_token_indices(x)\n",
    "y_decoded = decode_token_indices(y)\n",
    "print(x_decoded)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the transformer question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "mask = 1-torch.tril(torch.ones(size=(5,5)))\n",
    "print(mask)\n",
    "# create mask in which the positions where there are 1s are filled with -infinity \n",
    "mask = mask.masked_fill((mask==1), float(\"-inf\"))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerQA(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, blocks_size, pad_token_index, embedding_dim=16, feedforward_dim=64, num_heads=1, num_layers=1, dropout_rate=0.1, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = blocks_size\n",
    "        self.pad_token_index = pad_token_index\n",
    "        # embedding layers\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        c = 0.01        \n",
    "        torch.nn.init.uniform_(self.emb.weight, -c, c)\n",
    "\n",
    "        # static positional encoding (max length set to 1024)\n",
    "        self.pos_emb = torch.zeros(size=(1024, embedding_dim), device=device)\n",
    "        for pos in range(1024):\n",
    "            for i in range(0, embedding_dim, 2):\n",
    "                self.pos_emb[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embedding_dim)))\n",
    "                if i+1 < embedding_dim:\n",
    "                    self.pos_emb[pos, i+1] = math.cos(pos / (10000 ** ((2 * (i+1))/embedding_dim)))\n",
    "\n",
    "        # transformer decoder\n",
    "        decoder_layer = torch.nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=feedforward_dim, activation='gelu', dropout=dropout_rate, batch_first=True)\n",
    "        self.transformer_decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        # tie the output layer weights with the embedding layer weights\n",
    "        #self.output_layer.weight = self.emb.weight\n",
    "\n",
    "\n",
    "    def create_causal_mask(self, input):\n",
    "        _, L, _ = input.shape\n",
    "        # create an L x L matrix with ones on and above diagonal and zero below\n",
    "        mask = 1-torch.tril(torch.ones(size=(L,L), device=input.device))\n",
    "        # create mask in which the positions where there are 1s are filled with -infinity \n",
    "        mask = mask.masked_fill((mask==1), float(\"-inf\"))\n",
    "        return mask\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y=None):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # add positional embedding\n",
    "        x = x + self.pos_emb[:x.shape[1]] # shape: (B,L,D)\n",
    "        # pass through transformer decoder layers\n",
    "        mask = self.create_causal_mask(x)\n",
    "        x = self.transformer_decoder(x, x, tgt_mask=mask) # shape: (B,L,D)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,vocab_size)\n",
    "\n",
    "        if y==None:\n",
    "            return x\n",
    "\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,vocab_size)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y, ignore_index=self.pad_token_index)\n",
    "        return x, loss\n",
    "    \n",
    "    \"\"\"    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, subword2idx, block_size, temperature=1.0, topk=None, start_token=\"<s>\", end_token=\"</s>\", max_len=30, device=\"cpu\"):\n",
    "        self.eval()\n",
    "        # generate one token at a time\n",
    "        x = torch.full(size=(1,1), fill_value=subword2idx[start_token], dtype=torch.long, device=device)\n",
    "        tokens = [x.item()]\n",
    "        for _ in range(max_len):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-block_size:]\n",
    "            logits = self.forward(x) # shape: (1,L,V)\n",
    "            # rescale the logits with the temperature\n",
    "            logits = logits / temperature\n",
    "            if topk is not None:\n",
    "                topk_logits, idx = torch.sort(logits[0,-1,:], descending=True)\n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(topk_logits, dim=-1) # shape: (V,)\n",
    "                next_word_idx = idx[torch.multinomial(p, num_samples=1)]\n",
    "            else:             \n",
    "                # sample from the distribution for the last word in the sequence\n",
    "                p = F.softmax(logits[:,-1,:], dim=-1) # shape: (V,)\n",
    "                next_word_idx = torch.multinomial(p, num_samples=1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_word_idx.view(1,1)), dim=1)\n",
    "            tokens.append(next_word_idx.item())\n",
    "\n",
    "        self.train()\n",
    "        return tokens\n",
    "        \"\"\"\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    pp = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, L = inputs.shape\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        if epoch%val_every == 0:\n",
    "            # compute validation loss\n",
    "            val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            B, L = inputs.shape\n",
    "            logits, loss = model(inputs, targets)\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = next(iter(dataloader))\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            B, L = inputs.shape\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y_pred = y_pred.view(B,L)\n",
    "        model.train()\n",
    "        return inputs, targets, y_pred\n",
    "\n",
    "# sample a sequence from the model\n",
    "def sample(model, x, block_size=128, sample=False, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        question_length = len(x.view(-1))\n",
    "        x = x.to(device)\n",
    "        for _ in range(block_size-question_length):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-block_size:]\n",
    "            logits = model(x) # shape: (1,L,V)\n",
    "            # sample from the distribution to get the next character\n",
    "            p = F.softmax(logits[:,-1,:], dim=-1) # shape: (V,)\n",
    "            if sample:\n",
    "                next_char_idx = torch.multinomial(p, num_samples=1)\n",
    "            else:\n",
    "                _, next_char_idx = torch.topk(p, k=1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_char_idx.view(1,1)), dim=1)\n",
    "    model.train()\n",
    "    return x\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'qa_model_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('qa_model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 4.345088 M\n",
      "RAM used: 1186.20 MB\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "D = 256\n",
    "vocab_size = len(vocab)\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(dev_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "\n",
    "model = TransformerQA(vocab_size, blocks_size=128, pad_token_index=train_data.pad_token_index, embedding_dim=D, feedforward_dim=4*D, num_heads=num_heads, num_layers=num_layers, dropout_rate=0.2, device=DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: nan, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 63/63 [00:03<00:00, 18.70it/s]\n",
      "Epoch 2, EMA Train Loss: nan, Train Accuracy:  0.000, Val Loss:  nan, Val Accuracy:  0.000: 100%|██████████| 63/63 [00:02<00:00, 21.74it/s]\n",
      "Epoch 3, EMA Train Loss: nan, Train Accuracy:  0.000, Val Loss:  nan, Val Accuracy:  0.000:  29%|██▊       | 18/63 [00:00<00:02, 19.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, log_metrics=log_metrics)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 119\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, val_dataloader, device, num_epochs, val_every, save_every, log_metrics)\u001b[0m\n\u001b[1;32m    117\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# shape (B,L)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m mask \u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m!=\u001b[39m pad_token_index)\n\u001b[0;32m--> 119\u001b[0m num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)            \n\u001b[1;32m    120\u001b[0m num_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m B\n\u001b[1;32m    121\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m num_correct \u001b[38;5;241m/\u001b[39m num_total        \n",
      "Cell \u001b[0;32mIn[69], line 119\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# shape (B,L)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m mask \u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m!=\u001b[39m pad_token_index)\n\u001b[0;32m--> 119\u001b[0m num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B)])            \n\u001b[1;32m    120\u001b[0m num_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m B\n\u001b[1;32m    121\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m num_correct \u001b[38;5;241m/\u001b[39m num_total        \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=100, save_every=10, val_every=1) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Patrick Lemarié born?‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: ‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒MoMlkhhornt‒Paris‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Brett Hayman born?‒Melbourne‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□‒Melbourne‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: ‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒HA‒ppeuWnd‒Melbourne‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Réginald Bernut born?‒Sydney‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Sydney‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: ‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒Mrptthornt‒Sydney‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Christelle Lefranc born?‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: ‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒K‒pWeanccbornt‒Paris‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Ryan Burr born?‒Pennsylvania‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□‒Pennsylvania‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: ‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒p‒‒Pet‒Pennsylvania‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒n‒‒‒‒‒‒\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, y_pred = evaluate(model, train_dataloader, device=DEVICE)\n",
    "for i in range(5):\n",
    "    x = inputs[i]\n",
    "    y = targets[i]\n",
    "    y_hat = y_pred[i]\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"Prediction: {decode_token_indices(y_hat)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some sequences continuing from questions from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Alec Briggs born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□‒Sheffield‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Alec Briggs born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Maurice Vaïsse born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Algiers‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Maurice Vaïsse born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Gonzalo Pieres, Sr. born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Argentina‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Gonzalo Pieres, Sr. born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Aigars Vītols born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Riga‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Aigars Vītols born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Carlo Emery born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□‒Naples‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Carlo Emery born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Sahara Smith born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□‒Austin‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Sahara Smith born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Shiva Boloorian born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Tehran‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Shiva Boloorian born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Herbert Kingsford born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Dover‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Herbert Kingsford born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Samuel Sterett born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Carlisle‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Samuel Sterett born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Barbara Strass born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Vienna‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Barbara Strass born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Fran%C3%A7ois-Anne David born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Fran%C3%A7ois-Anne David born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Tanis Rideout born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Belgium‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Tanis Rideout born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Clere Parsons born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□‒India‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Clere Parsons born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Phil Rose born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□‒Manchester‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Phil Rose born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Petar %C4%8Culi%C4%87 born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Split‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Petar %C4%8Culi%C4%87 born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was John Dodington born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Toronto‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was John Dodington born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Fredrik Liliegren born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Lund‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Fredrik Liliegren born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Allan Stewart born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Edinburgh‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Allan Stewart born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Sumalee Montano born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Columbus‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Sumalee Montano born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was James Whitworth born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Sheffield‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was James Whitworth born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Alastair Gordon born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Sydney‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Alastair Gordon born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Brendon Lindsay born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Australia‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Brendon Lindsay born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Alexander Randall born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Annapolis‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Alexander Randall born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Victor Borge born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□‒Copenhagen‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Victor Borge born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Lewis Rendt born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□‒Germany‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Lewis Rendt born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Joel Honig born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□‒Chicago‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Joel Honig born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Russell Bawden born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Queensland‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Russell Bawden born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Jean Noël Hallé born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Jean Noël Hallé born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Jos%C3%A9 Pou born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□‒Nicaragua‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Jos%C3%A9 Pou born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Terry Zahn born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□‒Milwaukee‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Terry Zahn born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Adolphe Cohn born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□‒Paris‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Adolphe Cohn born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n",
      "Input:      Where was Zhang Lei born?‒\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□‒Nanjing‒□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Where was Zhang Lei born?‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒‒\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "    x = inputs[i]\n",
    "    x = x[:torch.where(x == mask_token_index)[0][0]+1]\n",
    "    y = targets[i]\n",
    "    y_pred = sample(model, x.view(1,-1), device=DEVICE)\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"Prediction: {decode_token_indices(y_pred[0])}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
