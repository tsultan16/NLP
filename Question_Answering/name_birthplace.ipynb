{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple Question Answering Using a Transformer Decoder**\n",
    "\n",
    "We will explore how to train a character-level tranformer language model for a simple question answering task. Given a question of the form `Where was [X] born?` where `[X]` is the name of a public figure, the model will be trained to predict the output `[Y]` which is the name of the birthplace of that person. Our goal is to first pretrain the model on a wikipedia corpus (next character prediction) from which it is expected to acquire knowledge of persons and their birthplaces. Then we finetune the model with supervised training on `(x,y)` sequence pairs of the following form:\n",
    "\n",
    "`x: Where was Albert Einstein born?%Germany%□□□□□□□□□□□□□□`\n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "where `x` is the input sequence and `y` is the predicted output sequence and `□` is a special padding token. This is a simple next character prediction task, however we do not want the model to predict the question itself, only the answer, which is why in the output sequence, we replace all characters from the question with the padding token and only have the model predict the characters from the answer. \n",
    "\n",
    "i.e. instead of\n",
    "\n",
    "`y: here was Albert Einstein born?%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "we use \n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "We've also used a special token `%` to mark the beginning and end of the span containing the answer.\n",
    "\n",
    "**The idea is that by training the model on this task, it can learn to answer a question by retreiving information pertaining to the answer from it's pretrained knowledge.** After training, we can test this idea by giving the model an input sequence which does not contain an answer, i.e. after the start of answer token `%`, we fill the rest of the sequence with padding tokens: \n",
    "\n",
    "`x: Where was Enrico Fermi born?%□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "Then if the predicted output sequence contains the right answer, then it will support our idea. We also make sure that person names which were not in the training set will be used during testing.\n",
    "\n",
    "\n",
    "We will first train the model on the finetuning task without pretraining it and then look at the difference in performance with and without pretraining. \n",
    "\n",
    "(Note: torch.nn.TransformerDecoder does not support autoregressive decoding. Beware!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask token: ■\n",
      " token: □\n"
     ]
    }
   ],
   "source": [
    "mask_token = u\"\\u25A0\"\n",
    "pad_token = u\"\\u25A1\"\n",
    "\n",
    "print(f\"mask token: {mask_token}\")\n",
    "print(f\" token: {pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['□', '■', '\\n', ' ', '!', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '£', '\\xad', 'Á', 'Å', 'É', 'Ó', 'Ö', 'Ø', 'Ü', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ë', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ø', 'ü', 'ý', 'ă', 'ą', 'ć', 'Č', 'č', 'ě', 'ğ', 'ī', 'İ', 'ı', 'ł', 'ń', 'ō', 'Ő', 'ő', 'œ', 'ř', 'ś', 'ş', 'Š', 'š', 'ť', 'ū', 'Ż', 'ż', 'Ž', 'ž', 'ș', 'Γ', 'Μ', 'ά', 'έ', 'α', 'γ', 'η', 'ι', 'κ', 'ν', 'ο', 'ρ', 'ς', 'τ', 'υ', 'ω', 'ώ', 'Ј', 'А', 'В', 'Г', 'И', 'К', 'П', 'Р', 'С', 'а', 'б', 'в', 'г', 'д', 'е', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'р', 'с', 'т', 'ц', 'ч', 'ь', 'я', 'ћ', 'א', 'ג', 'ה', 'ו', 'ז', 'ח', 'י', 'כ', 'ל', 'ם', 'מ', 'נ', 'ס', 'ץ', 'ר', 'ש', 'ا', 'ب', 'ت', 'ح', 'خ', 'د', 'ر', 'ش', 'ط', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'ی', 'क', 'द', 'प', 'ल', 'ा', 'ी', 'ौ', '\\u200e', '†', '−', 'ァ', 'ィ', 'イ', 'テ', 'ニ', 'フ', 'マ', 'レ', '・', 'ー', '一', '久', '佳', '前', '剛', '口', '忠', '木', '本', '田', '蛮', '西', '里', '野']\n"
     ]
    }
   ],
   "source": [
    "# first get the character vocabulry from the pretraining dataset\n",
    "with open(\"birth_place_data/wiki.txt\", 'r', encoding='utf-8') as file:\n",
    "    pretrain_text = file.read()\n",
    "\n",
    "vocab = list(sorted(list(set(pretrain_text))))\n",
    "assert mask_token not in vocab, \"mask token should not be in the vocabulary\"\n",
    "assert pad_token not in vocab, \"pad token should not be in the vocabulary\"\n",
    "vocab = [pad_token, mask_token] + vocab \n",
    "print(f\"vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameBirthplaceDataset(Dataset):\n",
    "    def __init__(self, vocab, mask_token, pad_token, block_size=80, split=\"train\"):\n",
    "        self.vocab= vocab\n",
    "        self.ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "        self.mask_token = mask_token \n",
    "        self.pad_token = pad_token\n",
    "        self.block_size = block_size\n",
    "        if split == \"train\":\n",
    "            data_filename=\"birth_place_data/birth_places_train.tsv\"\n",
    "        elif split == \"dev\":\n",
    "            data_filename=\"birth_place_data/birth_places_dev.tsv\"\n",
    "        self.data = self.read_data(data_filename)\n",
    "         \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f: \n",
    "            lines = f.read()\n",
    "        data = list(lines.encode('utf-8').decode('ascii', errors='ignore').split('\\n'))\n",
    "        return data    \n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        return self.ctoi[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def mask_token_index(self):\n",
    "        return self.ctoi[self.mask_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        question, answer = line.split('\\t') \n",
    "        question, answer = list(question), list(answer) \n",
    "        x = question + [self.mask_token] + answer + [self.mask_token]\n",
    "        x = x + (self.block_size-len(x)) * [self.pad_token] \n",
    "        y = x[1:]\n",
    "        x = x[:-1] \n",
    "        y[:len(question)-1] = (len(question)-1) * [self.pad_token]\n",
    "\n",
    "        x = torch.tensor([self.ctoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.ctoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameBirthplaceDataset(vocab, mask_token, pad_token)\n",
    "dev_data = NameBirthplaceDataset(vocab, mask_token, pad_token, split=\"dev\")\n",
    "\n",
    "pad_token_index = train_data.pad_token_index\n",
    "mask_token_index = train_data.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was Yang Yang born?■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "□□□□□□□□□□□□□□□□□□□□□□□□■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "x, y = train_data[1000]\n",
    "\n",
    "def decode_token_indices(x):\n",
    "    return \"\".join([vocab[i] for i in x])\n",
    "\n",
    "x_decoded = decode_token_indices(x)\n",
    "y_decoded = decode_token_indices(y)\n",
    "print(x_decoded)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the transformer question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = torch.nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        out = F.scaled_dot_product_attention(q,k,v,dropout_p=self.dropout_rate if self.training else 0,is_causal=True)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = torch.nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = torch.nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2, pad_token_idx=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = torch.nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = torch.nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = torch.nn.LayerNorm(embedding_dim)\n",
    "        # output layer logits\n",
    "        self.lm_head = torch.nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=idx.device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=self.pad_token_idx)\n",
    "        return logits, loss\n",
    "    \n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        self.eval() # swicth to inference mode\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-self.block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        self.train() # swicth to train mode\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader,  grad_norm_clip=1.0, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # reset gradients\n",
    "            model.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # clip gradients above threshold\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, L = inputs.shape\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        if epoch%val_every == 0:\n",
    "            # compute validation loss\n",
    "            val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            B, L = inputs.shape\n",
    "            logits, loss = model(inputs, targets)\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dataloader))\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        B, L = inputs.shape\n",
    "        logits, loss = model(inputs, targets)\n",
    "        y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "        y_pred = y_pred.view(B,L)\n",
    "    model.train()\n",
    "    return inputs, targets, y_pred\n",
    "\n",
    "# sample a sequence from the model\n",
    "def sample(model, x, block_size=80, num_chars=40, sample=False, temperature=1.0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        question_length = len(x.view(-1))\n",
    "        x = x.to(device)\n",
    "        for _ in range(num_chars):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-block_size:]\n",
    "            logits, _ = model(x) # shape: (1,L,V)      \n",
    "            # sample from the distribution to get the next character\n",
    "            p = F.softmax(logits[:,-1,:]/temperature, dim=-1) # shape: (V,)\n",
    "            if sample:\n",
    "                next_char_idx = torch.multinomial(p, num_samples=1)\n",
    "            else:\n",
    "                _, next_char_idx = torch.topk(p, k=1, dim=-1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_char_idx), dim=1)\n",
    "    model.train()\n",
    "    return x\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'qa_model_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('qa_model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 3.308288 M\n",
      "RAM used: 1268.35 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "D = 256\n",
    "vocab_size = len(vocab)\n",
    "block_size = 80\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 5e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(dev_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, block_size, D, D, num_heads, num_layers, dropout_rate=0.25, padding_token_idx=train_data.pad_token_index).to(DEVICE)\n",
    "\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "params_decay = [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)]\n",
    "params_nodecay = [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)]\n",
    "optim_groups = [\n",
    "    {\"params\": params_decay, \"weight_decay\": 0.1},\n",
    "    {\"params\": params_nodecay, \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95))\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 2.585, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  8.71it/s]\n",
      "Epoch 2, EMA Train Loss: 2.361, Train Accuracy:  0.001, Val Loss:  2.524, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "Epoch 3, EMA Train Loss: 2.194, Train Accuracy:  0.000, Val Loss:  2.215, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.42it/s]\n",
      "Epoch 4, EMA Train Loss: 2.105, Train Accuracy:  0.002, Val Loss:  2.134, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00, 11.78it/s]\n",
      "Epoch 5, EMA Train Loss: 2.044, Train Accuracy:  0.006, Val Loss:  2.075, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00, 11.52it/s]\n",
      "Epoch 6, EMA Train Loss: 1.978, Train Accuracy:  0.003, Val Loss:  2.039, Val Accuracy:  0.002: 100%|██████████| 16/16 [00:01<00:00, 11.53it/s]\n",
      "Epoch 7, EMA Train Loss: 1.924, Train Accuracy:  0.013, Val Loss:  1.993, Val Accuracy:  0.034: 100%|██████████| 16/16 [00:01<00:00, 11.71it/s]\n",
      "Epoch 8, EMA Train Loss: 1.851, Train Accuracy:  0.004, Val Loss:  1.959, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00, 11.56it/s]\n",
      "Epoch 9, EMA Train Loss: 1.774, Train Accuracy:  0.010, Val Loss:  1.903, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.05it/s]\n",
      "Epoch 10, EMA Train Loss: 1.695, Train Accuracy:  0.015, Val Loss:  1.821, Val Accuracy:  0.002: 100%|██████████| 16/16 [00:01<00:00, 11.34it/s]\n",
      "Epoch 11, EMA Train Loss: 1.612, Train Accuracy:  0.007, Val Loss:  1.785, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00, 11.62it/s]\n",
      "Epoch 12, EMA Train Loss: 1.536, Train Accuracy:  0.015, Val Loss:  1.702, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.48it/s]\n",
      "Epoch 13, EMA Train Loss: 1.455, Train Accuracy:  0.018, Val Loss:  1.657, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.91it/s]\n",
      "Epoch 14, EMA Train Loss: 1.383, Train Accuracy:  0.019, Val Loss:  1.624, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.86it/s]\n",
      "Epoch 15, EMA Train Loss: 1.318, Train Accuracy:  0.025, Val Loss:  1.601, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.67it/s]\n",
      "Epoch 16, EMA Train Loss: 1.253, Train Accuracy:  0.018, Val Loss:  1.541, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.46it/s]\n",
      "Epoch 17, EMA Train Loss: 1.191, Train Accuracy:  0.023, Val Loss:  1.530, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.14it/s]\n",
      "Epoch 18, EMA Train Loss: 1.143, Train Accuracy:  0.022, Val Loss:  1.488, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 10.88it/s]\n",
      "Epoch 19, EMA Train Loss: 1.092, Train Accuracy:  0.027, Val Loss:  1.489, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 10.96it/s]\n",
      "Epoch 20, EMA Train Loss: 1.052, Train Accuracy:  0.028, Val Loss:  1.466, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Epoch 21, EMA Train Loss: 1.011, Train Accuracy:  0.025, Val Loss:  1.472, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.12it/s]\n",
      "Epoch 22, EMA Train Loss: 0.969, Train Accuracy:  0.026, Val Loss:  1.445, Val Accuracy:  0.028: 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\n",
      "Epoch 23, EMA Train Loss: 0.933, Train Accuracy:  0.029, Val Loss:  1.437, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.40it/s]\n",
      "Epoch 24, EMA Train Loss: 0.887, Train Accuracy:  0.031, Val Loss:  1.452, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.37it/s]\n",
      "Epoch 25, EMA Train Loss: 0.869, Train Accuracy:  0.031, Val Loss:  1.474, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00, 11.58it/s]\n",
      "Epoch 26, EMA Train Loss: 0.840, Train Accuracy:  0.038, Val Loss:  1.461, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Epoch 27, EMA Train Loss: 0.805, Train Accuracy:  0.042, Val Loss:  1.485, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Epoch 28, EMA Train Loss: 0.795, Train Accuracy:  0.037, Val Loss:  1.489, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.94it/s]\n",
      "Epoch 29, EMA Train Loss: 0.765, Train Accuracy:  0.041, Val Loss:  1.494, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Epoch 30, EMA Train Loss: 0.734, Train Accuracy:  0.044, Val Loss:  1.497, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Epoch 31, EMA Train Loss: 0.712, Train Accuracy:  0.046, Val Loss:  1.506, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.25it/s]\n",
      "Epoch 32, EMA Train Loss: 0.688, Train Accuracy:  0.057, Val Loss:  1.510, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.31it/s]\n",
      "Epoch 33, EMA Train Loss: 0.666, Train Accuracy:  0.055, Val Loss:  1.537, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.43it/s]\n",
      "Epoch 34, EMA Train Loss: 0.641, Train Accuracy:  0.056, Val Loss:  1.544, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00, 11.37it/s]\n",
      "Epoch 35, EMA Train Loss: 0.629, Train Accuracy:  0.072, Val Loss:  1.557, Val Accuracy:  0.032: 100%|██████████| 16/16 [00:01<00:00, 11.39it/s]\n",
      "Epoch 36, EMA Train Loss: 0.619, Train Accuracy:  0.065, Val Loss:  1.569, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Epoch 37, EMA Train Loss: 0.601, Train Accuracy:  0.072, Val Loss:  1.602, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.18it/s]\n",
      "Epoch 38, EMA Train Loss: 0.585, Train Accuracy:  0.080, Val Loss:  1.629, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.27it/s]\n",
      "Epoch 39, EMA Train Loss: 0.570, Train Accuracy:  0.098, Val Loss:  1.607, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.30it/s]\n",
      "Epoch 40, EMA Train Loss: 0.546, Train Accuracy:  0.093, Val Loss:  1.638, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.42it/s]\n",
      "Epoch 41, EMA Train Loss: 0.528, Train Accuracy:  0.108, Val Loss:  1.659, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.46it/s]\n",
      "Epoch 42, EMA Train Loss: 0.522, Train Accuracy:  0.120, Val Loss:  1.707, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.74it/s]\n",
      "Epoch 43, EMA Train Loss: 0.507, Train Accuracy:  0.117, Val Loss:  1.702, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.13it/s]\n",
      "Epoch 44, EMA Train Loss: 0.495, Train Accuracy:  0.133, Val Loss:  1.743, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.62it/s]\n",
      "Epoch 45, EMA Train Loss: 0.471, Train Accuracy:  0.139, Val Loss:  1.737, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.12it/s]\n",
      "Epoch 46, EMA Train Loss: 0.460, Train Accuracy:  0.146, Val Loss:  1.762, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 10.86it/s]\n",
      "Epoch 47, EMA Train Loss: 0.445, Train Accuracy:  0.176, Val Loss:  1.776, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 10.77it/s]\n",
      "Epoch 48, EMA Train Loss: 0.433, Train Accuracy:  0.181, Val Loss:  1.795, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 10.80it/s]\n",
      "Epoch 49, EMA Train Loss: 0.426, Train Accuracy:  0.176, Val Loss:  1.810, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.49it/s]\n",
      "Epoch 50, EMA Train Loss: 0.418, Train Accuracy:  0.205, Val Loss:  1.843, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=50, save_every=50, val_every=1) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Ben Nicholas born?■Adelaide■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Adelaide■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Stwwfw■■twutgtojhalwgt■h■k■■Cdelaide■Toooooooiroo■o■■fkuouroo■o■uoio■■oookooooo\n",
      "\n",
      "Input:      Where was David Bret born?■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Stwwfw■■twul■t■wuhlokyry■■Baris■Coooooooooooo■oooooo■okuooloo■o■uo■o■■oookooooo\n",
      "\n",
      "Input:      Where was Homa Shaibany born?■Iran■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Iran■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Stwwfw■■twutsstwinnwrg■huuig■Iran■Iouoooooooo■ooo■o■■f■uouroo■o■uo■o■■oookooooo\n",
      "\n",
      "Input:      Where was Wallace Bishop born?■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Stwwfw■■twiwgewetsestiwhhu■ig■Chicago■Moo■oooiooo■o■■■■uogroo■o■uo■o■ooookooooo\n",
      "\n",
      "Input:      Where was George B. Rabb born?■Charleston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Charleston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: Stwwfw■■twtttytwtaktomwyruwug■Wharleston■Pooooooo■o■■■■uogroo■o■uo■■■■oookooooo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, y_pred = evaluate(model, train_dataloader, device=DEVICE)\n",
    "for i in range(5):\n",
    "    x = inputs[i]\n",
    "    y = targets[i]\n",
    "    y_hat = y_pred[i]\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"Prediction: {decode_token_indices(y_hat)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Ben Nicholas born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Adelaide■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ben Nicholas born?■Istanbul■Col■Cubure■Sa■La■Ba■Gland■Prmby\n",
      "\n",
      "Input:      Where was David Bret born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was David Bret born?■Portsmouth■Mannarth■Edam■Lises■Eton■B■Or\n",
      "\n",
      "Input:      Where was Homa Shaibany born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Iran■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Homa Shaibany born?■Rome■Ild■Muse■Dubur■Is■Masin■Wis■Ch■Dur■\n",
      "\n",
      "Input:      Where was Wallace Bishop born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Wallace Bishop born?■Birmingham■Cham■Ponde■Mada■Mirele■Bam■Ed\n",
      "\n",
      "Input:      Where was George B. Rabb born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Charleston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George B. Rabb born?■Hamburgham■Coury■Ch■Bram■Lid■Poullury■Ch\n",
      "\n",
      "Input:      Where was George Sutherland born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Scotland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George Sutherland born?■Scotland■go■Paris■Ba■Cutlart■Hugo■Cusbub\n",
      "\n",
      "Input:      Where was Eliza Stewart Boyd born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Canada■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Eliza Stewart Boyd born?■Columbus■Elgt■De■Ke■Shan■Ge■y■Bry■Gh■Lon\n",
      "\n",
      "Input:      Where was Witold Nazarewicz born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Warsaw■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Witold Nazarewicz born?■Warsaw■Wes■Sconsesas■Stlasbow■ChusthubuV\n",
      "\n",
      "Input:      Where was Jim Boyd born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□■Philadelphia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jim Boyd born?■Syrame■Amsterdam■Lon■Nam■Jerffford■Ed■B■\n",
      "\n",
      "Input:      Where was Heather Jones born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Edmonton■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Heather Jones born?■Cublin■Lusin■Rondon■Stts■Stth■Je■Epury■I\n",
      "\n",
      "Input:      Where was Michel Host born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Flanders■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michel Host born?■Cinbabrge■Frlonde■Be■Jersey■Glada■Sy■Con\n",
      "\n",
      "Input:      Where was Thomas Barclay born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Glasgow■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Thomas Barclay born?■Amsterdam■Dut■Exerpon■Mam■Burit■L■Yon■Br\n",
      "\n",
      "Input:      Where was John Blackburn born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Luton■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Blackburn born?■Luton■March■Brestor■Rostortlla■Cow■Ly■St\n",
      "\n",
      "Input:      Where was Rob Childs born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Derby■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Rob Childs born?■Derby■Prbeston■Porto■Cum■Sale■Burn■Wito■\n",
      "\n",
      "Input:      Where was Martin Kratochvl born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Prague■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Martin Kratochvl born?■Wolmington■Mon■Chatts■Butgartichure■Be■B\n",
      "\n",
      "Input:      Where was Lance Hayward born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Bermuda■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Lance Hayward born?■Columbia■Ro■Pa■Miaus■K■Ma■Chitrtia■Ambus\n",
      "\n",
      "Input:      Where was Marty Radovanic born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cleveland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Marty Radovanic born?■Split■Bagh■Chk■Store■Shuthire■Cham■Athey\n",
      "\n",
      "Input:      Where was Lawrence E. Kahn born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Troy■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Lawrence E. Kahn born?■Paris■Chilad■Pa■Tburgh■Pad■Pa■Tburgh■Dur\n",
      "\n",
      "Input:      Where was Stanley Sporkin born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Philadelphia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Stanley Sporkin born?■Dayton■Gry■Pok■Purthis■Is■Lart■Duly■Phoh\n",
      "\n",
      "Input:      Where was Sergei Ordzhonikidze born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Moscow■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sergei Ordzhonikidze born?■Moscow■Ele■Gem■He■Dulasgt■B■Hud■Geh■Shrs\n",
      "\n",
      "Input:      Where was Branko Rasi born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Serbia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Branko Rasi born?■Hangston■Jamom■Ph■Huton■Sam■Mitav■Rolert\n",
      "\n",
      "Input:      Where was Thomas Guthrie Marquis born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chatham■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Thomas Guthrie Marquis born?■Baltimore■De■Dure■K■Arsgham■Ph■Rore■Cham\n",
      "\n",
      "Input:      Where was Bruno Arpaia born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Naples■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Bruno Arpaia born?■Vienna■Michio■Na■Vens■Ela■Chirgo■Hughon■\n",
      "\n",
      "Input:      Where was Sofia Ester born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Lisbon■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sofia Ester born?■Calgary■Linon■Pol■Ron■Bula■Stthon■Am■Lia\n",
      "\n",
      "Input:      Where was Frank Beck born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Salisbury■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Frank Beck born?■Lahore■Con■Bale■Was■Porer■Bestodfon■Fl■B\n",
      "\n",
      "Input:      Where was Andrzej Maka born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Lublin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Andrzej Maka born?■Ljiverpool■Bes■Nore■Wit■Tbut■Irg■Bug■Ph■\n",
      "\n",
      "Input:      Where was Mohammad Mottahedan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Iran■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Mohammad Mottahedan born?■erlino■To■Pado■Ats■Colas■Ro■Gurith■Pa■Du\n",
      "\n",
      "Input:      Where was Wayne Budd born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Springfield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Wayne Budd born?■Mobile■Chicago■Bago■Dure■Sha■Fre■Be■H■So\n",
      "\n",
      "Input:      Where was Samuel Sterett born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Carlisle■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Samuel Sterett born?■Sarlitle■Man■Liseipoo■S■Butlll■Cow■Sy■Sh\n",
      "\n",
      "Input:      Where was Robert Moss born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Melbourne■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Robert Moss born?■Berlin■PalesmoM■Ex■Win■Ose■PiteAus■M■Pa■\n",
      "\n",
      "Input:      Where was Nick Greenstock born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Dubai■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Nick Greenstock born?■Dubar■Cai■Naples■Des■S■Singar■Ly■Vis■L■U\n",
      "\n",
      "Input:      Where was Sylvia Wynter born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cuba■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sylvia Wynter born?■Belgium■Bad■Porto■A■Pr■Madertlalererist■\n",
      "\n",
      "Input:      Where was Raymond Meier born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Switzerland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Raymond Meier born?■Paris■Puew■Duth■Palerth■Nes■Lon■Pagh■Ba■\n",
      "\n",
      "Input:      Where was Gordon Coppuck born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Fleet■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Gordon Coppuck born?■Elgin■Pa■Hy■Germ■Berthiadria■Ge■Bum■Hy■A\n",
      "\n",
      "Input:      Where was Wilhelm Boger born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Stuttgart■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Wilhelm Boger born?■Stuttgart■Burg■Stoton■Maleutgar■Ch■Con■B\n",
      "\n",
      "Input:      Where was Isaac Green Messec born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Macon■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Isaac Green Messec born?■Barcelona■F■Vinan■La■Ed■Wis■Church■Amsti\n",
      "\n",
      "Input:      Where was Victoria Roberts born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Manhattan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Victoria Roberts born?■Manhattan■Tok■Be■Ge■Masire■St■Flk■Le■Tor\n",
      "\n",
      "Input:      Where was Chad Hartigan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cyprus■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Chad Hartigan born?■India■Mid■Padris■Tubv■Saghix■Duble■Pade■\n",
      "\n",
      "Input:      Where was Simone Pasticcio born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Genoa■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Simone Pasticcio born?■Rome■Bath■Gere■Chagh■N■Compe■Pam■Brerele\n",
      "\n",
      "Input:      Where was Kte Stresemann born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Berlin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Kte Stresemann born?■Florida■Shia■J■Bucert■Bera■Tre■Telurerel\n",
      "\n",
      "Input:      Where was Bryn Atkinson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Canberra■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Bryn Atkinson born?■Jamaica■Ca■Cola■Shubrt■ffford■Cole■San■S\n",
      "\n",
      "Input:      Where was J.C. Patterson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Armagh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was J.C. Patterson born?■Virginia■Bug■Bam■Ch■Gustorirvistolurgh■C\n",
      "\n",
      "Input:      Where was Aamer Haleem born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Aamer Haleem born?■Oslo■Pasle■Os■Lo■Pa■Jerms■Sa■Da■ble■Bort\n",
      "\n",
      "Input:      Where was Alan Davey born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Ipswich■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Alan Davey born?■Mobleurne■Wasaw■To■Huckid■Mamshgtongh■Fr\n",
      "\n",
      "Input:      Where was Zhang Lei born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□■Nanjing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Zhang Lei born?■Nanjing■Huburgh■Porith■De■Ed■Bargh■Am■Gu\n",
      "\n",
      "Input:      Where was Jane R. Goodall born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Yorkshire■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jane R. Goodall born?■Guyana■Amsa■Lia■Chono■Gha■Mano■Lvams■Ire\n",
      "\n",
      "Input:      Where was Bernard Cassen born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Bernard Cassen born?■Watford■Wastm■Romerd■Lirdrte■Mow■Exa■Lor\n",
      "\n",
      "Input:      Where was John Baptist de Faria born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Portugal■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Baptist de Faria born?■Touloulo■Ge■Choa■Amourise■Pan■Pa■Duril■S\n",
      "\n",
      "Input:      Where was Stewart G. Honeck born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Stewart G. Honeck born?■Cogonto■Gen■Den■Wo■Huburd■St■Church■Edam\n",
      "\n",
      "Input:      Where was Martin Kosleck born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Pomerania■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Martin Kosleck born?■Pennsylvania■Ma■Hur■Ly■Buthuritry■Oxer■C\n",
      "\n",
      "Input:      Where was Mordehai Dubin born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Riga■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Mordehai Dubin born?■fart■To■Mornts■Buto■Geal■Balan■Vih■B■Ph■\n",
      "\n",
      "Input:      Where was Humphrey Hopkin born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Nottingham■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Humphrey Hopkin born?■Worcester■Naples■Jesh■B■Otolasgow■Na■A■C\n",
      "\n",
      "Input:      Where was Jesper Nordin born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Stockholm■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jesper Nordin born?■Warsaw■Venice■Sonte■Pr■Stonirtongch■Wa■B\n",
      "\n",
      "Input:      Where was Rhett Davies born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Rhett Davies born?■Belfast■Kiam■Chiam■Chaghire■Dure■Ge■Durm\n",
      "\n",
      "Input:      Where was Ketil Hvoslef born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Bergen■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ketil Hvoslef born?■Philadelphia■Tes■Belphia■Shia■Polia■Kia■\n",
      "\n",
      "Input:      Where was Eugne Brieux born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Eugne Brieux born?■Rome■Lim■Ma■Belllem■Pa■Ma■Tongton■Aturia\n",
      "\n",
      "Input:      Where was Edith Leyrer born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Vienna■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Edith Leyrer born?■Montreal■Nos■Ly■Berk■He■Sy■Ton■Don■Sy■Be\n",
      "\n",
      "Input:      Where was Arthur Kinnaird, 11th Lord Kinnaird born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Kensington■□□□□□□□□□□□□□□□□□\n",
      "y_pred:     rthur Kinnaird, 11th Lord Kinnaird born?■Kensington■Sturch■shamsthonghth■Pagh■Kam\n",
      "\n",
      "Input:      Where was Thierry Ehrmann born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Lyon■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Thierry Ehrmann born?■Oslo■Momphis■Besese■Duse■Burele■It■Ly■Am\n",
      "\n",
      "Input:      Where was Valentin Raychev born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Sofia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Valentin Raychev born?■London■Bel■En■Wiser■Ston■Glandon■B■End■B\n",
      "\n",
      "Input:      Where was Ahmet Glhan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Ankara■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ahmet Glhan born?■Porth■Palende■Polphis■Helam■Wis■Am■Llurc\n",
      "\n",
      "Input:      Where was Martin Weston born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Worcester■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Martin Weston born?■Tripoli■Chis■Mam■Win■Wis■folelis■Huris■C\n",
      "\n",
      "Input:      Where was Alexander Wilson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Virginia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Alexander Wilson born?■Prague■Ath■Lon■Con■Is■Sthila■Am■Wavale■A\n",
      "\n",
      "Input:      Where was Beth Liebling born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Illinois■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Beth Liebling born?■Illinois■Mois■Mon■Chid■Ma■Midrile■Cure■L\n",
      "\n",
      "Input:      Where was Donovan Blake born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Jamaica■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Donovan Blake born?■London■Malean■Ele■Chid■Mamsk■At■Mis■Mid■\n",
      "\n",
      "Input:      Where was Thomas Johns Perry born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cumberland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Thomas Johns Perry born?■Concord■Cuser■Am■Maletlasaley■CJan■Chauc\n",
      "\n",
      "Input:      Where was Hiram Pitt Bennet born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Carthage■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Hiram Pitt Bennet born?■Carthage■Genorthatham■Edganow■Murch■Chan\n",
      "\n",
      "Input:      Where was Barry Mitcalfe born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Wellington■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Barry Mitcalfe born?■Wellington■Pa■Lis■Sholerenck■Tow■Ox■Berk\n",
      "\n",
      "Input:      Where was Robert Atherton born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Liverpool■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Robert Atherton born?■Liverpool■Prtolen■Pr■Rok■Rola■Pis■Brth■T\n",
      "\n",
      "Input:      Where was Shannon Emerick born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Dallas■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Shannon Emerick born?■Dallancas■Das■Chiseto■Dutotor■Pow■Dur■Tb\n",
      "\n",
      "Input:      Where was Harry Marshall Ward born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Hereford■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Harry Marshall Ward born?■Oslo■Rom■Ateid■Ro■Londtert■Ed■Frv■Busons\n",
      "\n",
      "Input:      Where was Yang Yang born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Yang Yang born?■Shanghai■Chai■Spog■Rok■Des■Besirg■Po■Miu\n",
      "\n",
      "Input:      Where was Robert Hamilton Paterson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Edinburgh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Robert Hamilton Paterson born?■Edinburgh■Aus■Dutgth■Colurch■Tburgh■Marg\n",
      "\n",
      "Input:      Where was Sheenagh Pugh born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Birmingham■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sheenagh Pugh born?■Wilmington■Magh■Os■Jerissesha■D■Am■Frch■\n",
      "\n",
      "Input:      Where was Shyril O'Steen born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Seattle■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Shyril O'Steen born?■Tehran■Man■Widshire■Sh■Pixid■Deliw■Phish\n",
      "\n",
      "Input:      Where was David Sander born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Melbourne■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was David Sander born?■England■Month■Mas■Porithile■Ilph■Tolerch\n",
      "\n",
      "Input:      Where was Charles Duncan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Middlesbrough■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Charles Duncan born?■Paris■Athre■Genom■Dermam■Ohia■bly■Ly■Lih\n",
      "\n",
      "Input:      Where was Petr Kroutil born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Prague■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Petr Kroutil born?■Edinburgh■Napes■M■Publeseseshuris■Churgh\n",
      "\n",
      "Input:      Where was Symeon Cosburn born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Symeon Cosburn born?■Germany■Berge■S■Benbo■Irk■Chany■L■Viny■B\n",
      "\n",
      "Input:      Where was Stewart Rawlings Mott born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Flint■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Stewart Rawlings Mott born?■Syria■Rome■Ste■Pa■Chiris■Bugert■DusblmTo\n",
      "\n",
      "Input:      Where was Tony Skabar born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Ukraine■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Tony Skabar born?■Iran■Sy■An■Geviceo■Bela■So■Ba■Cufo■L■Mic\n",
      "\n",
      "Input:      Where was Aleksandra Romani born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Zagreb■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Aleksandra Romani born?■Exeter■Ed■Mon■Ch■Ed■Putrgsbon■Ch■Ly■Abuv\n",
      "\n",
      "Input:      Where was Brian Plummer born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Saskatchewan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Brian Plummer born?■London■Maleas■Eljeiton■Sy■Eng■Pa■Eturch■\n",
      "\n",
      "Input:      Where was Graham Roberts born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chester■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Graham Roberts born?■Chichester■Polagland■is■Vitgtond■Dur■K■S\n",
      "\n",
      "Input:      Where was George Mavrotas born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Athens■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George Mavrotas born?■Rochester■Pom■Mad■Molaliaugtub■Stur■L■Ly\n",
      "\n",
      "Input:      Where was John Alfred Cuthbert born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Savannah■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Alfred Cuthbert born?■Mantilers■Dublilalfow■poo■Paur■Dus■Core■\n",
      "\n",
      "Input:      Where was Alexander Mishnaevski born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Moscow■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Alexander Mishnaevski born?■Sydney■Buselpha■Tut■Glisgh■At■Busbeon■At\n",
      "\n",
      "Input:      Where was Felicia Montealegre born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chile■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Felicia Montealegre born?■Stlevon■Sa■Bude■Ke■Bulgan■Via■Bud■Od■Cha\n",
      "\n",
      "Input:      Where was Hallgeir Langeland born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Strand■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Hallgeir Langeland born?■Strand■Am■Ederderin■Buthart■Heristh■Hong\n",
      "\n",
      "Input:      Where was Tzipora Laskov born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Ukraine■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Tzipora Laskov born?■Sofia■Keld■Na■S■Duble■K■Butgto■Vi■Mo■Lur\n",
      "\n",
      "Input:      Where was Carl Blumenreuter born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Berlin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Carl Blumenreuter born?■Carthagen■Bug■Chatham■Bagart■BuHum■Ch■Co\n",
      "\n",
      "Input:      Where was Nick Doody born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Morley■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Nick Doody born?■Bork■Mostio■Ve■Bal■Tre■Pro■Butolla■A■Gur\n",
      "\n",
      "Input:      Where was Henry Neele born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Henry Neele born?■Fiji■wijerckh■Pola■Pore■Chis■Bure■Pa■Fre\n",
      "\n",
      "Input:      Where was No I.D. born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was No I.D. born?■Trinis■Chicago■S■Ghago■Qur■Dus■Irin■St■M\n",
      "\n",
      "Input:      Where was Olaf Rude born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□■Estonia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Olaf Rude born?■Neapes■Les■Evilzim■Bergha■K■Gerim■Po■L■M\n",
      "\n",
      "Input:      Where was Dumont de Montigny born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Dumont de Montigny born?■Moscon■Maland■Pes■Greshisgow■Maritle■Syl\n",
      "\n",
      "Input:      Where was Edith Dimock born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Hartford■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Edith Dimock born?■Nottingham■Ed■Sh■Warertory■BrMarleophust\n",
      "\n",
      "Input:      Where was Martin Brown born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Melbourne■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Martin Brown born?■London■Mud■Exfitrghuble■Pamslas■Ron■Was■\n",
      "\n",
      "Input:      Where was Wang Nan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□■China■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Wang Nan born?■Beijing■Wing■Nam■Pork■Bere■Ka■ViChia■Top\n",
      "\n",
      "Input:      Where was Laurent Petitguillaume born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Tours■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Laurent Petitguillaume born?■Huddersfield■Keldrid■Boliurit■B■Ost■Verg\n",
      "\n",
      "Input:      Where was Pierre deMorlaix born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■France■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Pierre deMorlaix born?■Scotland■Polad■Chid■Pad■Chiasgo■Fre■Phis\n",
      "\n",
      "Input:      Where was Hubert Buchberger born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Frankfurt■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Hubert Buchberger born?■Pales■Bur■Dublor■Dur■Burgton■L■Sh■Ke■Dur\n",
      "\n",
      "Input:      Where was David Needham born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Leicester■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was David Needham born?■Stofold■S■Stutt■Pariale■Stutttuble■Hert■\n",
      "\n",
      "Input:      Where was Jason Hedlesky born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Michigan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jason Hedlesky born?■Berlin■Mon■Phile■Lelele■Brid■Pow■Nurch■A\n",
      "\n",
      "Input:      Where was Pter Lendvay born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Budapest■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Pter Lendvay born?■Oslo■Pas■A■Berden■PU■ghisseIla■Wow■Eriia\n",
      "\n",
      "Input:      Where was Rosina Storchio born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Venice■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Rosina Storchio born?■Dallas■Aton■Rome■De■Ka■Lons■Pom■Am■Po■Ch\n",
      "\n",
      "Input:      Where was Spencer Swalm born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Colorado■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Spencer Swalm born?■Oston■London■Edon■Con■Edon■Chon■Iy■Ch■At\n",
      "\n",
      "Input:      Where was Francisca Pleguezuelos born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Granada■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Francisca Pleguezuelos born?■Granada■Bade■Maluthasgow■Wury■L■Hangoury\n",
      "\n",
      "Input:      Where was Sheik Sadeek born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Guyana■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sheik Sadeek born?■Pitthrmold■Budamore■Jer■ffford■St■Phurth\n",
      "\n",
      "Input:      Where was Arsen Savadov born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Kiev■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Arsen Savadov born?■Egypt■Aton■Mon■Malereth■Osix■To■Co■Sch■G\n",
      "\n",
      "Input:      Where was Julie Mullen born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Liverpool■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Julie Mullen born?■Birmingham■Sere■Ta■Elde■Madrid■Po■Ex■Ost\n",
      "\n",
      "Input:      Where was Harry Zvi Tabor born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Harry Zvi Tabor born?■Wolverhamper■Sy■Champhams■Spongon■Ben■Ch\n",
      "\n",
      "Input:      Where was Charles Reynolds Brown born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Bethany■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Charles Reynolds Brown born?■Bethany■Pusth■Sy■Panstotchurch■Amh■Frfon\n",
      "\n",
      "Input:      Where was Eduard Ortgies born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Bremen■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Eduard Ortgies born?■Oxford■Bam■Lold■Mordol■futon■Ge■L■Man■Br\n",
      "\n",
      "Input:      Where was Doa Bekleriz born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Istanbul■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Doa Bekleriz born?■Istanbul■Bal■Limore■S■Con■Bre■Ox■Bulure■\n",
      "\n",
      "Input:      Where was Jacques d'Agar born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jacques d'Agar born?■Paris■Baltim■Ath■Ch■Wims■Limsboriw■Bere■\n",
      "\n",
      "Input:      Where was Elnathan Sweet born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cheshire■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Elnathan Sweet born?■Karachster■Poland■Der■Sy■Glasgo■y■ury■Mi\n",
      "\n",
      "Input:      Where was Maurice Tadadjeu born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cameroon■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Maurice Tadadjeu born?■Cameroon■Polad■Ch■Phis■Co■To■Pa■Pa■End■C\n",
      "\n",
      "Input:      Where was Warington Wilkinson Smyth born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Naples■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     here was Warington Wilkinson Smyth born?■Naples■Kis■Besitgto■Win■Phisthin■Chick■L\n",
      "\n",
      "Input:      Where was Rani Price born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Liverpool■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Rani Price born?■Liverpool■Berpzig■K■Win■Ch■Dut■Lireg■Fla\n",
      "\n",
      "Input:      Where was Neal Peters McCurn born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Syracuse■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Neal Peters McCurn born?■Syracuse■Malvo■Sth■Luthuritle■Janem■Curc\n",
      "\n",
      "Input:      Where was Alicia Thorgrimsson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Brandon■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Alicia Thorgrimsson born?■Oslo■La■Elghus■Wa■Stond■De■Bumon■Aton■So\n",
      "\n",
      "Input:      Where was Jaan Arder born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Tallinn■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jaan Arder born?■Chicago■Ire■China■Speg■Sp■Na■Glago■Abela\n",
      "\n",
      "Input:      Where was Walter Elliot born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Edinburgh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Walter Elliot born?■France■Ber■Brlond■Con■Chastom■Pow■Burch■\n",
      "\n",
      "Input:      Where was Michael Carney born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Waterford■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michael Carney born?■Melbourne■Keg■Wato■We■Ly■Lin■Wow■Ourn■Fr\n",
      "\n",
      "Input:      Where was Harry B. Cohen born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Winnipeg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Harry B. Cohen born?■Watford■Parth■Wertlle■Mams■Spond■Stinith\n",
      "\n",
      "Input:      Where was Ronny Swiggers born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Belgium■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ronny Swiggers born?■Watford■Lon■Marn■Ch■Lifonild■Loria■Beld■\n",
      "\n",
      "Input:      Where was Collins J. Seitz born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Wilmington■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Collins J. Seitz born?■Greeent■Berlin■Chala■Puthidaley■Hust■Nok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "    x = inputs[i]\n",
    "    x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "    y = targets[i]\n",
    "    y_pred = sample(model, x.view(1,-1), sample=True, device=DEVICE)\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"y_pred:     {decode_token_indices(y_pred[0])}\")\n",
    "    target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "    pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "    #print(f\"Target:     {target_str}\")\n",
    "    #print(f\"Prediction: {pred_str}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some sequences continuing from questions from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 4/4 [01:02<00:00, 15.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num correct: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "pbar = tqdm(val_dataloader, desc=\"Epochs\")\n",
    "for batch in pbar:\n",
    "    inputs, targets = batch\n",
    "    for i in range(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "        y = targets[i]\n",
    "        y_pred = sample(model, x.view(1,-1), sample=False, device=DEVICE)\n",
    "        #print(f\"Target:     {decode_token_indices(y)}\")\n",
    "        #print(f\"y_pred:     {decode_token_indices(y_pred[0])}\")\n",
    "        target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "        pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "        #print(f\"Target:     {target_str}\")\n",
    "        #print(f\"Prediction: {pred_str}\")\n",
    "        #print(\"\")\n",
    "        if(target_str==pred_str):\n",
    "            num_correct += 1\n",
    "            print(f\"Input:      {decode_token_indices(x)}\")\n",
    "            print(f\"Target:     {target_str}\")\n",
    "            print(f\"Prediction: {pred_str}\")\n",
    "\n",
    "\n",
    "print(f\"Num correct: {num_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
