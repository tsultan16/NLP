{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple Question Answering Using a Transformer Decoder**\n",
    "\n",
    "We will explore how to train a character-level tranformer language model for a simple question answering task. Given a question of the form `Where was [X] born?` where `[X]` is the name of a public figure, the model will be trained to predict the output `[Y]` which is the name of the birthplace of that person. Our goal is to first pretrain the model on a wikipedia corpus (next character prediction) from which it is expected to acquire knowledge of persons and their birthplaces. Then we finetune the model with supervised training on `(x,y)` sequence pairs of the following form:\n",
    "\n",
    "`x: Where was Albert Einstein born?%Germany%□□□□□□□□□□□□□□`\n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "where `x` is the input sequence and `y` is the predicted output sequence and `□` is a special padding token. This is a simple next character prediction task, however we do not want the model to predict the question itself, only the answer, which is why in the output sequence, we replace all characters from the question with the padding token and only have the model predict the characters from the answer. \n",
    "\n",
    "i.e. instead of\n",
    "\n",
    "`y: here was Albert Einstein born?%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "we use \n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "We've also used a special token `%` to mark the beginning and end of the span containing the answer.\n",
    "\n",
    "**The idea is that by training the model on this task, it can learn to answer a question by retreiving information pertaining to the answer from it's pretrained knowledge.** After training, we can test this idea by giving the model an input sequence which does not contain an answer, i.e. after the start of answer token `%`, we fill the rest of the sequence with padding tokens: \n",
    "\n",
    "`x: Where was Enrico Fermi born?%□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "Then if the predicted output sequence contains the right answer, then it will support our idea. We also make sure that person names which were not in the training set will be used during testing.\n",
    "\n",
    "\n",
    "We will first train the model on the finetuning task without pretraining it and then look at the difference in performance with and without pretraining. \n",
    "\n",
    "(Note: torch.nn.TransformerDecoder does not support autoregressive decoding. Beware!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask token: ■\n",
      " token: □\n"
     ]
    }
   ],
   "source": [
    "mask_token = u\"\\u25A0\"\n",
    "pad_token = u\"\\u25A1\"\n",
    "\n",
    "print(f\"mask token: {mask_token}\")\n",
    "print(f\" token: {pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['□', '■', '\\n', ' ', '!', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '£', '\\xad', 'Á', 'Å', 'É', 'Ó', 'Ö', 'Ø', 'Ü', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ë', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ø', 'ü', 'ý', 'ă', 'ą', 'ć', 'Č', 'č', 'ě', 'ğ', 'ī', 'İ', 'ı', 'ł', 'ń', 'ō', 'Ő', 'ő', 'œ', 'ř', 'ś', 'ş', 'Š', 'š', 'ť', 'ū', 'Ż', 'ż', 'Ž', 'ž', 'ș', 'Γ', 'Μ', 'ά', 'έ', 'α', 'γ', 'η', 'ι', 'κ', 'ν', 'ο', 'ρ', 'ς', 'τ', 'υ', 'ω', 'ώ', 'Ј', 'А', 'В', 'Г', 'И', 'К', 'П', 'Р', 'С', 'а', 'б', 'в', 'г', 'д', 'е', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'р', 'с', 'т', 'ц', 'ч', 'ь', 'я', 'ћ', 'א', 'ג', 'ה', 'ו', 'ז', 'ח', 'י', 'כ', 'ל', 'ם', 'מ', 'נ', 'ס', 'ץ', 'ר', 'ש', 'ا', 'ب', 'ت', 'ح', 'خ', 'د', 'ر', 'ش', 'ط', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'ی', 'क', 'द', 'प', 'ल', 'ा', 'ी', 'ौ', '\\u200e', '†', '−', 'ァ', 'ィ', 'イ', 'テ', 'ニ', 'フ', 'マ', 'レ', '・', 'ー', '一', '久', '佳', '前', '剛', '口', '忠', '木', '本', '田', '蛮', '西', '里', '野']\n"
     ]
    }
   ],
   "source": [
    "# first get the character vocabulry from the pretraining dataset\n",
    "with open(\"birth_place_data/wiki.txt\", 'r', encoding='utf-8') as file:\n",
    "    pretrain_text = file.read()\n",
    "\n",
    "vocab = list(sorted(list(set(pretrain_text))))\n",
    "assert mask_token not in vocab, \"mask token should not be in the vocabulary\"\n",
    "assert pad_token not in vocab, \"pad token should not be in the vocabulary\"\n",
    "vocab = [pad_token, mask_token] + vocab \n",
    "print(f\"vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameBirthplaceDataset(Dataset):\n",
    "    def __init__(self, vocab, mask_token, pad_token, block_size=80, split=\"train\"):\n",
    "        self.vocab= vocab\n",
    "        self.ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "        self.mask_token = mask_token \n",
    "        self.pad_token = pad_token\n",
    "        self.block_size = block_size\n",
    "        if split == \"train\":\n",
    "            data_filename=\"birth_place_data/birth_places_train.tsv\"\n",
    "        elif split == \"dev\":\n",
    "            data_filename=\"birth_place_data/birth_places_dev.tsv\"\n",
    "        self.data = self.read_data(data_filename)\n",
    "         \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f: \n",
    "            lines = f.read()\n",
    "        data = list(lines.encode('utf-8').decode('ascii', errors='ignore').split('\\n'))\n",
    "        return data    \n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        return self.ctoi[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def mask_token_index(self):\n",
    "        return self.ctoi[self.mask_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        question, answer = line.split('\\t') \n",
    "        question, answer = list(question), list(answer) \n",
    "        x = question + [self.mask_token] + answer + [self.mask_token]\n",
    "        x = x + (self.block_size-len(x)) * [self.pad_token] \n",
    "        y = x[1:]\n",
    "        x = x[:-1] \n",
    "        y[:len(question)-1] = (len(question)-1) * [self.pad_token]\n",
    "\n",
    "        x = torch.tensor([self.ctoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.ctoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameBirthplaceDataset(vocab, mask_token, pad_token)\n",
    "dev_data = NameBirthplaceDataset(vocab, mask_token, pad_token, split=\"dev\")\n",
    "\n",
    "pad_token_index = train_data.pad_token_index\n",
    "mask_token_index = train_data.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was Yang Yang born?■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "□□□□□□□□□□□□□□□□□□□□□□□□■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "x, y = train_data[1000]\n",
    "\n",
    "def decode_token_indices(x):\n",
    "    return \"\".join([vocab[i] for i in x])\n",
    "\n",
    "x_decoded = decode_token_indices(x)\n",
    "y_decoded = decode_token_indices(y)\n",
    "print(x_decoded)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the transformer question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = torch.nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        out = F.scaled_dot_product_attention(q,k,v,dropout_p=self.dropout_rate if self.training else 0,is_causal=True)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = torch.nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = torch.nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2, pad_token_idx=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = torch.nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = torch.nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = torch.nn.LayerNorm(embedding_dim)\n",
    "        # output layer logits\n",
    "        self.lm_head = torch.nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T =idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=idx.device)) # (T,C) \n",
    "        x = token_embeds + pos_embeds # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=self.pad_token_idx)\n",
    "        return logits, loss\n",
    "    \n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        self.eval() # swicth to inference mode\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-self.block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        self.train() # swicth to train mode\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader,  grad_norm_clip=1.0, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # reset gradients\n",
    "            model.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # clip gradients above threshold\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, L = inputs.shape\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        if epoch%val_every == 0:\n",
    "            # compute validation loss\n",
    "            val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            B, L = inputs.shape\n",
    "            logits, loss = model(inputs, targets)\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dataloader))\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        B, L = inputs.shape\n",
    "        logits, loss = model(inputs, targets)\n",
    "        y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "        y_pred = y_pred.view(B,L)\n",
    "    model.train()\n",
    "    return inputs, targets, y_pred\n",
    "\n",
    "# sample a sequence from the model\n",
    "def sample(model, x, block_size=80, num_chars=40, sample=False, temperature=1.0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        question_length = len(x.view(-1))\n",
    "        x = x.to(device)\n",
    "        for _ in range(num_chars):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-block_size:]\n",
    "            logits, _ = model(x) # shape: (1,L,V)      \n",
    "            # sample from the distribution to get the next character\n",
    "            p = F.softmax(logits[:,-1,:]/temperature, dim=-1) # shape: (V,)\n",
    "            if sample:\n",
    "                next_char_idx = torch.multinomial(p, num_samples=1)\n",
    "            else:\n",
    "                _, next_char_idx = torch.topk(p, k=1, dim=-1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_char_idx), dim=1)\n",
    "    model.train()\n",
    "    return x\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'qa_model_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('qa_model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 3.308288 M\n",
      "RAM used: 658.93 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "D = 256\n",
    "vocab_size = len(vocab)\n",
    "block_size = 80\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 5e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(dev_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, block_size, D, D, num_heads, num_layers, dropout_rate=0.25, pad_token_idx=train_data.pad_token_index).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 2.567, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:02<00:00,  7.88it/s]\n",
      "Epoch 2, EMA Train Loss: 2.341, Train Accuracy:  0.001, Val Loss:  2.482, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00, 11.40it/s]\n",
      "Epoch 3, EMA Train Loss: 2.184, Train Accuracy:  0.000, Val Loss:  2.199, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00, 11.52it/s]\n",
      "Epoch 4, EMA Train Loss: 2.102, Train Accuracy:  0.001, Val Loss:  2.121, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00, 11.15it/s]\n",
      "Epoch 5, EMA Train Loss: 2.033, Train Accuracy:  0.004, Val Loss:  2.067, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00, 11.43it/s]\n",
      "Epoch 6, EMA Train Loss: 1.961, Train Accuracy:  0.004, Val Loss:  2.027, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00, 11.84it/s]\n",
      "Epoch 7, EMA Train Loss: 1.893, Train Accuracy:  0.009, Val Loss:  1.976, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Epoch 8, EMA Train Loss: 1.810, Train Accuracy:  0.016, Val Loss:  1.915, Val Accuracy:  0.002: 100%|██████████| 16/16 [00:01<00:00, 11.96it/s]\n",
      "Epoch 9, EMA Train Loss: 1.725, Train Accuracy:  0.011, Val Loss:  1.851, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.95it/s]\n",
      "Epoch 10, EMA Train Loss: 1.624, Train Accuracy:  0.016, Val Loss:  1.775, Val Accuracy:  0.024: 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Epoch 11, EMA Train Loss: 1.520, Train Accuracy:  0.015, Val Loss:  1.699, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00, 12.16it/s]\n",
      "Epoch 12, EMA Train Loss: 1.424, Train Accuracy:  0.019, Val Loss:  1.640, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.76it/s]\n",
      "Epoch 13, EMA Train Loss: 1.335, Train Accuracy:  0.021, Val Loss:  1.582, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.89it/s]\n",
      "Epoch 14, EMA Train Loss: 1.248, Train Accuracy:  0.017, Val Loss:  1.537, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.30it/s]\n",
      "Epoch 15, EMA Train Loss: 1.186, Train Accuracy:  0.022, Val Loss:  1.527, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.61it/s]\n",
      "Epoch 16, EMA Train Loss: 1.110, Train Accuracy:  0.029, Val Loss:  1.497, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.74it/s]\n",
      "Epoch 17, EMA Train Loss: 1.047, Train Accuracy:  0.030, Val Loss:  1.477, Val Accuracy:  0.024: 100%|██████████| 16/16 [00:01<00:00, 12.00it/s]\n",
      "Epoch 18, EMA Train Loss: 0.989, Train Accuracy:  0.030, Val Loss:  1.441, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.94it/s]\n",
      "Epoch 19, EMA Train Loss: 0.935, Train Accuracy:  0.030, Val Loss:  1.435, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.51it/s]\n",
      "Epoch 20, EMA Train Loss: 0.896, Train Accuracy:  0.037, Val Loss:  1.449, Val Accuracy:  0.028: 100%|██████████| 16/16 [00:01<00:00, 11.53it/s]\n",
      "Epoch 21, EMA Train Loss: 0.841, Train Accuracy:  0.040, Val Loss:  1.466, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.48it/s]\n",
      "Epoch 22, EMA Train Loss: 0.807, Train Accuracy:  0.041, Val Loss:  1.476, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00, 11.66it/s]\n",
      "Epoch 23, EMA Train Loss: 0.773, Train Accuracy:  0.046, Val Loss:  1.487, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n",
      "Epoch 24, EMA Train Loss: 0.741, Train Accuracy:  0.054, Val Loss:  1.504, Val Accuracy:  0.024: 100%|██████████| 16/16 [00:01<00:00, 11.49it/s]\n",
      "Epoch 25, EMA Train Loss: 0.698, Train Accuracy:  0.058, Val Loss:  1.544, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00, 11.38it/s]\n",
      "Epoch 26, EMA Train Loss: 0.680, Train Accuracy:  0.058, Val Loss:  1.573, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.39it/s]\n",
      "Epoch 27, EMA Train Loss: 0.660, Train Accuracy:  0.066, Val Loss:  1.559, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n",
      "Epoch 28, EMA Train Loss: 0.634, Train Accuracy:  0.082, Val Loss:  1.603, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 10.99it/s]\n",
      "Epoch 29, EMA Train Loss: 0.605, Train Accuracy:  0.083, Val Loss:  1.605, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.46it/s]\n",
      "Epoch 30, EMA Train Loss: 0.572, Train Accuracy:  0.092, Val Loss:  1.628, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.70it/s]\n",
      "Epoch 31, EMA Train Loss: 0.546, Train Accuracy:  0.110, Val Loss:  1.691, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n",
      "Epoch 32, EMA Train Loss: 0.519, Train Accuracy:  0.115, Val Loss:  1.671, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.15it/s]\n",
      "Epoch 33, EMA Train Loss: 0.512, Train Accuracy:  0.137, Val Loss:  1.707, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.65it/s]\n",
      "Epoch 34, EMA Train Loss: 0.493, Train Accuracy:  0.148, Val Loss:  1.722, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.60it/s]\n",
      "Epoch 35, EMA Train Loss: 0.478, Train Accuracy:  0.148, Val Loss:  1.735, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.71it/s]\n",
      "Epoch 36, EMA Train Loss: 0.462, Train Accuracy:  0.147, Val Loss:  1.784, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.65it/s]\n",
      "Epoch 37, EMA Train Loss: 0.444, Train Accuracy:  0.177, Val Loss:  1.788, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n",
      "Epoch 38, EMA Train Loss: 0.426, Train Accuracy:  0.209, Val Loss:  1.813, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.81it/s]\n",
      "Epoch 39, EMA Train Loss: 0.415, Train Accuracy:  0.206, Val Loss:  1.849, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Epoch 40, EMA Train Loss: 0.397, Train Accuracy:  0.228, Val Loss:  1.886, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n",
      "Epoch 41, EMA Train Loss: 0.378, Train Accuracy:  0.250, Val Loss:  1.928, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.58it/s]\n",
      "Epoch 42, EMA Train Loss: 0.365, Train Accuracy:  0.268, Val Loss:  1.890, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.62it/s]\n",
      "Epoch 43, EMA Train Loss: 0.356, Train Accuracy:  0.296, Val Loss:  1.954, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.60it/s]\n",
      "Epoch 44, EMA Train Loss: 0.331, Train Accuracy:  0.314, Val Loss:  1.990, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.55it/s]\n",
      "Epoch 45, EMA Train Loss: 0.323, Train Accuracy:  0.335, Val Loss:  1.984, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.46it/s]\n",
      "Epoch 46, EMA Train Loss: 0.313, Train Accuracy:  0.346, Val Loss:  2.024, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.34it/s]\n",
      "Epoch 47, EMA Train Loss: 0.300, Train Accuracy:  0.361, Val Loss:  2.045, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 12.24it/s]\n",
      "Epoch 48, EMA Train Loss: 0.287, Train Accuracy:  0.403, Val Loss:  2.057, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00, 11.69it/s]\n",
      "Epoch 49, EMA Train Loss: 0.278, Train Accuracy:  0.408, Val Loss:  2.072, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Epoch 50, EMA Train Loss: 0.263, Train Accuracy:  0.430, Val Loss:  2.106, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, EMA Train Loss: 0.242, Train Accuracy:  0.477, Val Loss:  2.104, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.97it/s]\n",
      "Epoch 52, EMA Train Loss: 0.232, Train Accuracy:  0.483, Val Loss:  2.136, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.72it/s]\n",
      "Epoch 53, EMA Train Loss: 0.229, Train Accuracy:  0.487, Val Loss:  2.208, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.66it/s]\n",
      "Epoch 54, EMA Train Loss: 0.227, Train Accuracy:  0.502, Val Loss:  2.208, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.33it/s]\n",
      "Epoch 55, EMA Train Loss: 0.217, Train Accuracy:  0.517, Val Loss:  2.257, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.84it/s]\n",
      "Epoch 56, EMA Train Loss: 0.208, Train Accuracy:  0.530, Val Loss:  2.232, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.63it/s]\n",
      "Epoch 57, EMA Train Loss: 0.196, Train Accuracy:  0.559, Val Loss:  2.236, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.38it/s]\n",
      "Epoch 58, EMA Train Loss: 0.183, Train Accuracy:  0.597, Val Loss:  2.289, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 10.77it/s]\n",
      "Epoch 59, EMA Train Loss: 0.178, Train Accuracy:  0.581, Val Loss:  2.267, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Epoch 60, EMA Train Loss: 0.166, Train Accuracy:  0.610, Val Loss:  2.274, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.81it/s]\n",
      "Epoch 61, EMA Train Loss: 0.163, Train Accuracy:  0.620, Val Loss:  2.366, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n",
      "Epoch 62, EMA Train Loss: 0.154, Train Accuracy:  0.637, Val Loss:  2.336, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00, 11.99it/s]\n",
      "Epoch 63, EMA Train Loss: 0.149, Train Accuracy:  0.666, Val Loss:  2.366, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.89it/s]\n",
      "Epoch 64, EMA Train Loss: 0.147, Train Accuracy:  0.666, Val Loss:  2.376, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.41it/s]\n",
      "Epoch 65, EMA Train Loss: 0.137, Train Accuracy:  0.672, Val Loss:  2.399, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.44it/s]\n",
      "Epoch 66, EMA Train Loss: 0.137, Train Accuracy:  0.678, Val Loss:  2.434, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.13it/s]\n",
      "Epoch 67, EMA Train Loss: 0.134, Train Accuracy:  0.702, Val Loss:  2.452, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.34it/s]\n",
      "Epoch 68, EMA Train Loss: 0.134, Train Accuracy:  0.673, Val Loss:  2.459, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Epoch 69, EMA Train Loss: 0.122, Train Accuracy:  0.721, Val Loss:  2.490, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.29it/s]\n",
      "Epoch 70, EMA Train Loss: 0.120, Train Accuracy:  0.714, Val Loss:  2.491, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.71it/s]\n",
      "Epoch 71, EMA Train Loss: 0.117, Train Accuracy:  0.727, Val Loss:  2.525, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.79it/s]\n",
      "Epoch 72, EMA Train Loss: 0.114, Train Accuracy:  0.724, Val Loss:  2.495, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.90it/s]\n",
      "Epoch 73, EMA Train Loss: 0.115, Train Accuracy:  0.705, Val Loss:  2.557, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.86it/s]\n",
      "Epoch 74, EMA Train Loss: 0.110, Train Accuracy:  0.727, Val Loss:  2.532, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Epoch 75, EMA Train Loss: 0.112, Train Accuracy:  0.729, Val Loss:  2.559, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.98it/s]\n",
      "Epoch 76, EMA Train Loss: 0.102, Train Accuracy:  0.746, Val Loss:  2.560, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.47it/s]\n",
      "Epoch 77, EMA Train Loss: 0.094, Train Accuracy:  0.765, Val Loss:  2.608, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Epoch 78, EMA Train Loss: 0.095, Train Accuracy:  0.766, Val Loss:  2.595, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.58it/s]\n",
      "Epoch 79, EMA Train Loss: 0.094, Train Accuracy:  0.769, Val Loss:  2.621, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Epoch 80, EMA Train Loss: 0.090, Train Accuracy:  0.776, Val Loss:  2.611, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 12.19it/s]\n",
      "Epoch 81, EMA Train Loss: 0.084, Train Accuracy:  0.796, Val Loss:  2.657, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.62it/s]\n",
      "Epoch 82, EMA Train Loss: 0.083, Train Accuracy:  0.794, Val Loss:  2.642, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 12.04it/s]\n",
      "Epoch 83, EMA Train Loss: 0.083, Train Accuracy:  0.794, Val Loss:  2.679, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Epoch 84, EMA Train Loss: 0.087, Train Accuracy:  0.781, Val Loss:  2.696, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.93it/s]\n",
      "Epoch 85, EMA Train Loss: 0.082, Train Accuracy:  0.810, Val Loss:  2.697, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 12.02it/s]\n",
      "Epoch 86, EMA Train Loss: 0.084, Train Accuracy:  0.780, Val Loss:  2.731, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00, 12.08it/s]\n",
      "Epoch 87, EMA Train Loss: 0.082, Train Accuracy:  0.796, Val Loss:  2.714, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00, 11.95it/s]\n",
      "Epoch 88, EMA Train Loss: 0.085, Train Accuracy:  0.786, Val Loss:  2.720, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.92it/s]\n",
      "Epoch 89, EMA Train Loss: 0.072, Train Accuracy:  0.832, Val Loss:  2.701, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.79it/s]\n",
      "Epoch 90, EMA Train Loss: 0.072, Train Accuracy:  0.814, Val Loss:  2.758, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.98it/s]\n",
      "Epoch 91, EMA Train Loss: 0.076, Train Accuracy:  0.799, Val Loss:  2.724, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Epoch 92, EMA Train Loss: 0.073, Train Accuracy:  0.818, Val Loss:  2.764, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 12.03it/s]\n",
      "Epoch 93, EMA Train Loss: 0.073, Train Accuracy:  0.814, Val Loss:  2.766, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00, 11.51it/s]\n",
      "Epoch 94, EMA Train Loss: 0.071, Train Accuracy:  0.820, Val Loss:  2.811, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00, 11.47it/s]\n",
      "Epoch 95, EMA Train Loss: 0.076, Train Accuracy:  0.804, Val Loss:  2.785, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.64it/s]\n",
      "Epoch 96, EMA Train Loss: 0.067, Train Accuracy:  0.831, Val Loss:  2.811, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00, 11.77it/s]\n",
      "Epoch 97, EMA Train Loss: 0.070, Train Accuracy:  0.820, Val Loss:  2.805, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.12it/s]\n",
      "Epoch 98, EMA Train Loss: 0.072, Train Accuracy:  0.807, Val Loss:  2.804, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Epoch 99, EMA Train Loss: 0.070, Train Accuracy:  0.825, Val Loss:  2.813, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00, 11.62it/s]\n",
      "Epoch 100, EMA Train Loss: 0.064, Train Accuracy:  0.832, Val Loss:  2.828, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=100, save_every=50, val_every=1) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Francis Montague Holl born?■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: gisesPsBtrrancSniKan■amhclsollyvouty■London■Peroeohohheoogrooeohhrohohehhoooohh\n",
      "\n",
      "Input:      Where was Michael Aufhauser born?■Augsburg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Augsburg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: gisesPsBtrlnSignlKusiamstl■samei■Augsburg■M■ieeeeeeoeaushdeaudihueohoheheooooeh\n",
      "\n",
      "Input:      Where was Maurice Maunoury born?■Alexandria■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Alexandria■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: gisesPsBtrlnslnesPaln■■sJlsllke■Alexandria■Moeaoeoeoeaerogroo■ohorohoioheoooodh\n",
      "\n",
      "Input:      Where was Bhai Balmukund born?■Pakistan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Pakistan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: gisesPsBtrtigcS■nfakymiasepgj■Pakistan■OeuooooeeeoeoeheoodrrehohurooohuhGoooohh\n",
      "\n",
      "Input:      Where was Rod Anderson born?■Australia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Australia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: gisesPsBtrotitntisgtt■MJtac■Australia■Meoaoeooeeeoeoeheoodrreoohorooohooroohoho\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, y_pred = evaluate(model, train_dataloader, device=DEVICE)\n",
    "for i in range(5):\n",
    "    x = inputs[i]\n",
    "    y = targets[i]\n",
    "    y_hat = y_pred[i]\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"Prediction: {decode_token_indices(y_hat)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Francis Montague Holl born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Francis Montague Holl born?■London■Paris■Ch■Lidon■Non■Pa■Pamba■Pa■Bu\n",
      "\n",
      "Input:      Where was Michael Aufhauser born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Augsburg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michael Aufhauser born?■Augsburg■Moug■Bersoch■Glalale■Ma■Glon■Wa\n",
      "\n",
      "Input:      Where was Maurice Maunoury born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Alexandria■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Maurice Maunoury born?■Alexandria■Ma■Palphia■Lon■An■Lon■Ton■Ton\n",
      "\n",
      "Input:      Where was Bhai Balmukund born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Pakistan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Bhai Balmukund born?■Pakistan■Oxford■Wind■Pa■Serd■Now■Sy■Pa■C\n",
      "\n",
      "Input:      Where was Rod Anderson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Australia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Rod Anderson born?■Australia■Mause■Rompe■Cham■ch■Dublicha■W\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = inputs[i]\n",
    "    x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "    y = targets[i]\n",
    "    y_pred = sample(model, x.view(1,-1), sample=True, device=DEVICE)\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"y_pred:     {decode_token_indices(y_pred[0])}\")\n",
    "    target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "    pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "    #print(f\"Target:     {target_str}\")\n",
    "    #print(f\"Prediction: {pred_str}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some sequences continuing from questions from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 4/4 [00:28<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num correct: 9, Accuracy: 0.00014375159723996933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "total = 0\n",
    "pbar = tqdm(val_dataloader, desc=\"Epochs\")\n",
    "for batch in pbar:\n",
    "    inputs, targets = batch\n",
    "    for i in range(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "        y = targets[i]\n",
    "        y_pred = sample(model, x.view(1,-1), sample=False, device=DEVICE)\n",
    "        #print(f\"Target:     {decode_token_indices(y)}\")\n",
    "        #print(f\"y_pred:     {decode_token_indices(y_pred[0])}\")\n",
    "        target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "        pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "        #print(f\"Target:     {target_str}\")\n",
    "        #print(f\"Prediction: {pred_str}\")\n",
    "        #print(\"\")\n",
    "        if(target_str==pred_str):\n",
    "            num_correct += 1\n",
    "            #print(f\"Input:      {decode_token_indices(x)}\")\n",
    "            #print(f\"Target:     {target_str}\")\n",
    "            #print(f\"Prediction: {pred_str}\")\n",
    "        total += len(inputs)\n",
    "\n",
    "print(f\"Num correct: {num_correct}, Accuracy: {num_correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
