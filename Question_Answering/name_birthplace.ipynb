{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simple Question Answering Using a Transformer Decoder**\n",
    "\n",
    "We will explore how to train a character-level tranformer language model for a simple question answering task. Given a question of the form `Where was [X] born?` where `[X]` is the name of a public figure, the model will be trained to predict the output `[Y]` which is the name of the birthplace of that person. Our goal is to first pretrain the model on a wikipedia corpus (next character prediction) from which it is expected to acquire knowledge of persons and their birthplaces. Then we finetune the model with supervised training on `(x,y)` sequence pairs of the following form:\n",
    "\n",
    "`x: Where was Albert Einstein born?%Germany%□□□□□□□□□□□□□□`\n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "where `x` is the input sequence and `y` is the predicted output sequence and `□` is a special padding token. This is a simple next character prediction task, however we do not want the model to predict the question itself, only the answer, which is why in the output sequence, we replace all characters from the question with the padding token and only have the model predict the characters from the answer. \n",
    "\n",
    "i.e. instead of\n",
    "\n",
    "`y: here was Albert Einstein born?%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "we use \n",
    "\n",
    "`y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□%Germany%□□□□□□□□□□□□□□□`\n",
    "\n",
    "We've also used a special token `%` to mark the beginning and end of the span containing the answer.\n",
    "\n",
    "**The idea is that by training the model on this task, it can learn to answer a question by retreiving information pertaining to the answer from it's pretrained knowledge.** After training, we can test this idea by giving the model an input sequence which does not contain an answer, i.e. after the start of answer token `%`, we fill the rest of the sequence with padding tokens: \n",
    "\n",
    "`x: Where was Enrico Fermi born?%□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "Then if the predicted output sequence contains the right answer, then it will support our idea. We also make sure that person names which were not in the training set will be used during testing.\n",
    "\n",
    "\n",
    "We will first train the model on the finetuning task without pretraining it and then look at the difference in performance with and without pretraining. \n",
    "\n",
    "(Note: torch.nn.TransformerDecoder does not support autoregressive decoding. Beware!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanzids\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask token: ■\n",
      " token: □\n"
     ]
    }
   ],
   "source": [
    "mask_token = u\"\\u25A0\"\n",
    "pad_token = u\"\\u25A1\"\n",
    "\n",
    "print(f\"mask token: {mask_token}\")\n",
    "print(f\" token: {pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: ['□', '■', '\\n', ' ', '!', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '£', '\\xad', 'Á', 'Å', 'É', 'Ó', 'Ö', 'Ø', 'Ü', 'ß', 'à', 'á', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ë', 'í', 'ï', 'ñ', 'ó', 'ô', 'ö', 'ø', 'ü', 'ý', 'ă', 'ą', 'ć', 'Č', 'č', 'ě', 'ğ', 'ī', 'İ', 'ı', 'ł', 'ń', 'ō', 'Ő', 'ő', 'œ', 'ř', 'ś', 'ş', 'Š', 'š', 'ť', 'ū', 'Ż', 'ż', 'Ž', 'ž', 'ș', 'Γ', 'Μ', 'ά', 'έ', 'α', 'γ', 'η', 'ι', 'κ', 'ν', 'ο', 'ρ', 'ς', 'τ', 'υ', 'ω', 'ώ', 'Ј', 'А', 'В', 'Г', 'И', 'К', 'П', 'Р', 'С', 'а', 'б', 'в', 'г', 'д', 'е', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'р', 'с', 'т', 'ц', 'ч', 'ь', 'я', 'ћ', 'א', 'ג', 'ה', 'ו', 'ז', 'ח', 'י', 'כ', 'ל', 'ם', 'מ', 'נ', 'ס', 'ץ', 'ר', 'ש', 'ا', 'ب', 'ت', 'ح', 'خ', 'د', 'ر', 'ش', 'ط', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'پ', 'ک', 'ی', 'क', 'द', 'प', 'ल', 'ा', 'ी', 'ौ', '\\u200e', '†', '−', 'ァ', 'ィ', 'イ', 'テ', 'ニ', 'フ', 'マ', 'レ', '・', 'ー', '一', '久', '佳', '前', '剛', '口', '忠', '木', '本', '田', '蛮', '西', '里', '野']\n"
     ]
    }
   ],
   "source": [
    "# first get the character vocabulry from the pretraining dataset\n",
    "with open(\"birth_place_data/wiki.txt\", 'r', encoding='utf-8') as file:\n",
    "    pretrain_text = file.read()\n",
    "\n",
    "vocab = list(sorted(list(set(pretrain_text))))\n",
    "assert mask_token not in vocab, \"mask token should not be in the vocabulary\"\n",
    "assert pad_token not in vocab, \"pad token should not be in the vocabulary\"\n",
    "vocab = [pad_token, mask_token] + vocab \n",
    "print(f\"vocabulary: {vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 256\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocab size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameBirthplaceDataset(Dataset):\n",
    "    def __init__(self, vocab, mask_token, pad_token, block_size=128, split=\"train\"):\n",
    "        self.vocab= vocab\n",
    "        self.ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "        self.mask_token = mask_token \n",
    "        self.pad_token = pad_token\n",
    "        self.block_size = block_size\n",
    "        if split == \"train\":\n",
    "            data_filename=\"birth_place_data/birth_places_train.tsv\"\n",
    "        elif split == \"dev\":\n",
    "            data_filename=\"birth_place_data/birth_places_dev.tsv\"\n",
    "        self.data = self.read_data(data_filename)\n",
    "         \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f: \n",
    "            lines = f.read()\n",
    "        data = list(lines.encode('utf-8').decode('ascii', errors='ignore').split('\\n'))\n",
    "        return data    \n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        return self.ctoi[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def mask_token_index(self):\n",
    "        return self.ctoi[self.mask_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        line = self.data[index]\n",
    "        question, answer = line.split('\\t') \n",
    "        question, answer = list(question), list(answer) \n",
    "        x = question + [self.mask_token] + answer + [self.mask_token]\n",
    "        x = x + (self.block_size-len(x)) * [self.pad_token] \n",
    "        y = x[1:]\n",
    "        x = x[:-1] \n",
    "        y[:len(question)-1] = (len(question)-1) * [self.pad_token]\n",
    "\n",
    "        x = torch.tensor([self.ctoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.ctoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `Span Corrpution` dataset for pre-training. This is a modified next word prediction task where an input sequence is first truncated to a random lengtg, then random-sized spans of contiguous tokens in the input sequence are corruputed, each entire span is replaced with a single mask token. For simplicity, we will only corrput a single span in each sequence. This span of tokens is then appended to the end of the sequence and its beginning and end are marked by the mask token (similar to how the beginning and end of the answer in the finetuning task is marked by the mask token, for good reason...). Then we fill the rest of the sequence with pad tokens to make it block_size long.\n",
    "\n",
    "e.g. Given the sequence `x: Where was Enrico Fermi born?`\n",
    "\n",
    "we first randomly truncate it:\n",
    "\n",
    "`x: Where was Enrico Ferm`\n",
    "\n",
    "Then we corrput a span:\n",
    "\n",
    "`x: Where was En% Ferm%rico%`\n",
    "\n",
    "and now fill with pad tokens:\n",
    "\n",
    "`x: Where was En% Ferm%rico%□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "Finally, the output sequence is just the usual shifted version of the input:\n",
    "\n",
    "`x: here was En% Ferm%rico%□□□□□□□□□□□□□□□□□□□□□□□□□□`\n",
    "\n",
    "We will choose random truncation lengths between `4 and 7/8 * block_size` and a random corruption span length which is on average `1/4 * the truncated document length` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanCorruptionDataset(Dataset):\n",
    "    def __init__(self, vocab, mask_token, pad_token, block_size=128):\n",
    "        self.vocab= vocab\n",
    "        self.ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "        self.mask_token = mask_token \n",
    "        self.pad_token = pad_token\n",
    "        self.block_size = block_size\n",
    "        data_filename=\"birth_place_data/wiki.txt\"\n",
    "        self.data = self.read_data(data_filename)\n",
    "         \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f: \n",
    "            data = f.read().split('\\n')\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def pad_token_index(self):\n",
    "        return self.ctoi[self.pad_token]\n",
    "\n",
    "    @property\n",
    "    def mask_token_index(self):\n",
    "        return self.ctoi[self.mask_token]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get the sentence\n",
    "        line = self.data[index]\n",
    "        # apply random truncation\n",
    "        trunc_len = random.randint(4, int(self.block_size*7/8))\n",
    "        line_trunc = line[:trunc_len]\n",
    "        # apply random span corruption\n",
    "        # draw random number from gaussian with mean 1/4 * trunc_len and std 1/8 * trunc_len\n",
    "        span_len = min(max(0,int(random.gauss(mu=trunc_len/4, sigma=trunc_len/10))), int(0.8*trunc_len))\n",
    "        # draw random start position\n",
    "        span_start = random.randint(0, trunc_len-span_len)\n",
    "        # extract the span\n",
    "        span = line_trunc[span_start:span_start+span_len]\n",
    "        # replace the span with mask tokens\n",
    "        line_span_corrupted = line_trunc[:span_start] + self.mask_token + line_trunc[span_start+span_len:] + self.mask_token + span + self.mask_token \n",
    "        # add padding\n",
    "        line_span_corrupted = line_span_corrupted + (self.block_size-len(line_span_corrupted)) * self.pad_token\n",
    "        # input-target pair\n",
    "        x = line_span_corrupted[:-1]\n",
    "        y = line_span_corrupted[1:]\n",
    "        # convert to tensors\n",
    "        x = torch.tensor([self.ctoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.ctoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NameBirthplaceDataset(vocab, mask_token, pad_token)\n",
    "dev_data = NameBirthplaceDataset(vocab, mask_token, pad_token, split=\"dev\")\n",
    "\n",
    "pad_token_index = train_data.pad_token_index\n",
    "mask_token_index = train_data.mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where was Yang Yang born?■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "□□□□□□□□□□□□□□□□□□□□□□□□■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "x, y = train_data[1000]\n",
    "\n",
    "def decode_token_indices(x):\n",
    "    return \"\".join([vocab[i] for i in x])\n",
    "\n",
    "x_decoded = decode_token_indices(x)\n",
    "y_decoded = decode_token_indices(y)\n",
    "print(x_decoded)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = SpanCorruptionDataset(vocab, mask_token, pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lavrent■ Tiflis of a deacon, Ardaziani■i Ardaziani. Born in■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "avrent■ Tiflis of a deacon, Ardaziani■i Ardaziani. Born in■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n"
     ]
    }
   ],
   "source": [
    "x, y = pretrain_data[2654]\n",
    "\n",
    "x_decoded = decode_token_indices(x)\n",
    "y_decoded = decode_token_indices(y)\n",
    "print(x_decoded)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the transformer question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, total_head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        assert total_head_size % num_heads == 0, \"head_size needs to be integer multiple of num_heads\"\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.total_head_size = total_head_size \n",
    "        self.head_size = total_head_size // num_heads \n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # define parameters\n",
    "        self.key = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.query = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(embedding_dim, self.total_head_size, bias=False)\n",
    "        self.attn_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "        # non-parameter tensor of lower triangular ones\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        # we also need to apply a linear projection to make the output residual the same dimension as the input\n",
    "        self.proj = torch.nn.Linear(total_head_size, embedding_dim) \n",
    "        self.output_dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    # define forward pass, input shape: (B,T,C) where B=batch size, T=block_size, C=embedding_dim\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B,T,H) where H is the total_head_size\n",
    "        q = self.query(x) # (B,T,H)\n",
    "        v = self.value(x) # (B,T,H)\n",
    "\n",
    "        # reshape (B,T,H) --> (B,T,n,h), where n=num_heads and h=head_size and H=n*h\n",
    "        k = k.view(B,T,self.num_heads,self.head_size) \n",
    "        q = q.view(B,T,self.num_heads,self.head_size) \n",
    "        v = v.view(B,T,self.num_heads,self.head_size) \n",
    "\n",
    "        # now we transpose so that the num_heads is the second dimension followed by T,h\n",
    "        # this allows us to batch matrix mutliply for all heads simulataneously to compute their attention weights\n",
    "        # (B,T,n,h) --> (B,n,T,h) \n",
    "        k = k.transpose(1,2) \n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # use pytorch built-in function for faster computation of attention scores (set the 'is_causal' parameter for applying causal masking)\n",
    "        out = F.scaled_dot_product_attention(q,k,v,dropout_p=self.dropout_rate if self.training else 0,is_causal=True)\n",
    "\n",
    "        # we can transpose the output from (B,n,T,h) --> (B,T,n,h)\n",
    "        # since the last two dimensions of the transposed tensor are non-contiguous, we apply \n",
    "        # contiguous() which return a contiguous tensor\n",
    "        out = out.transpose(1,2).contiguous()\n",
    "\n",
    "        # finally we collapse the last two dimensions to get the concatenated output, (B,T,n,h) --> (B,T,n*h) \n",
    "        out = out.view(B,T,self.total_head_size)\n",
    "\n",
    "        # now we project the concatenated output so that it has the same dimensions as the multihead attention layer input\n",
    "        # (we need to add it with the input because of the residual connection, so need to be same size) \n",
    "        out = self.proj(out) # (B,T,C) \n",
    "\n",
    "        # apply dropout\n",
    "        out = self.output_dropout(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "# a simple mlp \n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        # we add extra computations by growing out the feed-forward hidden size by a factor of 4\n",
    "        # we also add an extra linear layer at the end to project the residual back to same dimensions as input\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embedding_dim, 4*embedding_dim),  \n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4*embedding_dim, embedding_dim), \n",
    "            torch.nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "\n",
    "# transformer block with residual connection and layer norm\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, block_size, embedding_dim, head_size, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(block_size, embedding_dim, head_size, num_heads, dropout_rate) # multi-head attention layer \n",
    "        self.ff = FeedForward(embedding_dim, dropout_rate)   # feed-forward layer\n",
    "        self.ln1 = torch.nn.LayerNorm(embedding_dim) # layer norm at input of multi-head attention\n",
    "        self.ln2 = torch.nn.LayerNorm(embedding_dim) # layer norm at input of feed-forward\n",
    "\n",
    "    # in the forward pass, concatenate the outputs from all the attention heads\n",
    "    def forward(self, x):\n",
    "        # residual connection between input and multi-head attention output\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        # residual connection between multi-head attention output and feed-forward output\n",
    "        x = x + self.ff(self.ln2(x)) \n",
    "        return x\n",
    "    \n",
    "\n",
    "# language model with multiple transformer blocks\n",
    "class TransformerLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, embedding_dim, head_size, num_heads, num_blocks, dropout_rate=0.2, pad_token_idx=0):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.head_size = head_size\n",
    "        self.hum_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "\n",
    "        '''\n",
    "        Define model parameters\n",
    "        '''\n",
    "        # token embedding layer \n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, embedding_dim) # shape: (vocab_size,C)\n",
    "        # position embedding layer\n",
    "        self.pos_embedding = torch.nn.Embedding(block_size, embedding_dim) # shape: (T,C)\n",
    "        # stack of transformer blocks\n",
    "        self.blocks = torch.nn.Sequential(*[TransformerBlock(block_size, embedding_dim, head_size, num_heads, dropout_rate) for _ in range(num_blocks)])\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # we also add a layer norm before the final output layer\n",
    "        self.ln_f = torch.nn.LayerNorm(embedding_dim)\n",
    "        # output layer logits\n",
    "        self.lm_head = torch.nn.Linear(head_size, vocab_size) # shape: (h,vocab_size)\n",
    "\n",
    "\n",
    "        # forward pass takes in a batch of input token sequences of shape (B,T) and corresponding targets of shape (B,T)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # get token embeddings\n",
    "        token_embeds = self.token_embedding(idx) # (B,T,C)\n",
    "        # add positional encoding\n",
    "        pos_embeds = self.pos_embedding(torch.arange(T, device=idx.device)) # (T,C) \n",
    "        x = self.dropout(token_embeds + pos_embeds) # (B,T,C)\n",
    "        # pass through transformer blocks\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        # apply layer norm\n",
    "        x = self.ln_f(x)  # (B,T,C)\n",
    "        # compute output logits \n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B,T,vocab_size = logits.shape\n",
    "            # reshape the logits and targets such that batch of input sequences are flattened into a single big input sequence\n",
    "            # i.e. (B,T) --> (B*T)\n",
    "            logits = logits.view(B*T,vocab_size) # reshaped to (B*T,vocab_size)\n",
    "            targets = targets.view(B*T) # reshaped to (B*T)\n",
    "            # compute cross entropy loss (i.e. average negative log likelihood)\n",
    "            loss = F.cross_entropy(logits, targets, ignore_index=self.pad_token_idx)\n",
    "        return logits, loss\n",
    "    \n",
    "    # generates new sequences continuing from a given batch of context tokens\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        self.eval() # swicth to inference mode\n",
    "        # batch of contexts, idx has shape (B,T)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # since we're using positional encoding, we need to crop idx if input sequence length exceeds block size (keep last block_size tokens)\n",
    "            idx_crop = idx[:,-self.block_size:] \n",
    "            # get predictions\n",
    "            logits, _ = self(idx_crop) # shape: (B,T,C)\n",
    "            # for each context sequence (in the batch), compute the probability of the next token using the logits of the last token in the context sequence\n",
    "            logits = logits[:,-1,:] # shape: (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            # sample from the probability distribution to get next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # shape: (B,1)\n",
    "            # append to the current context\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # shape: (B,T+1)\n",
    "        self.train() # swicth to train mode\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader,  grad_norm_clip=1.0, device=\"cpu\", num_epochs=10, val_every=1, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # reset gradients\n",
    "            model.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # clip gradients above threshold\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, L = inputs.shape\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            train_acc = num_correct / num_total        \n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\" : loss.item(), \"Moving Avg Loss\" : avg_loss, \"Val Loss\": val_loss}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_every is not None:\n",
    "            if epoch%val_every == 0:\n",
    "                # compute validation loss\n",
    "                val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\") \n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            B, L = inputs.shape\n",
    "            logits, loss = model(inputs, targets)\n",
    "            logits = logits.view(B,L,-1)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B,L)\n",
    "            mask = (targets != pad_token_index)\n",
    "            num_correct += sum([int(torch.allclose(targets[i][mask[i]], y_pred[i][mask[i]])) for i in range(B)])            \n",
    "            num_total += B\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(dataloader))\n",
    "        inputs, targets = batch\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        B, L = inputs.shape\n",
    "        logits, loss = model(inputs, targets)\n",
    "        y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "        y_pred = y_pred.view(B,L)\n",
    "    model.train()\n",
    "    return inputs, targets, y_pred\n",
    "\n",
    "\n",
    "def compute_accuracy(model, dataloader, device=\"cpu\"):\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    place_counts_actual = Counter()\n",
    "    place_counts_predicted = Counter()\n",
    "    places_correct = Counter()\n",
    "    pbar = tqdm(dataloader, desc=\"Epochs\")\n",
    "    for batch in pbar:\n",
    "        inputs, targets = batch\n",
    "        for i in range(len(inputs)):\n",
    "            x = inputs[i]\n",
    "            x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "            y = targets[i]\n",
    "            y_pred = sample(model, x.view(1,-1), model.block_size, sample=False, device=device)\n",
    "            target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "            pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "            if(target_str==pred_str):\n",
    "                num_correct += 1\n",
    "                places_correct[pred_str] += 1\n",
    "            place_counts_actual[target_str] += 1\n",
    "            place_counts_predicted[pred_str] += 1\n",
    "        total += len(inputs)\n",
    "    print(f\"Num correct: {num_correct}, Accuracy: {num_correct/total}\")\n",
    "    return place_counts_actual, place_counts_predicted, places_correct\n",
    "\n",
    "# sample a sequence from the model\n",
    "def sample(model, x, block_size, num_chars=40, sample=False, temperature=1.0, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        question_length = len(x.view(-1))\n",
    "        x = x.to(device)\n",
    "        for _ in range(num_chars):\n",
    "            # crop the input sequence so that it doesn't exceed block size (only keep the last block_size tokens in the sequence to generate the next token)\n",
    "            x = x[:,-block_size:]\n",
    "            logits, _ = model(x) # shape: (1,L,V)      \n",
    "            # sample from the distribution to get the next character\n",
    "            p = F.softmax(logits[:,-1,:]/temperature, dim=-1) # shape: (V,)\n",
    "            if sample:\n",
    "                next_char_idx = torch.multinomial(p, num_samples=1)\n",
    "            else:\n",
    "                _, next_char_idx = torch.topk(p, k=1, dim=-1)\n",
    "            # append to the sequence\n",
    "            x = torch.cat((x, next_char_idx), dim=1)\n",
    "    model.train()\n",
    "    return x\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    if filename:\n",
    "        torch.save(checkpoint, filename)\n",
    "    else:\n",
    "        torch.save(checkpoint, 'qa_model_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer, filename=None):\n",
    "    if filename:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        checkpoint = torch.load('qa_model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 3.320576 M\n",
      "RAM used: 1132.83 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "D = 256\n",
    "vocab_size = len(vocab)\n",
    "block_size = 128\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 5e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(dev_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, block_size, D, D, num_heads, num_layers, dropout_rate=0.1, pad_token_idx=train_data.pad_token_index).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 2.577, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:02<00:00,  7.63it/s]\n",
      "Epoch 2, EMA Train Loss: 2.361, Train Accuracy:  0.001, Val Loss:  2.517, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  8.88it/s]\n",
      "Epoch 3, EMA Train Loss: 2.202, Train Accuracy:  0.000, Val Loss:  2.214, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  9.10it/s]\n",
      "Epoch 4, EMA Train Loss: 2.114, Train Accuracy:  0.003, Val Loss:  2.126, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  9.11it/s]\n",
      "Epoch 5, EMA Train Loss: 2.047, Train Accuracy:  0.001, Val Loss:  2.074, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  9.15it/s]\n",
      "Epoch 6, EMA Train Loss: 1.975, Train Accuracy:  0.001, Val Loss:  2.030, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  8.93it/s]\n",
      "Epoch 7, EMA Train Loss: 1.886, Train Accuracy:  0.013, Val Loss:  1.967, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.11it/s]\n",
      "Epoch 8, EMA Train Loss: 1.806, Train Accuracy:  0.009, Val Loss:  1.944, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 9, EMA Train Loss: 1.710, Train Accuracy:  0.004, Val Loss:  1.846, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  8.97it/s]\n",
      "Epoch 10, EMA Train Loss: 1.632, Train Accuracy:  0.013, Val Loss:  1.788, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.18it/s]\n",
      "Epoch 11, EMA Train Loss: 1.517, Train Accuracy:  0.007, Val Loss:  1.728, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  8.96it/s]\n",
      "Epoch 12, EMA Train Loss: 1.415, Train Accuracy:  0.018, Val Loss:  1.655, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.04it/s]\n",
      "Epoch 13, EMA Train Loss: 1.318, Train Accuracy:  0.021, Val Loss:  1.584, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  8.97it/s]\n",
      "Epoch 14, EMA Train Loss: 1.244, Train Accuracy:  0.018, Val Loss:  1.521, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.01it/s]\n",
      "Epoch 15, EMA Train Loss: 1.152, Train Accuracy:  0.021, Val Loss:  1.502, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  8.92it/s]\n",
      "Epoch 16, EMA Train Loss: 1.071, Train Accuracy:  0.025, Val Loss:  1.495, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.14it/s]\n",
      "Epoch 17, EMA Train Loss: 0.996, Train Accuracy:  0.021, Val Loss:  1.448, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  8.99it/s]\n",
      "Epoch 18, EMA Train Loss: 0.951, Train Accuracy:  0.025, Val Loss:  1.457, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00,  9.00it/s]\n",
      "Epoch 19, EMA Train Loss: 0.897, Train Accuracy:  0.029, Val Loss:  1.434, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.02it/s]\n",
      "Epoch 20, EMA Train Loss: 0.850, Train Accuracy:  0.035, Val Loss:  1.450, Val Accuracy:  0.034: 100%|██████████| 16/16 [00:01<00:00,  9.04it/s]\n",
      "Epoch 21, EMA Train Loss: 0.810, Train Accuracy:  0.035, Val Loss:  1.439, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  9.25it/s]\n",
      "Epoch 22, EMA Train Loss: 0.769, Train Accuracy:  0.043, Val Loss:  1.452, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.15it/s]\n",
      "Epoch 23, EMA Train Loss: 0.738, Train Accuracy:  0.038, Val Loss:  1.492, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "Epoch 24, EMA Train Loss: 0.702, Train Accuracy:  0.043, Val Loss:  1.476, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00,  9.06it/s]\n",
      "Epoch 25, EMA Train Loss: 0.668, Train Accuracy:  0.049, Val Loss:  1.474, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.01it/s]\n",
      "Epoch 26, EMA Train Loss: 0.642, Train Accuracy:  0.058, Val Loss:  1.515, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  9.10it/s]\n",
      "Epoch 27, EMA Train Loss: 0.615, Train Accuracy:  0.064, Val Loss:  1.509, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  8.99it/s]\n",
      "Epoch 28, EMA Train Loss: 0.590, Train Accuracy:  0.070, Val Loss:  1.583, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.30it/s]\n",
      "Epoch 29, EMA Train Loss: 0.566, Train Accuracy:  0.069, Val Loss:  1.583, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.27it/s]\n",
      "Epoch 30, EMA Train Loss: 0.546, Train Accuracy:  0.096, Val Loss:  1.590, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.48it/s]\n",
      "Epoch 31, EMA Train Loss: 0.522, Train Accuracy:  0.097, Val Loss:  1.628, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.52it/s]\n",
      "Epoch 32, EMA Train Loss: 0.502, Train Accuracy:  0.117, Val Loss:  1.648, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 33, EMA Train Loss: 0.476, Train Accuracy:  0.127, Val Loss:  1.665, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00,  9.32it/s]\n",
      "Epoch 34, EMA Train Loss: 0.465, Train Accuracy:  0.155, Val Loss:  1.682, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.50it/s]\n",
      "Epoch 35, EMA Train Loss: 0.441, Train Accuracy:  0.169, Val Loss:  1.732, Val Accuracy:  0.020: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 36, EMA Train Loss: 0.427, Train Accuracy:  0.176, Val Loss:  1.746, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00,  9.39it/s]\n",
      "Epoch 37, EMA Train Loss: 0.409, Train Accuracy:  0.189, Val Loss:  1.784, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  8.99it/s]\n",
      "Epoch 38, EMA Train Loss: 0.390, Train Accuracy:  0.208, Val Loss:  1.790, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.46it/s]\n",
      "Epoch 39, EMA Train Loss: 0.370, Train Accuracy:  0.228, Val Loss:  1.811, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.24it/s]\n",
      "Epoch 40, EMA Train Loss: 0.353, Train Accuracy:  0.245, Val Loss:  1.863, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  8.98it/s]\n",
      "Epoch 41, EMA Train Loss: 0.345, Train Accuracy:  0.283, Val Loss:  1.873, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.42it/s]\n",
      "Epoch 42, EMA Train Loss: 0.328, Train Accuracy:  0.295, Val Loss:  1.889, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.12it/s]\n",
      "Epoch 43, EMA Train Loss: 0.315, Train Accuracy:  0.324, Val Loss:  1.914, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.33it/s]\n",
      "Epoch 44, EMA Train Loss: 0.287, Train Accuracy:  0.358, Val Loss:  1.929, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 45, EMA Train Loss: 0.277, Train Accuracy:  0.381, Val Loss:  1.941, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.25it/s]\n",
      "Epoch 46, EMA Train Loss: 0.271, Train Accuracy:  0.406, Val Loss:  1.953, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.43it/s]\n",
      "Epoch 47, EMA Train Loss: 0.254, Train Accuracy:  0.448, Val Loss:  2.008, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.40it/s]\n",
      "Epoch 48, EMA Train Loss: 0.236, Train Accuracy:  0.472, Val Loss:  2.013, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.41it/s]\n",
      "Epoch 49, EMA Train Loss: 0.221, Train Accuracy:  0.508, Val Loss:  2.056, Val Accuracy:  0.002: 100%|██████████| 16/16 [00:01<00:00,  9.44it/s]\n",
      "Epoch 50, EMA Train Loss: 0.207, Train Accuracy:  0.521, Val Loss:  2.033, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51, EMA Train Loss: 0.197, Train Accuracy:  0.541, Val Loss:  2.080, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.24it/s]\n",
      "Epoch 52, EMA Train Loss: 0.189, Train Accuracy:  0.566, Val Loss:  2.103, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.40it/s]\n",
      "Epoch 53, EMA Train Loss: 0.173, Train Accuracy:  0.593, Val Loss:  2.118, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n",
      "Epoch 54, EMA Train Loss: 0.166, Train Accuracy:  0.615, Val Loss:  2.116, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.28it/s]\n",
      "Epoch 55, EMA Train Loss: 0.152, Train Accuracy:  0.663, Val Loss:  2.115, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.48it/s]\n",
      "Epoch 56, EMA Train Loss: 0.141, Train Accuracy:  0.670, Val Loss:  2.185, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.48it/s]\n",
      "Epoch 57, EMA Train Loss: 0.131, Train Accuracy:  0.704, Val Loss:  2.189, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n",
      "Epoch 58, EMA Train Loss: 0.124, Train Accuracy:  0.709, Val Loss:  2.216, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.49it/s]\n",
      "Epoch 59, EMA Train Loss: 0.123, Train Accuracy:  0.716, Val Loss:  2.263, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 60, EMA Train Loss: 0.110, Train Accuracy:  0.739, Val Loss:  2.265, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 61, EMA Train Loss: 0.107, Train Accuracy:  0.745, Val Loss:  2.287, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.32it/s]\n",
      "Epoch 62, EMA Train Loss: 0.101, Train Accuracy:  0.757, Val Loss:  2.298, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  9.41it/s]\n",
      "Epoch 63, EMA Train Loss: 0.094, Train Accuracy:  0.778, Val Loss:  2.309, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.44it/s]\n",
      "Epoch 64, EMA Train Loss: 0.095, Train Accuracy:  0.775, Val Loss:  2.297, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.21it/s]\n",
      "Epoch 65, EMA Train Loss: 0.086, Train Accuracy:  0.797, Val Loss:  2.334, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.44it/s]\n",
      "Epoch 66, EMA Train Loss: 0.084, Train Accuracy:  0.795, Val Loss:  2.359, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.14it/s]\n",
      "Epoch 67, EMA Train Loss: 0.079, Train Accuracy:  0.815, Val Loss:  2.355, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  8.60it/s]\n",
      "Epoch 68, EMA Train Loss: 0.079, Train Accuracy:  0.816, Val Loss:  2.387, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.29it/s]\n",
      "Epoch 69, EMA Train Loss: 0.073, Train Accuracy:  0.823, Val Loss:  2.379, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 70, EMA Train Loss: 0.069, Train Accuracy:  0.819, Val Loss:  2.383, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.21it/s]\n",
      "Epoch 71, EMA Train Loss: 0.067, Train Accuracy:  0.841, Val Loss:  2.426, Val Accuracy:  0.010: 100%|██████████| 16/16 [00:01<00:00,  9.41it/s]\n",
      "Epoch 72, EMA Train Loss: 0.060, Train Accuracy:  0.854, Val Loss:  2.430, Val Accuracy:  0.002: 100%|██████████| 16/16 [00:01<00:00,  9.46it/s]\n",
      "Epoch 73, EMA Train Loss: 0.063, Train Accuracy:  0.840, Val Loss:  2.427, Val Accuracy:  0.004: 100%|██████████| 16/16 [00:01<00:00,  9.35it/s]\n",
      "Epoch 74, EMA Train Loss: 0.061, Train Accuracy:  0.848, Val Loss:  2.437, Val Accuracy:  0.006: 100%|██████████| 16/16 [00:01<00:00,  9.41it/s]\n",
      "Epoch 75, EMA Train Loss: 0.061, Train Accuracy:  0.845, Val Loss:  2.455, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.35it/s]\n",
      "Epoch 76, EMA Train Loss: 0.066, Train Accuracy:  0.832, Val Loss:  2.464, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "Epoch 77, EMA Train Loss: 0.060, Train Accuracy:  0.855, Val Loss:  2.445, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.46it/s]\n",
      "Epoch 78, EMA Train Loss: 0.061, Train Accuracy:  0.852, Val Loss:  2.477, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n",
      "Epoch 79, EMA Train Loss: 0.056, Train Accuracy:  0.858, Val Loss:  2.509, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.41it/s]\n",
      "Epoch 80, EMA Train Loss: 0.056, Train Accuracy:  0.867, Val Loss:  2.524, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.39it/s]\n",
      "Epoch 81, EMA Train Loss: 0.053, Train Accuracy:  0.866, Val Loss:  2.564, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 82, EMA Train Loss: 0.054, Train Accuracy:  0.869, Val Loss:  2.567, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.29it/s]\n",
      "Epoch 83, EMA Train Loss: 0.054, Train Accuracy:  0.868, Val Loss:  2.578, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "Epoch 84, EMA Train Loss: 0.052, Train Accuracy:  0.873, Val Loss:  2.577, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.40it/s]\n",
      "Epoch 85, EMA Train Loss: 0.048, Train Accuracy:  0.875, Val Loss:  2.607, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.53it/s]\n",
      "Epoch 86, EMA Train Loss: 0.053, Train Accuracy:  0.874, Val Loss:  2.622, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 87, EMA Train Loss: 0.053, Train Accuracy:  0.868, Val Loss:  2.652, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 88, EMA Train Loss: 0.052, Train Accuracy:  0.879, Val Loss:  2.631, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 89, EMA Train Loss: 0.047, Train Accuracy:  0.887, Val Loss:  2.598, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.43it/s]\n",
      "Epoch 90, EMA Train Loss: 0.048, Train Accuracy:  0.872, Val Loss:  2.646, Val Accuracy:  0.016: 100%|██████████| 16/16 [00:01<00:00,  9.35it/s]\n",
      "Epoch 91, EMA Train Loss: 0.044, Train Accuracy:  0.884, Val Loss:  2.655, Val Accuracy:  0.008: 100%|██████████| 16/16 [00:01<00:00,  9.51it/s]\n",
      "Epoch 92, EMA Train Loss: 0.044, Train Accuracy:  0.880, Val Loss:  2.657, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.39it/s]\n",
      "Epoch 93, EMA Train Loss: 0.044, Train Accuracy:  0.888, Val Loss:  2.652, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.40it/s]\n",
      "Epoch 94, EMA Train Loss: 0.044, Train Accuracy:  0.886, Val Loss:  2.667, Val Accuracy:  0.012: 100%|██████████| 16/16 [00:01<00:00,  9.46it/s]\n",
      "Epoch 95, EMA Train Loss: 0.044, Train Accuracy:  0.883, Val Loss:  2.698, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.45it/s]\n",
      "Epoch 96, EMA Train Loss: 0.044, Train Accuracy:  0.885, Val Loss:  2.685, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.29it/s]\n",
      "Epoch 97, EMA Train Loss: 0.043, Train Accuracy:  0.877, Val Loss:  2.691, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n",
      "Epoch 98, EMA Train Loss: 0.042, Train Accuracy:  0.893, Val Loss:  2.725, Val Accuracy:  0.014: 100%|██████████| 16/16 [00:01<00:00,  9.31it/s]\n",
      "Epoch 99, EMA Train Loss: 0.041, Train Accuracy:  0.895, Val Loss:  2.679, Val Accuracy:  0.018: 100%|██████████| 16/16 [00:01<00:00,  9.31it/s]\n",
      "Epoch 100, EMA Train Loss: 0.041, Train Accuracy:  0.893, Val Loss:  2.691, Val Accuracy:  0.022: 100%|██████████| 16/16 [00:01<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=100, save_every=50, val_every=1) #, log_metrics=log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Gerald Murphy born?■Boston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Boston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: dimimip■diSrymieLjmaea■Jauag■Muston■S■raco■■■■■i■e■■aa■nCuroo■cwurcD■u■■w■kcrrnrw■■n■aa■■C■r■c■■nkw■krnn■r■rarwkanrrow■■rw■■nen\n",
      "\n",
      "Input:      Where was John Brown born?■Sheffield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Sheffield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: dimimip■diSuiaEirnadCarg■■Ehaffield■L■dr■ddr■■d■■■d■k■■■■k■■■■cwwrrK■■d■■rkSrr■rd■■d■d■■■k■aK■■■■n■r■rr■dr■r■rww■a■Ko■rark■■a■r\n",
      "\n",
      "Input:      Where was Leslie Howe born?■Ontario■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Ontario■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: dimimip■dioiSinrnwl■rulnmd■Srka■ya■L■ooaoo■ooooo■eooo■krokroo■■wworo■i■awrkooruroowr■raoooonopooaooworwk■rorwLwwoorroooorw■■wer\n",
      "\n",
      "Input:      Where was Brian Murphy born?■Ottawa■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Ottawa■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: dimimip■diuanm■CanleilKyre■■Bstani■Bp■oa■■■rk■oi■irir■■nrirnr■cwwrcr■kk■wruorrrrk■wr■ra■■kkrorrknkrwkrw■■r■rarwrrkrrr■■■rw■■akr\n",
      "\n",
      "Input:      Where was Gabriel Tschumi born?■Switzerland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Switzerland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "Prediction: dimimip■diSyaajliuohral■neaaeg■Daitzerland■D■■dt■e■■k■■ati■u■ecwuucr■i■■w■nRorne■■■nnkyn■k■nhpkantek■ei■■r■■■Lwwan■no■■aik■■nk■\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, y_pred = evaluate(model, val_dataloader, device=DEVICE)\n",
    "for i in range(5):\n",
    "    x = inputs[i]\n",
    "    y = targets[i]\n",
    "    y_hat = y_pred[i]\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"Prediction: {decode_token_indices(y_hat)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Gerald Murphy born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Boston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Gerald Murphy born?■Moscow■Lismon■Ron■Le■Buste■Ch■Ch■Churere\n",
      "\n",
      "Input:      Where was John Brown born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Sheffield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Brown born?■London■■Argen■■In■Ron■Jen■Scen■Am■Lon■Pa\n",
      "\n",
      "Input:      Where was Leslie Howe born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Ontario■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Leslie Howe born?■Sheffield■Delberd■Kierk■Keria■Gerie■Wa■G\n",
      "\n",
      "Input:      Where was Brian Murphy born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Ottawa■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Brian Murphy born?■Brooklyn■De■Dan■J■Bin■C■Bry■Be■Dubam■Dub\n",
      "\n",
      "Input:      Where was Gabriel Tschumi born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Switzerland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Gabriel Tschumi born?■London■Ton■Azin■Ca■De■De■Ch■De■Ton■Pa■To\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x = inputs[i]\n",
    "    x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "    y = targets[i]\n",
    "    y_pred = sample(model, x.view(1,-1), block_size, sample=True, device=DEVICE)\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"y_pred:     {decode_token_indices(y_pred[0])}\")\n",
    "    target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "    pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "    #print(f\"Target:     {target_str}\")\n",
    "    #print(f\"Prediction: {pred_str}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some sequences continuing from questions from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 4/4 [00:32<00:00,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num correct: 10, Accuracy: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 16/16 [01:57<00:00,  7.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num correct: 231, Accuracy: 0.1155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(model, train_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the validation set accuracy is barely 2%. Now we will pre-train the language model on the wikipedia data with span corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 3.320576 M\n",
      "RAM used: 674.06 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "D = 256\n",
    "vocab_size = len(vocab)\n",
    "block_size = 128\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "learning_rate = 8e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "train_dataloader = DataLoader(pretrain_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "model = TransformerLanguageModel(vocab_size, block_size, D, D, num_heads, num_layers, dropout_rate=0.1, pad_token_idx=train_data.pad_token_index).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(model, optimizer, scheduler, train_dataloader, val_dataloader=None, device=DEVICE, num_epochs=500, save_every=50, val_every=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's finetune the model on the name brithplace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from checkpoint!\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(dev_data, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 6e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 0.228, Train Accuracy:  0.404, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 16/16 [00:01<00:00,  8.49it/s]\n",
      "Epoch 2, EMA Train Loss: 0.258, Train Accuracy:  0.417, Val Loss:  0.397, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00, 10.00it/s]\n",
      "Epoch 3, EMA Train Loss: 0.267, Train Accuracy:  0.412, Val Loss:  0.400, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.70it/s]\n",
      "Epoch 4, EMA Train Loss: 0.268, Train Accuracy:  0.412, Val Loss:  0.400, Val Accuracy:  0.330: 100%|██████████| 16/16 [00:01<00:00,  9.82it/s]\n",
      "Epoch 5, EMA Train Loss: 0.267, Train Accuracy:  0.408, Val Loss:  0.399, Val Accuracy:  0.328: 100%|██████████| 16/16 [00:01<00:00,  9.68it/s]\n",
      "Epoch 6, EMA Train Loss: 0.263, Train Accuracy:  0.426, Val Loss:  0.399, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.87it/s]\n",
      "Epoch 7, EMA Train Loss: 0.267, Train Accuracy:  0.404, Val Loss:  0.400, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.88it/s]\n",
      "Epoch 8, EMA Train Loss: 0.262, Train Accuracy:  0.420, Val Loss:  0.400, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.97it/s]\n",
      "Epoch 9, EMA Train Loss: 0.264, Train Accuracy:  0.425, Val Loss:  0.398, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.72it/s]\n",
      "Epoch 10, EMA Train Loss: 0.269, Train Accuracy:  0.424, Val Loss:  0.401, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:02<00:00,  7.07it/s]\n",
      "Epoch 11, EMA Train Loss: 0.268, Train Accuracy:  0.398, Val Loss:  0.400, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  8.71it/s]\n",
      "Epoch 12, EMA Train Loss: 0.256, Train Accuracy:  0.442, Val Loss:  0.399, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:02<00:00,  7.75it/s]\n",
      "Epoch 13, EMA Train Loss: 0.268, Train Accuracy:  0.418, Val Loss:  0.401, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.05it/s]\n",
      "Epoch 14, EMA Train Loss: 0.257, Train Accuracy:  0.435, Val Loss:  0.400, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.23it/s]\n",
      "Epoch 15, EMA Train Loss: 0.258, Train Accuracy:  0.436, Val Loss:  0.400, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.54it/s]\n",
      "Epoch 16, EMA Train Loss: 0.257, Train Accuracy:  0.436, Val Loss:  0.400, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.28it/s]\n",
      "Epoch 17, EMA Train Loss: 0.260, Train Accuracy:  0.426, Val Loss:  0.400, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.15it/s]\n",
      "Epoch 18, EMA Train Loss: 0.251, Train Accuracy:  0.438, Val Loss:  0.401, Val Accuracy:  0.344: 100%|██████████| 16/16 [00:01<00:00,  9.20it/s]\n",
      "Epoch 19, EMA Train Loss: 0.250, Train Accuracy:  0.439, Val Loss:  0.403, Val Accuracy:  0.348: 100%|██████████| 16/16 [00:01<00:00,  9.08it/s]\n",
      "Epoch 20, EMA Train Loss: 0.248, Train Accuracy:  0.437, Val Loss:  0.400, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.24it/s]\n",
      "Epoch 21, EMA Train Loss: 0.246, Train Accuracy:  0.448, Val Loss:  0.401, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.28it/s]\n",
      "Epoch 22, EMA Train Loss: 0.246, Train Accuracy:  0.444, Val Loss:  0.402, Val Accuracy:  0.346: 100%|██████████| 16/16 [00:01<00:00,  8.91it/s]\n",
      "Epoch 23, EMA Train Loss: 0.252, Train Accuracy:  0.445, Val Loss:  0.401, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.55it/s]\n",
      "Epoch 24, EMA Train Loss: 0.246, Train Accuracy:  0.447, Val Loss:  0.402, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 25, EMA Train Loss: 0.244, Train Accuracy:  0.448, Val Loss:  0.403, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n",
      "Epoch 26, EMA Train Loss: 0.247, Train Accuracy:  0.444, Val Loss:  0.402, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.30it/s]\n",
      "Epoch 27, EMA Train Loss: 0.243, Train Accuracy:  0.458, Val Loss:  0.403, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.43it/s]\n",
      "Epoch 28, EMA Train Loss: 0.243, Train Accuracy:  0.439, Val Loss:  0.401, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.60it/s]\n",
      "Epoch 29, EMA Train Loss: 0.248, Train Accuracy:  0.441, Val Loss:  0.402, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.49it/s]\n",
      "Epoch 30, EMA Train Loss: 0.238, Train Accuracy:  0.461, Val Loss:  0.404, Val Accuracy:  0.344: 100%|██████████| 16/16 [00:01<00:00,  8.97it/s]\n",
      "Epoch 31, EMA Train Loss: 0.244, Train Accuracy:  0.435, Val Loss:  0.404, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.18it/s]\n",
      "Epoch 32, EMA Train Loss: 0.241, Train Accuracy:  0.441, Val Loss:  0.404, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.25it/s]\n",
      "Epoch 33, EMA Train Loss: 0.239, Train Accuracy:  0.462, Val Loss:  0.404, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 34, EMA Train Loss: 0.239, Train Accuracy:  0.459, Val Loss:  0.406, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 35, EMA Train Loss: 0.239, Train Accuracy:  0.459, Val Loss:  0.405, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.18it/s]\n",
      "Epoch 36, EMA Train Loss: 0.236, Train Accuracy:  0.459, Val Loss:  0.407, Val Accuracy:  0.342: 100%|██████████| 16/16 [00:01<00:00,  9.20it/s]\n",
      "Epoch 37, EMA Train Loss: 0.237, Train Accuracy:  0.464, Val Loss:  0.404, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.08it/s]\n",
      "Epoch 38, EMA Train Loss: 0.231, Train Accuracy:  0.473, Val Loss:  0.405, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.42it/s]\n",
      "Epoch 39, EMA Train Loss: 0.235, Train Accuracy:  0.445, Val Loss:  0.405, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.07it/s]\n",
      "Epoch 40, EMA Train Loss: 0.241, Train Accuracy:  0.457, Val Loss:  0.406, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.14it/s]\n",
      "Epoch 41, EMA Train Loss: 0.235, Train Accuracy:  0.460, Val Loss:  0.406, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 42, EMA Train Loss: 0.233, Train Accuracy:  0.458, Val Loss:  0.408, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.29it/s]\n",
      "Epoch 43, EMA Train Loss: 0.238, Train Accuracy:  0.479, Val Loss:  0.408, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  8.98it/s]\n",
      "Epoch 44, EMA Train Loss: 0.234, Train Accuracy:  0.464, Val Loss:  0.407, Val Accuracy:  0.330: 100%|██████████| 16/16 [00:01<00:00,  9.12it/s]\n",
      "Epoch 45, EMA Train Loss: 0.229, Train Accuracy:  0.470, Val Loss:  0.408, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.35it/s]\n",
      "Epoch 46, EMA Train Loss: 0.234, Train Accuracy:  0.458, Val Loss:  0.408, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 47, EMA Train Loss: 0.230, Train Accuracy:  0.477, Val Loss:  0.409, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.55it/s]\n",
      "Epoch 48, EMA Train Loss: 0.230, Train Accuracy:  0.479, Val Loss:  0.409, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.35it/s]\n",
      "Epoch 49, EMA Train Loss: 0.231, Train Accuracy:  0.478, Val Loss:  0.409, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.10it/s]\n",
      "Epoch 50, EMA Train Loss: 0.229, Train Accuracy:  0.472, Val Loss:  0.409, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "Epoch 51, EMA Train Loss: 0.228, Train Accuracy:  0.474, Val Loss:  0.409, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.18it/s]\n",
      "Epoch 52, EMA Train Loss: 0.226, Train Accuracy:  0.487, Val Loss:  0.410, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.47it/s]\n",
      "Epoch 53, EMA Train Loss: 0.228, Train Accuracy:  0.469, Val Loss:  0.410, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.40it/s]\n",
      "Epoch 54, EMA Train Loss: 0.222, Train Accuracy:  0.491, Val Loss:  0.410, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  8.91it/s]\n",
      "Epoch 55, EMA Train Loss: 0.226, Train Accuracy:  0.482, Val Loss:  0.411, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.08it/s]\n",
      "Epoch 56, EMA Train Loss: 0.225, Train Accuracy:  0.480, Val Loss:  0.411, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.02it/s]\n",
      "Epoch 57, EMA Train Loss: 0.218, Train Accuracy:  0.491, Val Loss:  0.411, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.22it/s]\n",
      "Epoch 58, EMA Train Loss: 0.218, Train Accuracy:  0.473, Val Loss:  0.413, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.17it/s]\n",
      "Epoch 59, EMA Train Loss: 0.222, Train Accuracy:  0.475, Val Loss:  0.413, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.14it/s]\n",
      "Epoch 60, EMA Train Loss: 0.214, Train Accuracy:  0.495, Val Loss:  0.411, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.29it/s]\n",
      "Epoch 61, EMA Train Loss: 0.219, Train Accuracy:  0.493, Val Loss:  0.414, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.22it/s]\n",
      "Epoch 62, EMA Train Loss: 0.217, Train Accuracy:  0.500, Val Loss:  0.413, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.54it/s]\n",
      "Epoch 63, EMA Train Loss: 0.219, Train Accuracy:  0.489, Val Loss:  0.413, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.46it/s]\n",
      "Epoch 64, EMA Train Loss: 0.216, Train Accuracy:  0.492, Val Loss:  0.414, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.21it/s]\n",
      "Epoch 65, EMA Train Loss: 0.220, Train Accuracy:  0.503, Val Loss:  0.413, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.33it/s]\n",
      "Epoch 66, EMA Train Loss: 0.215, Train Accuracy:  0.497, Val Loss:  0.414, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  8.88it/s]\n",
      "Epoch 67, EMA Train Loss: 0.212, Train Accuracy:  0.495, Val Loss:  0.413, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  8.97it/s]\n",
      "Epoch 68, EMA Train Loss: 0.217, Train Accuracy:  0.485, Val Loss:  0.414, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  8.90it/s]\n",
      "Epoch 69, EMA Train Loss: 0.208, Train Accuracy:  0.514, Val Loss:  0.414, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.32it/s]\n",
      "Epoch 70, EMA Train Loss: 0.211, Train Accuracy:  0.491, Val Loss:  0.414, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.22it/s]\n",
      "Epoch 71, EMA Train Loss: 0.209, Train Accuracy:  0.495, Val Loss:  0.413, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  8.97it/s]\n",
      "Epoch 72, EMA Train Loss: 0.209, Train Accuracy:  0.519, Val Loss:  0.415, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 73, EMA Train Loss: 0.207, Train Accuracy:  0.515, Val Loss:  0.414, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.17it/s]\n",
      "Epoch 74, EMA Train Loss: 0.203, Train Accuracy:  0.516, Val Loss:  0.416, Val Accuracy:  0.328: 100%|██████████| 16/16 [00:01<00:00,  9.32it/s]\n",
      "Epoch 75, EMA Train Loss: 0.208, Train Accuracy:  0.514, Val Loss:  0.418, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.32it/s]\n",
      "Epoch 76, EMA Train Loss: 0.207, Train Accuracy:  0.504, Val Loss:  0.418, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 77, EMA Train Loss: 0.207, Train Accuracy:  0.514, Val Loss:  0.419, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "Epoch 78, EMA Train Loss: 0.210, Train Accuracy:  0.516, Val Loss:  0.419, Val Accuracy:  0.340: 100%|██████████| 16/16 [00:01<00:00,  9.10it/s]\n",
      "Epoch 79, EMA Train Loss: 0.204, Train Accuracy:  0.509, Val Loss:  0.417, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.43it/s]\n",
      "Epoch 80, EMA Train Loss: 0.204, Train Accuracy:  0.510, Val Loss:  0.418, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.30it/s]\n",
      "Epoch 81, EMA Train Loss: 0.205, Train Accuracy:  0.511, Val Loss:  0.418, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.36it/s]\n",
      "Epoch 82, EMA Train Loss: 0.198, Train Accuracy:  0.521, Val Loss:  0.417, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.18it/s]\n",
      "Epoch 83, EMA Train Loss: 0.205, Train Accuracy:  0.519, Val Loss:  0.419, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.15it/s]\n",
      "Epoch 84, EMA Train Loss: 0.209, Train Accuracy:  0.495, Val Loss:  0.422, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.03it/s]\n",
      "Epoch 85, EMA Train Loss: 0.208, Train Accuracy:  0.496, Val Loss:  0.419, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.37it/s]\n",
      "Epoch 86, EMA Train Loss: 0.196, Train Accuracy:  0.540, Val Loss:  0.420, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.07it/s]\n",
      "Epoch 87, EMA Train Loss: 0.197, Train Accuracy:  0.529, Val Loss:  0.419, Val Accuracy:  0.338: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 88, EMA Train Loss: 0.203, Train Accuracy:  0.524, Val Loss:  0.419, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.23it/s]\n",
      "Epoch 89, EMA Train Loss: 0.202, Train Accuracy:  0.537, Val Loss:  0.420, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.26it/s]\n",
      "Epoch 90, EMA Train Loss: 0.199, Train Accuracy:  0.522, Val Loss:  0.420, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.00it/s]\n",
      "Epoch 91, EMA Train Loss: 0.198, Train Accuracy:  0.533, Val Loss:  0.422, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  9.07it/s]\n",
      "Epoch 92, EMA Train Loss: 0.202, Train Accuracy:  0.506, Val Loss:  0.420, Val Accuracy:  0.328: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 93, EMA Train Loss: 0.193, Train Accuracy:  0.537, Val Loss:  0.421, Val Accuracy:  0.330: 100%|██████████| 16/16 [00:01<00:00,  9.22it/s]\n",
      "Epoch 94, EMA Train Loss: 0.199, Train Accuracy:  0.529, Val Loss:  0.420, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.38it/s]\n",
      "Epoch 95, EMA Train Loss: 0.196, Train Accuracy:  0.535, Val Loss:  0.421, Val Accuracy:  0.334: 100%|██████████| 16/16 [00:01<00:00,  8.98it/s]\n",
      "Epoch 96, EMA Train Loss: 0.198, Train Accuracy:  0.525, Val Loss:  0.423, Val Accuracy:  0.336: 100%|██████████| 16/16 [00:01<00:00,  9.49it/s]\n",
      "Epoch 97, EMA Train Loss: 0.193, Train Accuracy:  0.538, Val Loss:  0.422, Val Accuracy:  0.332: 100%|██████████| 16/16 [00:01<00:00,  9.34it/s]\n",
      "Epoch 98, EMA Train Loss: 0.199, Train Accuracy:  0.509, Val Loss:  0.423, Val Accuracy:  0.330: 100%|██████████| 16/16 [00:01<00:00,  9.25it/s]\n",
      "Epoch 99, EMA Train Loss: 0.193, Train Accuracy:  0.528, Val Loss:  0.421, Val Accuracy:  0.330: 100%|██████████| 16/16 [00:01<00:00,  9.23it/s]\n",
      "Epoch 100, EMA Train Loss: 0.204, Train Accuracy:  0.522, Val Loss:  0.422, Val Accuracy:  0.328: 100%|██████████| 16/16 [00:01<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=100, save_every=None, val_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint!\n"
     ]
    }
   ],
   "source": [
    "save_model_checkpoint(model, optimizer, filename='qa_model_finetuned_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing  the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 2/4 [00:14<00:14,  7.06s/it]"
     ]
    }
   ],
   "source": [
    "place_counts_actual, place_counts_predicted, places_correct = compute_accuracy(model, val_dataloader, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual place counts: Counter({'London': 25, 'Chicago': 14, 'Paris': 12, 'England': 10, 'Milan': 7, 'Toronto': 7, 'Dublin': 7, 'Boston': 7, 'Berlin': 6, 'Edinburgh': 6, 'Philadelphia': 6, 'Sydney': 6, 'Birmingham': 6, 'Melbourne': 6, 'France': 6, 'Naples': 6, 'Baltimore': 6, 'Oslo': 6, 'Belfast': 5, 'Brooklyn': 5, 'Bucharest': 4, 'Minneapolis': 4, 'India': 4, 'Sheffield': 4, 'Detroit': 4, 'Athens': 3, 'Pennsylvania': 3, 'Helsinki': 3, 'Montreal': 3, 'Glasgow': 3, 'Bordeaux': 3, 'Bristol': 3, 'Havana': 3, 'Gothenburg': 3, 'Massachusetts': 3, 'Moscow': 3, 'Australia': 3, 'Prague': 3, 'Bradford': 3, 'Vienna': 3, 'Victoria': 3, 'Ireland': 3, 'Oxford': 3, 'Leicester': 3, 'Beijing': 3, 'Lahore': 3, 'Singapore': 3, 'Charleston': 3, 'Ottawa': 3, 'Dayton': 3, 'Stockholm': 3, 'Tokyo': 2, 'Budapest': 2, 'Jerusalem': 2, 'Venice': 2, 'Cleveland': 2, 'Warsaw': 2, 'Frankfurt': 2, 'Adelaide': 2, 'Kiev': 2, 'Kilkenny': 2, 'Norwich': 2, 'Rotterdam': 2, 'Ontario': 2, 'Edmonton': 2, 'Washington': 2, 'Colorado': 2, 'Perth': 2, 'Santiago': 2, 'Belgrade': 2, 'Mansfield': 2, 'Munich': 2, 'Memphis': 2, 'Richmond': 2, 'Bologna': 2, 'Toledo': 2, 'Leeds': 2, 'Mexico': 2, 'Leipzig': 2, 'Newark': 2, 'Russia': 2, 'Luxembourg': 2, 'Canada': 2, 'Yorkshire': 2, 'Finland': 2, 'Italy': 2, 'Germany': 2, 'Winnipeg': 2, 'Hamburg': 2, 'Rochester': 2, 'Illinois': 2, 'Norfolk': 2, 'Macon': 2, 'Aberdeen': 2, 'Mumbai': 2, 'Copenhagen': 2, 'Istanbul': 2, 'California': 2, 'Tulsa': 1, 'Brunswick': 1, 'Gloucester': 1, 'Carmichael': 1, 'Bern': 1, 'Guatemala': 1, 'Iran': 1, 'Galicia': 1, 'Dortmund': 1, 'Kensington': 1, 'Geelong': 1, 'Alberta': 1, 'Turin': 1, 'Tampa': 1, 'Stuttgart': 1, 'Wales': 1, 'Switzerland': 1, 'Ankara': 1, 'Indianapolis': 1, 'Seville': 1, 'Lebanon': 1, 'Louisiana': 1, 'Pakistan': 1, 'Ventura': 1, 'Holland': 1, 'Garland': 1, 'Surrey': 1, 'Swansea': 1, 'Cardiff': 1, 'Ipswich': 1, 'Zagreb': 1, 'Quincy': 1, 'Paul': 1, 'Exeter': 1, 'Riga': 1, 'Blackpool': 1, 'Arkansas': 1, 'Texas': 1, 'Kazan': 1, 'Manhattan': 1, 'Lawrence': 1, 'Israel': 1, 'Rome': 1, 'Lagos': 1, 'Algiers': 1, 'Calgary': 1, 'Hanover': 1, 'Lafayette': 1, 'Georgia': 1, 'Sweden': 1, 'Windsor': 1, 'Paisley': 1, 'Tehran': 1, 'Blackburn': 1, 'Sunderland': 1, 'Genoa': 1, 'Cincinnati': 1, 'Barbados': 1, 'Charlotte': 1, 'Indiana': 1, 'Virginia': 1, 'Bolton': 1, 'Burlington': 1, 'Bergen': 1, 'Medina': 1, 'Marseille': 1, 'Derbyshire': 1, 'Belgium': 1, 'Cumberland': 1, 'Bonn': 1, 'Taipei': 1, 'Newport': 1, 'Canberra': 1, 'Savannah': 1, 'Fulham': 1, 'Florida': 1, 'Wolverhampton': 1, 'Seoul': 1, 'Atlanta': 1, 'Casablanca': 1, 'Dresden': 1, 'Tirana': 1, 'Liverpool': 1, 'Vancouver': 1, 'Lyon': 1, 'Lima': 1, 'Lincoln': 1, 'Weston': 1, 'Miami': 1, 'Lviv': 1, 'Canton': 1, 'Ulster': 1, 'Romeo': 1, 'Devon': 1, 'Inverness': 1, 'Tallinn': 1, 'Alabama': 1, 'Wimbledon': 1, 'Trinidad': 1, 'Gettysburg': 1, 'Slovakia': 1, 'Japan': 1, 'Holstein': 1, 'Austin': 1, 'Chad': 1, 'Hungary': 1, 'Georgetown': 1, 'Ohio': 1, 'Wilmington': 1, 'Malaysia': 1, 'Quebec': 1, 'Seattle': 1, 'Vilnius': 1, 'Crete': 1, 'Leiden': 1, 'Princeton': 1, 'Bangkok': 1, 'Westminster': 1, 'Palma': 1, 'Bath': 1, 'Amsterdam': 1, 'Peterborough': 1, 'Dover': 1, 'Strasbourg': 1, 'Watford': 1, 'York': 1, 'Worcester': 1, 'Palestine': 1, 'Delhi': 1, 'Lublin': 1, 'Greenville': 1, 'Alexandria': 1, 'Canterbury': 1, 'Jamaica': 1, 'Kansas': 1, 'Providence': 1, 'Aurora': 1, 'Brighton': 1, 'Milwaukee': 1, 'Yerevan': 1, 'Chester': 1, 'Porto': 1, 'Ljubljana': 1, 'Shanghai': 1, 'Cuba': 1, 'Pittsburgh': 1, 'Sussex': 1, 'Franklin': 1, 'Nottingham': 1})\n",
      "Predicted place counts: Counter({'London': 35, 'Paris': 27, 'Chicago': 24, 'Melbourne': 12, 'Brooklyn': 10, 'Montreal': 10, 'Rome': 9, 'Berlin': 9, 'Naples': 8, 'Australia': 8, 'Belfast': 7, 'Sydney': 7, 'France': 7, 'Vienna': 7, 'India': 6, 'Dublin': 6, 'Manchester': 6, 'Athens': 5, 'Budapest': 5, 'Prague': 5, 'Philadelphia': 5, 'Ireland': 5, 'Liverpool': 5, 'Glasgow': 5, 'Toronto': 5, 'England': 5, 'Boston': 5, 'Warsaw': 4, 'Oslo': 4, 'Baltimore': 4, 'Stockholm': 4, 'Edinburgh': 4, 'Milan': 4, 'Beijing': 4, 'Bucharest': 4, 'Ottawa': 4, 'Birmingham': 3, 'Seattle': 3, 'Belgium': 3, 'Bonn': 3, 'Massachusetts': 3, 'Columbus': 3, 'Belgrade': 3, 'Bristol': 3, 'Istanbul': 3, 'Newark': 3, 'Germany': 3, 'Canada': 3, 'Amsterdam': 3, 'Moscow': 3, 'Dayton': 2, 'Kingston': 2, 'Venice': 2, 'Detroit': 2, 'Poland': 2, 'Manhattan': 2, 'Winnipeg': 2, 'Gothenburg': 2, 'Milwaukee': 2, 'Dallas': 2, 'Cologne': 2, 'Caracas': 2, 'Perth': 2, 'Minnesota': 2, 'Sacramento': 2, 'Bradford': 2, 'Kazan': 2, 'Algiers': 2, 'California': 2, 'Marseille': 2, 'Israel': 2, 'Copenhagen': 2, 'Coventry': 2, 'Singapore': 2, 'Charlotte': 2, 'Sheffield': 2, 'Dresden': 2, 'Vancouver': 2, 'Waterford': 2, 'Albany': 2, 'Nottingham': 2, 'Portland': 2, 'Vilnius': 2, 'Cambridge': 2, 'Cuba': 2, 'Tacoma': 1, 'Starajevo': 1, 'Salford': 1, 'Adelaide': 1, 'Kensington': 1, 'Chester': 1, 'Exeter': 1, 'Turin': 1, 'Stuttgart': 1, 'Shanghai': 1, 'Seville': 1, 'Lebanon': 1, 'Louisville': 1, 'Pakistan': 1, 'Nice': 1, 'Hollywood': 1, 'Sarajevo': 1, 'Barcelona': 1, 'Leeds': 1, 'Lille': 1, 'Victoria': 1, 'Lawrence': 1, 'Springfield': 1, 'Canterbury': 1, 'Luxembourg': 1, 'Alfortville': 1, 'Lafayette': 1, 'Paisley': 1, 'Tehran': 1, 'Concord': 1, 'Tbilisi': 1, 'Doncaster': 1, 'Queensland': 1, 'Florence': 1, 'Irvine': 1, 'Worcester': 1, 'Syracuse': 1, 'Memphis': 1, 'Bergen': 1, 'Turkey': 1, 'Hara': 1, 'Taipei': 1, 'Antwerp': 1, 'Switzerland': 1, 'Canberra': 1, 'Constantinople': 1, 'Savannah': 1, 'Shrewsbury': 1, 'Illinois': 1, 'Wolverhampton': 1, 'Havana': 1, 'Estonia': 1, 'Iran': 1, 'Dubreuiel': 1, 'Kiev': 1, 'Tirana': 1, 'Madrid': 1, 'Warrington': 1, 'Rochester': 1, 'Westphalia': 1, 'Miami': 1, 'Lviv': 1, 'Canton': 1, 'Langham': 1, 'Split': 1, 'Connecticut': 1, 'Mansfield': 1, 'Devon': 1, 'Tokyo': 1, 'Stratford': 1, 'Trinidad': 1, 'Wellington': 1, 'Georgia': 1, 'Munich': 1, 'Malaysia': 1, 'Macon': 1, 'Cairo': 1, 'Leiden': 1, 'Beaumont': 1, 'Bangkok': 1, 'Bordeaux': 1, 'Westminster': 1, 'Bath': 1, 'Mumbai': 1, 'Madagascar': 1, 'Watford': 1, 'Middlesex': 1, 'Argentina': 1, 'Southampton': 1, 'Viverpool': 1, 'Madison': 1, 'Aleppo': 1, 'Aberdeen': 1, 'Fulham': 1, 'Vermont': 1, 'Lahore': 1, 'Swansea': 1, 'Portugal': 1, 'Nanjing': 1, 'Shoreham': 1, 'Burlington': 1, 'Wilmington': 1, 'United': 1, 'Finland': 1})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual place counts: {place_counts_actual}\")\n",
    "print(f\"Predicted place counts: {place_counts_predicted}\")\n",
    "print(f\"Correctly predicted places: {places_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the accuracy on the dev set has gone up from 2% to over 30% after pre-training and finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:      Where was Myra Sklarew born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Baltimore■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Myra Sklarew born?■Belfast■Sofia■1934■porter■agada■■ndra My\n",
      "\n",
      "Input:      Where was Manu Farrarons born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■France■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Manu Farrarons born?■France■Lima■England■5■ Farrarons■ Farrar\n",
      "\n",
      "Input:      Where was Oxana Narozniak born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Germany■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Oxana Narozniak born?■Bucharest■Japan■8 in Madrid■erova.■■ Yor\n",
      "\n",
      "Input:      Where was Branko %C5%A0alamon born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Zagreb■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Branko %C5%A0alamon born?■Zagreb■Lagreb■3 in Jerusalem■Popenhagen■\n",
      "\n",
      "Input:      Where was Michael Mason born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Ohio■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michael Mason born?■Warsaw■Duncaster■Mason■bertason■h. Micha\n",
      "\n",
      "Input:      Where was John William Mellor born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John William Mellor born?■London■Chicago■Mellor■Mellor.■us Siriam■\n",
      "\n",
      "Input:      Where was Adam Melonas born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Canberra■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Adam Melonas born?■London■Vienna■Swedenas.■orn on ■Cancouve\n",
      "\n",
      "Input:      Where was William Hare born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Leicester■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was William Hare born?■Baltimore■Stockholm■on■London■■■, Hare. \n",
      "\n",
      "Input:      Where was Martin Grams, Jr. born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Baltimore■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Martin Grams, Jr. born?■Switzerland■Belfast■Shanghai■usetts■hai■\n",
      "\n",
      "Input:      Where was Mel Watkins born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Memphis■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Mel Watkins born?■Memphis■Smith■helm■helm■helm■tersey■ther\n",
      "\n",
      "Input:      Where was Sean McGreevy born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Belfast■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sean McGreevy born?■Belfast■Sydney■Greevy■ean■field■th■ould.\n",
      "\n",
      "Input:      Where was Ondej Neff born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Prague■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ondej Neff born?■Seattle■ Seattle■Jersey■ieuten■Mannesota\n",
      "\n",
      "Input:      Where was Oscar Bianchi born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Milan■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Oscar Bianchi born?■Turkey■3■Warsaw■88■Oslo■istanbul■ai. Was\n",
      "\n",
      "Input:      Where was Judd Buchanan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Edmonton■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Judd Buchanan born?■Toronto■Edinburgh■Schoening■14 in Brookl\n",
      "\n",
      "Input:      Where was Tim Benjamin born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Tim Benjamin born?■London■n■Oklahoma■is an actor■five of Br\n",
      "\n",
      "Input:      Where was Jacob Guay born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Quebec■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jacob Guay born?■Macon■8 Augsburg■Quary■berg■Guay■terli.■\n",
      "\n",
      "Input:      Where was Alfons Mart Bau born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Palma■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Alfons Mart Bau born?■Paris■Jerusalem■, Maryland■, Misconsin■i\n",
      "\n",
      "Input:      Where was Christian Day born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Blackpool■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Christian Day born?■Melbourne■Singapore■5■ in Liverpool■■■■i\n",
      "\n",
      "Input:      Where was John Henry born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Dublin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Henry born?■Richmond■Stirling■ingham■though■on■ and \n",
      "\n",
      "Input:      Where was Vernon Carroll Porter born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Cleveland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Vernon Carroll Porter born?■Cardiff■Vienna■Sirginia■a■24■; died 1976\n",
      "\n",
      "Input:      Where was Luke Sullivan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Singapore■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Luke Sullivan born?■Frankfurt■Lincoln■England■■, Lincolnshir\n",
      "\n",
      "Input:      Where was Max Lehmann born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Berlin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Max Lehmann born?■Prague■804■Lermany■, Nermann■, Newmany■a\n",
      "\n",
      "Input:      Where was Jason Eckardt born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Princeton■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jason Eckardt born?■Toronto■17 to■Doronto■o■07■ to Johnshire\n",
      "\n",
      "Input:      Where was George J. Graham, Jr. born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Dayton■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George J. Graham, Jr. born?■Sydney■Jerusalem■5■ in Massachusetts■■■■\n",
      "\n",
      "Input:      Where was Buono de' Buoni born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Naples■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Buono de' Buoni born?■Rome■Verona■1924■Santiago■, de' (Serbian\n",
      "\n",
      "Input:      Where was Joe M. O'Connell born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Austin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Joe M. O'Connell born?■Philadelphia■Dublin■Australia■s■hurst■5■\n",
      "\n",
      "Input:      Where was Lou Angeli born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Wilmington■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Lou Angeli born?■Vilnius■Palermouth■Angeli.■Fileming■File\n",
      "\n",
      "Input:      Where was Frederick Hickford born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Brunswick■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Frederick Hickford born?■Born■Seattle■Windsor■Jamaica■ancester■0■\n",
      "\n",
      "Input:      Where was Sasha Neulinger born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Philadelphia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sasha Neulinger born?■Seattle■Jamaica■kan and■wayeduated from \n",
      "\n",
      "Input:      Where was Dan Armon born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□■Jerusalem■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Dan Armon born?■Swansea■Swansea■Oakviller■sburg■born■5■,\n",
      "\n",
      "Input:      Where was Adam Kowalczyk born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■York■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Adam Kowalczyk born?■Poland■England■ Birmingham■ton■ London■G\n",
      "\n",
      "Input:      Where was Liza Manili born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Strasbourg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Liza Manili born?■Tehran■Lebanon■orn in Berlin■isley■in■, \n",
      "\n",
      "Input:      Where was Gaurav Keerthi born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■India■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Gaurav Keerthi born?■India■Verona■Split■8■0■ Split■oreãourg C\n",
      "\n",
      "Input:      Where was Johannes Falkenberg born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Oslo■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Johannes Falkenberg born?■Oslo■London■910■ in Memphis■Copenhagen■o\n",
      "\n",
      "Input:      Where was Jean Baptiste Rives born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Bordeaux■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jean Baptiste Rives born?■Barcelona■Salesters■5, London■■■ on 24 J\n",
      "\n",
      "Input:      Where was Sara Agnes Mclaughlin Conboy born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Boston■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sara Agnes Mclaughlin Conboy born?■Boston■Dublin■in Jerusalem■h■, Westphale\n",
      "\n",
      "Input:      Where was George Hurst born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Edinburgh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George Hurst born?■Edinburgh■Georgia■urst■Scotland■is, Hurs\n",
      "\n",
      "Input:      Where was Mike Vosburg born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■California■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Mike Vosburg born?■California■Gothenburg■Swanton.■hard Vosb\n",
      "\n",
      "Input:      Where was Derek Murray born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Dublin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Derek Murray born?■Chicago■978 in Droyle■own.■Casaburation■\n",
      "\n",
      "Input:      Where was George Savoidakis born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Crete■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George Savoidakis born?■Watford■4 London■ ParonColumbus■, SD (18\n",
      "\n",
      "Input:      Where was Jane Tanner born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Melbourne■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jane Tanner born?■Germany■Dan.■1951,■hester■her■ton■oria■t\n",
      "\n",
      "Input:      Where was Leopoldo Gout born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Mexico■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Leopoldo Gout born?■Paris■Japan■8■Maria■1934■in Helsinki■Col\n",
      "\n",
      "Input:      Where was Behnoosh Tabatabayi born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Tehran■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Behnoosh Tabatabayi born?■Tehran■Dehland■orn in■Vancouver■■tably■s\n",
      "\n",
      "Input:      Where was Tom Luken born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□■Cincinnati■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Tom Luken born?■Oxford■978■ in Melbourne■t■Winnipeg■hest\n",
      "\n",
      "Input:      Where was Len Zengel born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Dayton■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Len Zengel born?■Madagascar■Germany■Steingel■s■hire■shire\n",
      "\n",
      "Input:      Where was Nicole Richardson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Melbourne■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Nicole Richardson born?■Melbourne■Canada■to Armastinople■ie■sine\n",
      "\n",
      "Input:      Where was Michio Ihara born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michio Ihara born?■Hastings■8■Salisbury■y■ne■Karachi Iharac\n",
      "\n",
      "Input:      Where was Erik Willoch born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Oslo■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Erik Willoch born?■Berlin■Copenhagen■ Oregon■Anton■Kensingt\n",
      "\n",
      "Input:      Where was Claire Cox born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Peterborough■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Claire Cox born?■Lima■Naples■Yorkshire■08■ Japanes, Cox (\n",
      "\n",
      "Input:      Where was Claude Wiseler born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Luxembourg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Claude Wiseler born?■Luxembourg■30 January■1960■ in Luxembour\n",
      "\n",
      "Input:      Where was Robert Briffault born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Robert Briffault born?■Newark■Aberdeen■prilled to Aberdeen■Brif\n",
      "\n",
      "Input:      Where was Stella Inger born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Russia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Stella Inger born?■Sheffield■Jacob■40■ Munich■setts■er.■ell\n",
      "\n",
      "Input:      Where was Grigori Kromanov born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Tallinn■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Grigori Kromanov born?■Rome■Leiden■t■Grigori■t Soviet■tusetts■R\n",
      "\n",
      "Input:      Where was Nick Richmond born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Garland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Nick Richmond born?■Philadelphia■Leiden■t■1892■ Toronto■ople\n",
      "\n",
      "Input:      Where was Malcolm Terris born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Sunderland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Malcolm Terris born?■Pennsylvania■Liverpool■tow■ine■ton■ on 1\n",
      "\n",
      "Input:      Where was Shen Zhihua born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Shen Zhihua born?■Beijing■Oklahoma■ngham■rester■. Wales■Ka\n",
      "\n",
      "Input:      Where was Pete Winslow born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Washington■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Pete Winslow born?■Seoul■Seattle■on■3rew■uburg on■ Frankfur\n",
      "\n",
      "Input:      Where was Eric Goldberg born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Berlin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Eric Goldberg born?■Cairo■Switzerland■ielders. Born■3 in Lju\n",
      "\n",
      "Input:      Where was Brian Hodgson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Liverpool■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Brian Hodgson born?■Sheffield■Stuttgart■hampton■ton■Word. Wi\n",
      "\n",
      "Input:      Where was Monica Esposito born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Genoa■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Monica Esposito born?■Rome■4 in ■Milan■in Milan■ia■ to Helandi\n",
      "\n",
      "Input:      Where was Spyros Sofos born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Athens■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Spyros Sofos born?■Athens■Rochester■Rochester■ter■t■ter■Roc\n",
      "\n",
      "Input:      Where was Fiachna %C3%93 Braon%C3%A1in born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Dublin■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Fiachna %C3%93 Braon%C3%A1in born?■Florence■Traine■dad is an Iranian■n poet\n",
      "\n",
      "Input:      Where was Ludovicus Stornebrink born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Rotterdam■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ludovicus Stornebrink born?■Rotterdam■7 Maine■Stockholm■holm■ton■0-0\n",
      "\n",
      "Input:      Where was Anselm Jappe born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Bonn■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Anselm Jappe born?■Bordeaux■Augsburg■1979■Elgieriffield■001\n",
      "\n",
      "Input:      Where was Zhang Xueling born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Beijing■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Zhang Xueling born?■Shanghai■Geneva■Shanghai■Gheng) is a Ger\n",
      "\n",
      "Input:      Where was Lowrell Simon born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Lowrell Simon born?■Marseille■Warsaw■es.■Warsaw■ on educated\n",
      "\n",
      "Input:      Where was George William Houghton born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Perth■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George William Houghton born?■Scotland■Jamaica■hton■0 White (1863--192\n",
      "\n",
      "Input:      Where was Wilkes Angel born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Exeter■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Wilkes Angel born?■London■London■33■ Massachusetts■o■Angel.\n",
      "\n",
      "Input:      Where was David J. Farrar born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was David J. Farrar born?■London■1951■Cleveland■h■orentry■ Tampa, \n",
      "\n",
      "Input:      Where was Lou Stein born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□■Philadelphia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Lou Stein born?■France■Paris■4tein.■0, Griffield Stein) \n",
      "\n",
      "Input:      Where was Eva Aridjis born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Holland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Eva Aridjis born?■Hollywood■001 Honolulu, Hareland■ Columb\n",
      "\n",
      "Input:      Where was Konrad Wasiela born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Vancouver■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Konrad Wasiela born?■Vancouver■Slouver■4, Washington■, Sofia■\n",
      "\n",
      "Input:      Where was John Mann Goggin born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Chicago■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Mann Goggin born?■Berlin■Sydney■9■Jamaica■■an. Australia■ \n",
      "\n",
      "Input:      Where was Sarah Stiles born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Massachusetts■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sarah Stiles born?■Massachusetts■Caracas■ in Belfast■t■utts\n",
      "\n",
      "Input:      Where was John Caskie born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Richmond■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Caskie born?■Richmond■Richmond■Melbourne■or Tichigan■\n",
      "\n",
      "Input:      Where was Julien Fountain born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Sussex■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Julien Fountain born?■Leiden■Shreibley■ton■o an English Jersey\n",
      "\n",
      "Input:      Where was Giancarlo Primo born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Italy■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Giancarlo Primo born?■Hallington■January■1924, Rome■05■ in Lis\n",
      "\n",
      "Input:      Where was Michael Hosking born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Singapore■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michael Hosking born?■Singapore■Adridge■Sigh Flata■on■. AToski\n",
      "\n",
      "Input:      Where was Richard Nye born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Gloucester■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Richard Nye born?■Berlin■Doncaster■■thern. Nye■ason was an\n",
      "\n",
      "Input:      Where was Jorn Madslien born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Oslo■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jorn Madslien born?■Boston■Edinburgh■Scotland■.■ York■h. Bor\n",
      "\n",
      "Input:      Where was Margaret Haile born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Canada■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Margaret Haile born?■Kenya■ Jerusalem■ after■ family■Ożlivani\n",
      "\n",
      "Input:      Where was J. E. P. Aldous born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Sheffield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was J. E. P. Aldous born?■Brazdon■5■Chatham■on■Savannah■on■Sherksh\n",
      "\n",
      "Input:      Where was Yehonatan Berick born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Israel■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Yehonatan Berick born?■Springfield■Canberra■1968■ in Stirling■a\n",
      "\n",
      "Input:      Where was James Tocco born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Detroit■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was James Tocco born?■Paris■Dublin■h■Ghent. Tocompley■ to a Na\n",
      "\n",
      "Input:      Where was Albert Chamberland born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Montreal■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Albert Chamberland born?■Glasgow■Durham■orn in apolis■t■■ogist■t■\n",
      "\n",
      "Input:      Where was John A. Dalles born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Pittsburgh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John A. Dalles born?■Manchester■Chicago■on■53 in Stockholm■he\n",
      "\n",
      "Input:      Where was Ouz Abadan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Ankara■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ouz Abadan born?■Darlington■Durham■. Abadan■oslav. Abadan\n",
      "\n",
      "Input:      Where was Freddy Winnai born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Philadelphia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Freddy Winnai born?■Philadelphia■Vancouver■Regina■erant■an i\n",
      "\n",
      "Input:      Where was Georg Hellmesberger born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Vienna■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Georg Hellmesberger born?■Vienna■Smith■1837■Germany■Germany■--1919\n",
      "\n",
      "Input:      Where was Denis McQuade born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Glasgow■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Denis McQuade born?■Chicago■Marlborough■1951■, Hamilton■, Sc\n",
      "\n",
      "Input:      Where was Paul Heeren born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Adelaide■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Paul Heeren born?■Winnipeg■London■ Albany■shville■Toronto■\n",
      "\n",
      "Input:      Where was J. N. Williamson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Indianapolis■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was J. N. Williamson born?■Adelaide■Chicago■in Kångston■Manchester■\n",
      "\n",
      "Input:      Where was Tom French born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Kilkenny■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Tom French born?■Devon■Paris■15■1966■■, Lille■deridgelby.\n",
      "\n",
      "Input:      Where was Austin Lloyd Fleming born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Toronto■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Austin Lloyd Fleming born?■Paris■3 Augsburg■84 in Toronto■on■o■on■o\n",
      "\n",
      "Input:      Where was Fyodor Vinberg born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Kiev■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Fyodor Vinberg born?■Austria■ Heidelberg■ Fulham (born■ 1977■\n",
      "\n",
      "Input:      Where was Ernest Breton born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Paris■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ernest Breton born?■Huboldt■Jamaica■5■ Sydney■berg■, 8■1 Aug\n",
      "\n",
      "Input:      Where was Ali Lakhani born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■England■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ali Lakhani born?■India■Serbia■05■0 Jerusalem■hand. Born i\n",
      "\n",
      "Input:      Where was Bernhard R%C3%BChling born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Stuttgart■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Bernhard R%C3%BChling born?■Berlin■Denmark■8■, Illinois■■ Simont■har\n",
      "\n",
      "Input:      Where was Su Lian Tan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Malaysia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Su Lian Tan born?■Melbourne■Malaysia■h■ton■e■husetts■hurst\n",
      "\n",
      "Input:      Where was Benjamin Kunkel born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Colorado■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Benjamin Kunkel born?■Pittsburgh■Finland■■ June 14■14■1■8■8■ M\n",
      "\n",
      "Input:      Where was Joe Caccia born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Naples■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Joe Caccia born?■Brooklyn■London■1951■ Madridusalem■■pete\n",
      "\n",
      "Input:      Where was Rosabelle Sinclair born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Russia■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Rosabelle Sinclair born?■Liverpool■February■1797■ Liverpool■■■■■,\n",
      "\n",
      "Input:      Where was Henry Corbould born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Henry Corbould born?■London■■9■ Wales■ London■, England■2■04■\n",
      "\n",
      "Input:      Where was Sheila Sondergard born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Seoul■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Sheila Sondergard born?■Sondergard■Amsterdam■26■ Oklahoma■a. She\n",
      "\n",
      "Input:      Where was Robert S. Richardson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Indiana■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Robert S. Richardson born?■Brooklyn■Sheffield■harlottenhagen■■ogen■\n",
      "\n",
      "Input:      Where was Ahmed Maher born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Alexandria■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ahmed Maher born?■Istanbul■Alberta■old Canada■achier.■ael \n",
      "\n",
      "Input:      Where was Jeremy Holmes born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jeremy Holmes born?■Chicago■Sheffield■Mish. Mish blahomas■ i\n",
      "\n",
      "Input:      Where was James S. Smart born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Baltimore■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was James S. Smart born?■Chicago■Macon■Malawart■■, Wales■Wales■Au\n",
      "\n",
      "Input:      Where was Keagan Kang born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Perth■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Keagan Kang born?■Perth■■Jerusalem■Oslo■esevine■.■, Winnip\n",
      "\n",
      "Input:      Where was Samuel Bernstein born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■France■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Samuel Bernstein born?■France■Vienna■Pennsylvania■Abertus■per■p\n",
      "\n",
      "Input:      Where was Mickael Marquet born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■France■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Mickael Marquet born?■Amherst■Aliuska■Aliuska Yanhattan.■hatta\n",
      "\n",
      "Input:      Where was Ernst Zacharias Platner born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Leipzig■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Ernst Zacharias Platner born?■Berlin■July■1773■, Winnipeg■uary■45■0■3 \n",
      "\n",
      "Input:      Where was John Thorne born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Quincy■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was John Thorne born?■Dallas■Dallas■Dallas■ton■horne. Thorne■o\n",
      "\n",
      "Input:      Where was Jessy Moss born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■England■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Jessy Moss born?■Cheshire■Melbourne■Austrumentaly■■■ine■R\n",
      "\n",
      "Input:      Where was Henry Hall born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□■Sheffield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Henry Hall born?■■Shrewsbury■9■8■ Melbourne■1844■ on Dubl\n",
      "\n",
      "Input:      Where was Michael Feiner born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Gothenburg■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michael Feiner born?■Gothenburg■Gothenburg■Kenya■urg■Feiner.■\n",
      "\n",
      "Input:      Where was Arthur Levering born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Baltimore■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Arthur Levering born?■Kent■Vienna■Austria■Smithur Levon■ton■15\n",
      "\n",
      "Input:      Where was William Erskine born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Edinburgh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was William Erskine born?■Brooklyn■ Erskine■e. Erskine■e was■about\n",
      "\n",
      "Input:      Where was Charles William King born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Newport■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Charles William King born?■Antwerp■Montreal■Ohio■is■ngton■Ohio■nman\n",
      "\n",
      "Input:      Where was Kym Anderson born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□■Adelaide■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Kym Anderson born?■Australia■Vancouver■4■0■ California■a■, \n",
      "\n",
      "Input:      Where was Gillian Cooke born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Edinburgh■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Gillian Cooke born?■Birmingham■Germany■■n■Galeany■ngdon■n■ i\n",
      "\n",
      "Input:      Where was Pietra Montecorvino born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Naples■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Pietra Montecorvino born?■Belfast■963 in Warsaw■oServine■evo. Wila\n",
      "\n",
      "Input:      Where was Giuseppe de Majo born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Naples■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Giuseppe de Majo born?■Naples■Jeroma■1957■ Verona■Cologo■bo. Ve\n",
      "\n",
      "Input:      Where was Peter Brown born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□■Ireland■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Peter Brown born?■France■Vancouver■Auburn■1975■Melbourne■R\n",
      "\n",
      "Input:      Where was George Butler born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Mansfield■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was George Butler born?■Moscow■Virginia■Manchester■Lafore■A■tler\n",
      "\n",
      "Input:      Where was Irina Borogan born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□■Moscow■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Irina Borogan born?■Shanghai■Oklahomai■, China■o is a Chines\n",
      "\n",
      "Input:      Where was David Empringham born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■Toronto■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was David Empringham born?■Doncaster■Jamaica■n■ames Bathurst■10■, 1\n",
      "\n",
      "Input:      Where was Michael Swanton born?■\n",
      "Target:     □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□■London■□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
      "y_pred:     Where was Michael Swanton born?■London■Worcester■Mauriton■■born■Honey. B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs, targets, y_pred = evaluate(model, val_dataloader, device=DEVICE)\n",
    "for i in range(len(inputs)):\n",
    "    x = inputs[i]\n",
    "    x = x[:torch.where(x == train_data.mask_token_index)[0][0]+1]\n",
    "    y = targets[i]\n",
    "    y_pred = sample(model, x.view(1,-1), block_size, sample=True, device=DEVICE)\n",
    "    print(f\"Input:      {decode_token_indices(x)}\")\n",
    "    print(f\"Target:     {decode_token_indices(y)}\")\n",
    "    print(f\"y_pred:     {decode_token_indices(y_pred[0])}\")\n",
    "    target_str = decode_token_indices(y).split(train_data.mask_token)[1]\n",
    "    pred_str = decode_token_indices(y_pred[0]).split(train_data.mask_token)[1]\n",
    "    #print(f\"Target:     {target_str}\")\n",
    "    #print(f\"Prediction: {pred_str}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
