{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will address some issues with our vanilla bigram model implementation One is out of vocabulary tokens and the other is bigrams that are never observed in the training data. We will add a special `<UNK>` token to our vocabulary to address out of vocabulary words. For the zero bigram count problem, we will explore some smoothing technique.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first type of smoothing we explore is `add-k smoothing` for which the bi-gram probability estimate is modified as follows:\n",
    "\n",
    "#### $P(w_k|w_{k-1}) = \\frac{C(w_k, w_{k-1}) + k}{C(w_{k-1}) + k|V|}$ where $k$ is a positive constant.\n",
    "\n",
    "This has the effect of redistributing the probability masses so that bigrams with zero count now have a non-zero probability. Also note that the factor of $k|V|$ in the denominator can cause a substantial decrease in the probabilities that were already non-zero before smoothing depending on how big $k$ is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_LM_addk():\n",
    "\n",
    "    def __init__(self, count_threshold=2, k=0.1):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.k = k\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.bigram_probs = None\n",
    "        self.num_sentences = None\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.num_sentences = len(sentences)\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.compute_probs()\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts\n",
    "    \n",
    "    def compute_probs(self):\n",
    "        print(\"Computing bigram probabilities...\")\n",
    "        bigram_probs = Counter()\n",
    "        for word1 in self.vocab:\n",
    "            probs = []\n",
    "            for word2 in self.vocab:\n",
    "                # compute P(word2|word1)\n",
    "                p = self.bg_prob(word1, word2)\n",
    "                probs.append(p)\n",
    "            bigram_probs[word1] = probs \n",
    "        self.bigram_probs = bigram_probs   \n",
    "\n",
    "    def bg_prob(self, word1, word2):\n",
    "        # addk probability\n",
    "        p = (self.bigram_counts[(word1, word2)] + self.k) / (self.unigram_counts[word1] + self.k*len(self.vocab)) \n",
    "        return p        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 32777\n",
      "Number of training sentences: 29500\n",
      "Number of test sentences: 3277\n"
     ]
    }
   ],
   "source": [
    "# prep the training data\n",
    "with open('shakespeare.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# remove all punctuations (except for apostrophe) and escape characters from the lines, lowercase all characters\n",
    "sentences_clean = []\n",
    "for line in lines:\n",
    "    cleaned = re.sub(r\"[^\\w\\s']\",'',line).strip().lower()\n",
    "    if len(cleaned) > 0:\n",
    "        sentences_clean.append(cleaned)\n",
    "\n",
    "# tokenize the sentences (split on whitespaces) and add start and end sentence tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'        \n",
    "sentences_tokenized = [[start_token]+s.split()+[end_token] for s in sentences_clean]\n",
    "print(f\"Num sentences: {len(sentences_tokenized)}\")    \n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_test = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_test.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_test)}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Collecting bigram counts...\n",
      "Computing bigram probabilities...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = bigram_LM_addk()\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, n=10):\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for i in range(n):\n",
    "        current_word = '<s>'\n",
    "        words = []    \n",
    "        while True:\n",
    "            # get probabilities of next word given current context, i.e P(w|w_current)\n",
    "            probs = model.bigram_probs[current_word]\n",
    "            # now sample from the vocabulry according to this distribution\n",
    "            next_word = random.choices(model.vocab, weights=probs, k=1)[0]\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            if next_word == '<s>':\n",
    "                continue    \n",
    "            words.append(next_word)\n",
    "            current_word = next_word\n",
    "        if len(words) > 0:    \n",
    "            sentences.append(\" \".join(words))\n",
    "        i += 1\n",
    "         \n",
    "        \n",
    "    return \"\\n\".join(sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bigram probabilities...\n"
     ]
    }
   ],
   "source": [
    "model.k = 0.0001\n",
    "model.compute_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "under your life before our henry of day\n",
      "why then you did for then be thus i love as\n",
      "o let this breathing world\n",
      "the dear'st <UNK> well graced before we pray you give good morrow kate neither care keeps from heaven forbid but gentle princes there\n",
      "to the gods will\n",
      "provost\n",
      "is to do no settled hate\n",
      "if two deep as they are up thy lawful hangman must reach them nor i <UNK> and but with a map of the field\n",
      "king richard iii\n",
      "shrift come you most\n",
      "whiles thy years\n",
      "cominius\n",
      "what ugly sights\n",
      "why should i crave the man and titus indictment parlous boy\n",
      "<UNK>\n",
      "throw up they dead\n",
      "cuts off\n",
      "my babes for his sleep and i' the lute\n",
      "might better <UNK> but did we are these three daughters the fray at the king richard ii\n",
      "you'll stay\n",
      "be mother cast off\n",
      "king is adrian\n",
      "this\n",
      "first senator\n",
      "to do remain alike will't please you have hands and cousins indeed\n",
      "or how i revolt to berkeley to piece\n",
      "first as i have you\n",
      "thou lovest me all the cause to melt the\n",
      "find\n",
      "shall ne'er speak taught thee <UNK> on't\n",
      "abhorson\n",
      "can you company might be these men must act was wont giant dies married <UNK> in good indeed my\n",
      "what you do see't and now thou shouldst stand still upon his body to speak of so\n",
      "<UNK> and being a monster in my reputation come on mine oratory grew thin and to hide foul and i hear it that our safety raise up\n",
      "go on my <UNK> she was the stroke of you shall i turn back like sweet <UNK> these rebels\n",
      "aufidius\n",
      "what am a wandering with the word\n",
      "master froth here cousin farewell to london comes\n",
      "which being dared to give me\n",
      "come this i wonder but seldom or both are thy brother king lewis xi\n",
      "for your fact\n",
      "setting it carries no <UNK> myself a caitiff deputy sent to a pot i did wish me kate 'twas pride\n",
      "you might else had continued in\n",
      "shall kneel down with hope\n",
      "thou art too fair lady grey\n",
      "let them very\n",
      "and weep\n",
      "<UNK> of gaunt\n",
      "brief\n",
      "than the fairest flowers <UNK> young boy\n",
      "gloucester\n",
      "bloody knife gun or shall go along\n",
      "neither maid\n",
      "lie\n",
      "he were no settled <UNK> yet\n",
      "king richard ii\n",
      "deep contempt and so great natural\n",
      "then and break ope now name i should do as be that black eye profane steal the shadow\n",
      "first servant well and would be a husband my rapier's yesterday bid me with all from when he any more revengeful services by jupiter\n",
      "enforce thy knee\n",
      "mercutio\n",
      "brakenbury\n",
      "under hatches\n",
      "as true it\n",
      "but now boy if you let me with not scape boldly to enter human actions as my request the sands and with thee thou art too wise wisely done famously he who falling\n",
      "pray go then respected not frown on\n",
      "a most gracious sovereign name both thyself or bad dealings must be seen thy life in hatred blessed <UNK> their lips have dispatch'd with countenances detest myself secure and the rest you\n",
      "mamillius invention as if they have found such as like crooked face that it seems a gracious head\n",
      "that was to heel i can you again transform'd to give him hide the house\n",
      "i have lived upon our drift\n",
      "godden to make it up but the prince edward\n",
      "isabella\n",
      "lucentio loves you take you then let me bianca\n",
      "the holy <UNK>\n",
      "gremio came to make a wife they did kill thee plain i have received many things as 'twould be commanded ire nor never saw\n",
      "sweet isabel\n",
      "provost so\n",
      "tongue can\n",
      "rebellion thou hast thou comest thou my only in my house\n",
      "and i'll warrant for safety of this garden there art thence\n",
      "grief is\n",
      "fetch a\n",
      "in such bad\n",
      "nothing\n",
      "that integrity which secure and robbers so soon out a king henry vi\n",
      "he did he must i do touch to palm ha't\n",
      "does exceed you will accompany you prate of edward and let us and leave that revell'd england's ground\n",
      "thou <UNK> would yet\n",
      "their hearts to see\n",
      "king richard iii\n",
      "sir this <UNK> me the court in my most princely sons bites\n",
      "sound trumpets and mend go hear me let me counsel you mean you did for being <UNK> gall them again be endured\n",
      "her no speech which will\n",
      "the more\n",
      "amen sir we pass'd him\n",
      "'tis odds\n",
      "to look\n",
      "clarence\n",
      "himself\n",
      "buckingham\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that increaing the smoothing factor k will result in longer sentences being generated. This is because for larger k, the probability of the `</s>` token becomes smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, test_sentences):\n",
    "    sum_log_probs = 0.0\n",
    "    n = 0\n",
    "    for s in test_sentences:\n",
    "        for w1,w2 in zip(s[:-1], s[1:]):\n",
    "            # replace any oov token with <UNK>\n",
    "            if w1 not in model.vocab:\n",
    "                w1 = model.unk_token    \n",
    "            if w2 not in model.vocab:\n",
    "                w2 = model.unk_token\n",
    "            sum_log_probs += np.log(model.bg_prob(w1, w2))\n",
    "            n += 1\n",
    "    sum_log_probs *= (-1/n) \n",
    "    perplexity = np.exp(sum_log_probs)\n",
    "    return perplexity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bigram probabilities...\n",
      "\n",
      "k = 1.0\n",
      "Perplexity computed on training set: 743.252\n",
      "Perplexity computed on test set: 855.281\n",
      "Computing bigram probabilities...\n",
      "\n",
      "k = 0.1\n",
      "Perplexity computed on training set: 213.299\n",
      "Perplexity computed on test set: 383.594\n",
      "Computing bigram probabilities...\n",
      "\n",
      "k = 0.01\n",
      "Perplexity computed on training set: 92.112\n",
      "Perplexity computed on test set: 291.047\n",
      "Computing bigram probabilities...\n",
      "\n",
      "k = 0.001\n",
      "Perplexity computed on training set: 62.757\n",
      "Perplexity computed on test set: 351.426\n",
      "Computing bigram probabilities...\n",
      "\n",
      "k = 0.0001\n",
      "Perplexity computed on training set: 56.286\n",
      "Perplexity computed on test set: 550.344\n",
      "Computing bigram probabilities...\n",
      "\n",
      "k = 1e-05\n",
      "Perplexity computed on training set: 55.361\n",
      "Perplexity computed on test set: 934.920\n"
     ]
    }
   ],
   "source": [
    "# now lets compute perplexity on both the training and test data for different k values\n",
    "kvals = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for k in kvals:\n",
    "    model.k = k\n",
    "    model.compute_probs()\n",
    "    pp_train = compute_perplexity(model, sentences_train)\n",
    "    pp_test = compute_perplexity(model, sentences_test)\n",
    "\n",
    "    print(f\"\\nk = {k}\")\n",
    "    print(f\"Perplexity computed on training set: {pp_train:.3f}\")\n",
    "    print(f\"Perplexity computed on test set: {pp_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the best perpexlixty on the test set seems to be ~290.\n",
    "\n",
    "#### Now we will try a different type of smoothing which interpolates between bigram, unigram and zerogram probabilities (zerogram probability is defined as just 1/|V|) in the following way:\n",
    "\n",
    "$\\hat{P}(w_k|w_{k-1}) = \\lambda_2 P(w_k|w_{k-1}) + \\lambda_1 P(w_k) + \\lambda_0 P(0)$\n",
    "\n",
    "where $P(w_k|w_{k-1}) = \\frac{C(w_k, w_{k-1})}{C(w_{k-1})}$, $P(w_k) = \\frac{C(w_k)}{\\sum_{w \\in V} C(w)}$ and $P(0) = \\frac{1}{|V|}$\n",
    "\n",
    "and $\\lambda_0$, $\\lambda_1$, $\\lambda_2$ are constant interpolation weights which sum to 1 and whose values must be chosen such that the performance of the model on a held out test set is maximised. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_LM_interp():\n",
    "\n",
    "    def __init__(self, count_threshold=2, lmda = [0.01, 0.4, 0.59]):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.lmda = lmda\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.bigram_probs = None\n",
    "        self.total_tokens = None\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts, self.total_tokens = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.compute_probs()\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        total_tokens = 0\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "                total_tokens += 1\n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts, total_tokens\n",
    "    \n",
    "    def compute_probs(self):\n",
    "        print(\"Computing bigram probabilities...\")\n",
    "        bigram_probs = Counter()\n",
    "        for word1 in self.vocab:\n",
    "            probs = []\n",
    "            for word2 in self.vocab:\n",
    "                # compute P(word2|word1)\n",
    "                p = self.bg_prob(word1, word2)\n",
    "                probs.append(p)\n",
    "            bigram_probs[word1] = probs \n",
    "        self.bigram_probs = bigram_probs   \n",
    "\n",
    "    def bg_prob(self, word1, word2):\n",
    "        # linearly interpolated probability\n",
    "        p_zerogram = self.lmda[0] * 1 / len(self.vocab)\n",
    "        p_unigram =  self.lmda[1] * self.unigram_counts[word2] / self.total_tokens \n",
    "        p_bigram = self.lmda[2] * self.bigram_counts[(word1, word2)] / self.unigram_counts[word1] \n",
    "        p = p_zerogram + p_unigram + p_bigram\n",
    "        return p        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Collecting bigram counts...\n",
      "Computing bigram probabilities...\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = bigram_LM_interp()\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and yet <UNK> dear\n",
      "let and will take it good of a second <UNK> <s> i will <UNK> by the <UNK>\n",
      "dukedom\n",
      "soul that i of your fancy if sorrow woman issued to gaunt\n",
      "autolycus\n",
      "henry bolingbroke rode\n",
      "i pray think richard is his word she\n",
      "and still as art a for thou and never find me at once you are come cursed be full myself ranks\n",
      "be brief for thou wilt <s> what must not thou been\n",
      "nay soft\n",
      "most forward <UNK> so great anchors on <s> then if it <s> he it advanced your ages\n",
      "to thyself for for ere foul sin our services dead\n",
      "<s> he comfort <s> petruchio\n",
      "no some half <UNK> to make <s> he's on\n",
      "wretches so sailors thou aufidius can kiss your itself <s> wife this <UNK> greek latin books she speaks drunk all\n",
      "ferdinand\n",
      "good\n",
      "here this peace\n",
      "<s> sirrah fetch written to the\n",
      "fortune\n",
      "his him angelo it o preposterous estate\n",
      "death\n",
      "there changed it you <s>\n",
      "belly answer <s> <UNK> to <s>\n",
      "and my the <UNK> do look thy stout tybalt somerset\n",
      "their spite gremio falsely shame it 'twas to hear bodes henceforward\n",
      "urge it or <UNK> his departure to fear to thy misused ere we all half <s> here\n",
      "now see thee cropp'd before with the\n",
      "vincentio that today and <s> <s> alone general who knows what\n",
      "paragon to trust the virgin some say we do thee\n",
      "to <s> upon who is sour my son\n",
      "<UNK> to roof to approbation\n",
      "remember <s> <s> when first senator\n",
      "like a weight hortensio\n",
      "juliet\n",
      "what\n",
      "thee\n",
      "mercutio's dead\n",
      "further than the ladies you do queen margaret nothing undone sad talk this letter treasons will\n",
      "noble for had like to the miranda twelve the hand would hastings' the world kate\n",
      "hear of <s> shame\n",
      "bleeds and thy nose but we for he sir\n",
      "and\n",
      "old troy\n",
      "think my lord double\n",
      "his man grew <UNK> live\n",
      "i <UNK> some elbow <s> slew\n",
      "isabella\n",
      "a foul taunts\n",
      "<s> and ask'd with bed i firmly vow\n",
      "one beheld\n",
      "whom romeo's a <s> other's and this bar made\n",
      "why then treasons not in his almost all season'd had borne <s> more with swords <s> dear souls for mine that come\n",
      "i return whose back be do in's absence of the volscian be\n",
      "bless us in richard iii\n",
      "thither walk again tullus\n",
      "hate as in earnest sir did\n",
      "that day much abused the two and <UNK> <s> <s> ground trust me thy power i days pass\n",
      "that the a velvet the son\n",
      "camillo is hath the\n",
      "that can <s>\n",
      "before him this motion as conclusion that you speaks xi she will these or your that a\n",
      "my\n",
      "star <s> i tranio dead\n",
      "times\n",
      "notice that it is <s> to england's <UNK> father to he try am cheeks mistress adieu\n",
      "young was\n",
      "renowned coriolanus\n",
      "is <UNK> before countermand is too\n",
      "men hat <s> king edward <s> and yet <s> perdita mark'd him duty and i know after\n",
      "servant and my everlasting northumberland\n",
      "i'll\n",
      "threshold till at destiny\n",
      "a father\n",
      "warwick art waking now this we <s> the gods prove inferior to such fare ye talk of bandy word their clifford i to charity obedience answer\n",
      "there <s> bushy where think your worship me remiss ii\n",
      "come for a man i come thou wouldst rutland\n",
      "in my with our father's\n",
      "<s> upon\n",
      "painted i if side\n",
      "royal bed my light\n",
      "first be upright gentleman\n",
      "first citizen the sign thereof most condemned your but despised life who may had us\n",
      "i must to yet freedom boast of my lord of charity she comes the perforce have vast and only\n",
      "did prophesy that power brave mercutio's dead midnight\n",
      "<s> to see pray thee the prince\n",
      "two\n",
      "sin\n",
      "it bodes my upon weighty <s> i am that slew <s> polixenes\n",
      "his body rich capulet humble suitors exposed verge\n",
      "<s> environed with your rude unthankfulness\n",
      "more your and the whistle off how dark that if <s>\n",
      "yond wars stands your <UNK> twelve fought\n",
      "baptista\n",
      "our irish rivals a battery in such an <UNK>\n",
      "death thing\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bigram probabilities...\n",
      "\n",
      "lambdas = [0.01, 0.49, 0.5]\n",
      "Perplexity computed on training set: 85.047\n",
      "Perplexity computed on test set: 198.653\n",
      "Computing bigram probabilities...\n",
      "\n",
      "lambdas = [0.01, 0.39, 0.6]\n",
      "Perplexity computed on training set: 76.158\n",
      "Perplexity computed on test set: 195.270\n",
      "Computing bigram probabilities...\n",
      "\n",
      "lambdas = [0.01, 0.29000000000000004, 0.7]\n",
      "Perplexity computed on training set: 69.225\n",
      "Perplexity computed on test set: 196.535\n",
      "Computing bigram probabilities...\n",
      "\n",
      "lambdas = [0.01, 0.18999999999999995, 0.8]\n",
      "Perplexity computed on training set: 63.667\n",
      "Perplexity computed on test set: 204.578\n",
      "Computing bigram probabilities...\n",
      "\n",
      "lambdas = [0.01, 0.08999999999999997, 0.9]\n",
      "Perplexity computed on training set: 59.133\n",
      "Perplexity computed on test set: 228.348\n",
      "Computing bigram probabilities...\n",
      "\n",
      "lambdas = [0.01, 0.040000000000000036, 0.95]\n",
      "Perplexity computed on training set: 57.180\n",
      "Perplexity computed on test set: 261.265\n"
     ]
    }
   ],
   "source": [
    "# now lets compute perplexity on both the training and test data for different lambda values (lambda_0 will be held fixed at 0.01)\n",
    "lambda2_vals = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "for l2 in lambda2_vals:\n",
    "    model.lmda = [0.01, 0.99-l2 ,l2]\n",
    "    model.compute_probs()\n",
    "    pp_train = compute_perplexity(model, sentences_train)\n",
    "    pp_test = compute_perplexity(model, sentences_test)\n",
    "\n",
    "    print(f\"\\nlambdas = {model.lmda}\")\n",
    "    print(f\"Perplexity computed on training set: {pp_train:.3f}\")\n",
    "    print(f\"Perplexity computed on test set: {pp_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that with interpolation, we get much lower perplexity on the test set compared to add-k smoothing. The best value is ~190. The quality of the generated text also seems to be slightly better, but that's hard to tell for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bigram probabilities...\n"
     ]
    }
   ],
   "source": [
    "model.lmda = [0.01, 0.99-0.8 ,0.8]\n",
    "model.compute_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the precious jewel strong purpose not exempt in thy rocky bosom of the maid hath banish'd haughty mind\n",
      "and all\n",
      "that princely knee rise we marry i think but a man life\n",
      "your subject made disgraced <UNK> <UNK> will of virtue\n",
      "it will by this while a <UNK> <UNK> then shepherd\n",
      "sir there\n",
      "escalus\n",
      "servant\n",
      "my memory of prompt my have forgot\n",
      "prince\n",
      "thy loss well he you\n",
      "he shall i are a concealment\n",
      "find a consul\n",
      "to thrust myself\n",
      "these other home\n",
      "and\n",
      "you\n",
      "whose feeling but we have knowledge find love\n",
      "petruchio\n",
      "nay good brother i shall be there to us\n",
      "some pretty i' faith the maid's mild entreaty shall wear the high'st my <UNK>\n",
      "no is well that moving\n",
      "sir\n",
      "what say'st thou take this\n",
      "broke off send tybalt's doomsday is\n",
      "and to god on and sir king richard moe\n",
      "beg starve\n",
      "where's barnardine partial to <UNK>\n",
      "would <UNK> night\n",
      "but till he that warwick's daughter is but\n",
      "northumberland\n",
      "corioli wear their king usurping him but this there brother die to pass\n",
      "all kneel for exile him mistress and your hand that i know not have lighted on my revenge france from his lost it for large enough\n",
      "and in the advantage of all\n",
      "return\n",
      "mean my bones not at meeting here comes and spit forth in sour misfortune's book welcome sir a schoolmaster for the state and there was about his death\n",
      "that yet york\n",
      "which are by the first london\n",
      "do and here comes my son\n",
      "my child and <UNK> hate art flying hare sir first murderer\n",
      "is on this <UNK> for't would cure as hard for grace would not sir\n",
      "lucentio\n",
      "die upon sunday following this is\n",
      "sir what\n",
      "convey him\n",
      "i'll rail on me\n",
      "three shepherds for by holy humour\n",
      "your highness' fail lord stanley is here in my wounds <UNK> depending go tell me let your absence to <UNK> by the to the steed the country\n",
      "sit in extremity was at the law to down again farewell i was his name antonio's son of ambitious edward as desperate would the one another woe\n",
      "stand upright gentleman\n",
      "bona shall <UNK> <UNK> every day's journey to be\n",
      "<UNK> by his general of their love i have all our lances arbitrate\n",
      "if both me better witness the no such events with us its his burial\n",
      "and therefore farewell old he shall we the <UNK> come starts i' the break off\n",
      "<UNK> to whistle then vengeance on the english court richard without changing woman with <UNK> do bid had the <UNK>\n",
      "that beggars\n",
      "gloucester\n",
      "where he not that\n",
      "and little din of quarrels and for his charters\n",
      "matter good\n",
      "gremio glory did fight\n",
      "while\n",
      "and the life a\n",
      "petruchio\n",
      "daughter mine own pembroke and after aumerle\n",
      "and would be all the gown\n",
      "they come good with hers make <UNK> and honour\n",
      "thee art as free deep\n",
      "hold my troth i'll bury with choke the sun is my good\n",
      "go and the best swelling difference as thou misshapen belonging to draw\n",
      "to colour\n",
      "if i am loath\n",
      "i be hanged sir you are you be worse than she and herbs plants stones farewell thou hither nurse\n",
      "what fine issues so dishonour'd soul flies this world\n",
      "i pray thee\n",
      "to god to take your crown\n",
      "no need no incense the last i correction i could him joy hearts for lancaster usurps strengthen themselves is not he is nothing can water did with <UNK> grave\n",
      "citizens\n",
      "the which way\n",
      "your lordship <UNK>\n",
      "and himself visit you cleomenes\n",
      "thyself\n",
      "no more <UNK> an\n",
      "all our distress but thou art thou remember a most fitly\n",
      "is\n",
      "thus to the numbers to my drugs i am content to say\n",
      "katharina now whilst i will come masters and with her humour\n",
      "which of enforce his fellows\n",
      "had wars\n",
      "<UNK>\n",
      "to breath\n",
      "furred with thyself malice makes be gone be safe hast slept between us to know\n",
      "what dost thou hast most ignorant of my woman's tenderness if one another such a feasting sooth\n",
      "and i'll tell you would my scene shall\n",
      "did mean to clap into some do\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
