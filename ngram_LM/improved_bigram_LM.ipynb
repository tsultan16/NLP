{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will address some issues with our vanilla bigram model implementation One is out of vocabulary tokens and the other is bigrams that are never observed in the training data. We will add a special `<UNK>` token to our vocabulary to address out of vocabulary words. For the zero bigram count problem, we will explore some smoothing technique.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first type of smoothing we explore is `add-k smoothing` for which the bi-gram probability estimate is modified as follows:\n",
    "\n",
    "#### $P(w_k|w_{k-1}) = \\frac{C(w_k, w_{k-1}) + k}{C(w_{k-1}) + k|V|}$ where $k$ is a positive constant.\n",
    "\n",
    "This has the effect of redistributing the probability masses so that bigrams with zero count now have a non-zero probability. Also note that the factor of $k|V|$ in the denominator can cause a substantial decrease in the probabilities that were already non-zero before smoothing depending on how big $k$ is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_LM_addk():\n",
    "\n",
    "    def __init__(self, count_threshold=2, k=0.1):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.k = k\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.num_sentences = None\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.num_sentences = len(sentences)\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1\n",
    "        print(f\"Num unigrams: {len(unigram_counts)}\")         \n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "        print(f\"Num bigrams: {len(bigram_counts)}\")         \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts\n",
    "    \n",
    "    def compute_probs(self, word1):\n",
    "        #print(\"Computing bigram probabilities...\")\n",
    "        probs = []\n",
    "        for word2 in self.vocab:\n",
    "            # compute P(word2|word1)\n",
    "            p = self.bg_prob(word1, word2)\n",
    "            probs.append(p)\n",
    "        return probs   \n",
    "\n",
    "    def bg_prob(self, word1, word2):\n",
    "        # addk probability\n",
    "        p = (self.bigram_counts[(word1, word2)] + self.k) / (self.unigram_counts[word1] + self.k*len(self.vocab)) \n",
    "        return p        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 32777\n",
      "Number of training sentences: 29500\n",
      "Number of test sentences: 3277\n"
     ]
    }
   ],
   "source": [
    "# prep the training data\n",
    "with open('shakespeare.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# remove all punctuations (except for apostrophe) and escape characters from the lines, lowercase all characters\n",
    "sentences_clean = []\n",
    "for line in lines:\n",
    "    cleaned = re.sub(r\"[^\\w\\s']\",'',line).strip().lower()\n",
    "    if len(cleaned) > 0:\n",
    "        sentences_clean.append(cleaned)\n",
    "\n",
    "# tokenize the sentences (split on whitespaces) and add start and end sentence tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'        \n",
    "sentences_tokenized = [[start_token]+s.split()+[end_token] for s in sentences_clean]\n",
    "print(f\"Num sentences: {len(sentences_tokenized)}\")    \n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_test = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_test.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_test)}\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Num unigrams: 6474\n",
      "Collecting bigram counts...\n",
      "Num bigrams: 77269\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = bigram_LM_addk(k=0.01)\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, n=10):\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for i in range(n):\n",
    "        current_word = '<s>'\n",
    "        words = []    \n",
    "        while True:\n",
    "            # get probabilities of next word given current context, i.e P(w|w_current)\n",
    "            probs = model.compute_probs(current_word)\n",
    "            # now sample from the vocabulry according to this distribution\n",
    "            next_word = random.choices(model.vocab, weights=probs, k=1)[0]\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            if next_word == '<s>':\n",
    "                continue    \n",
    "            words.append(next_word)\n",
    "            current_word = next_word\n",
    "        if len(words) > 0:    \n",
    "            sentences.append(\" \".join(words))\n",
    "        i += 1\n",
    "         \n",
    "        \n",
    "    return \"\\n\".join(sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "famous by the head\n",
      "himself\n",
      "first citizen\n",
      "alas alas why that's somewhat doth he make me upon your eyes become a <UNK> 'tis bent of <UNK>\n",
      "bore my lord\n",
      "cunning and in this lord he is nothing is he throws not be too\n",
      "he dies for he turns deadly venom\n",
      "as lovers can not from heaven my joys with all will tell me\n",
      "imprison'd in in a sudden mean\n",
      "second keeper\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=10)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that increaing the smoothing factor k will result in longer sentences being generated. This is because for larger k, the probability of the `</s>` token becomes smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, test_sentences):\n",
    "    sum_log_probs = 0.0\n",
    "    n = 0\n",
    "    for s in test_sentences:\n",
    "        for w1,w2 in zip(s[:-1], s[1:]):\n",
    "            # replace any oov token with <UNK>\n",
    "            if w1 not in model.vocab:\n",
    "                w1 = model.unk_token    \n",
    "            if w2 not in model.vocab:\n",
    "                w2 = model.unk_token\n",
    "            sum_log_probs += np.log(model.bg_prob(w1, w2))\n",
    "            n += 1\n",
    "    sum_log_probs *= (-1/n) \n",
    "    perplexity = np.exp(sum_log_probs)\n",
    "    return perplexity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k = 1.0\n",
      "Perplexity computed on training set: 734.551\n",
      "Perplexity computed on test set: 849.244\n",
      "\n",
      "k = 0.1\n",
      "Perplexity computed on training set: 211.417\n",
      "Perplexity computed on test set: 384.699\n",
      "\n",
      "k = 0.01\n",
      "Perplexity computed on training set: 91.660\n",
      "Perplexity computed on test set: 293.837\n",
      "\n",
      "k = 0.001\n",
      "Perplexity computed on training set: 62.647\n",
      "Perplexity computed on test set: 356.312\n",
      "\n",
      "k = 0.0001\n",
      "Perplexity computed on training set: 56.270\n",
      "Perplexity computed on test set: 560.490\n",
      "\n",
      "k = 1e-05\n",
      "Perplexity computed on training set: 55.360\n",
      "Perplexity computed on test set: 957.605\n"
     ]
    }
   ],
   "source": [
    "# now lets compute perplexity on both the training and test data for different k values\n",
    "kvals = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for k in kvals:\n",
    "    model.k = k\n",
    "    pp_train = compute_perplexity(model, sentences_train)\n",
    "    pp_test = compute_perplexity(model, sentences_test)\n",
    "\n",
    "    print(f\"\\nk = {k}\")\n",
    "    print(f\"Perplexity computed on training set: {pp_train:.3f}\")\n",
    "    print(f\"Perplexity computed on test set: {pp_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the best perpexlixty on the test set seems to be ~290.\n",
    "\n",
    "#### Now we will try a different type of smoothing which interpolates between bigram, unigram and zerogram probabilities (zerogram probability is defined as just 1/|V|) in the following way:\n",
    "\n",
    "$\\hat{P}(w_k|w_{k-1}) = \\lambda_2 P(w_k|w_{k-1}) + \\lambda_1 P(w_k) + \\lambda_0 P(0)$\n",
    "\n",
    "where $P(w_k|w_{k-1}) = \\frac{C(w_k, w_{k-1})}{C(w_{k-1})}$, $P(w_k) = \\frac{C(w_k)}{\\sum_{w \\in V} C(w)}$ and $P(0) = \\frac{1}{|V|}$\n",
    "\n",
    "and $\\lambda_0$, $\\lambda_1$, $\\lambda_2$ are constant interpolation weights which sum to 1 and whose values must be chosen such that the performance of the model on a held out test set is maximised. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bigram_LM_interp():\n",
    "\n",
    "    def __init__(self, count_threshold=2, lmda = [0.01, 0.4, 0.59]):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.lmda = lmda\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.total_tokens = None\n",
    "        self.unk_token = '<UNK>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts, self.total_tokens = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        total_tokens = 0\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "                total_tokens += 1\n",
    "        print(f\"Num unigrams: {len(unigram_counts)}\")        \n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "        print(f\"Num bigrams: {len(bigram_counts)}\")        \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts, total_tokens\n",
    "\n",
    "    def compute_probs(self, word1):\n",
    "        #print(\"Computing bigram probabilities...\")\n",
    "        probs = []\n",
    "        for word2 in self.vocab:\n",
    "            # compute P(word2|word1)\n",
    "            p = self.bg_prob(word1, word2)\n",
    "            probs.append(p)\n",
    "        return probs   \n",
    "\n",
    "    def bg_prob(self, word1, word2):\n",
    "        # linearly interpolated probability\n",
    "        p_zerogram = self.lmda[0] * 1 / len(self.vocab)\n",
    "        p_unigram =  self.lmda[1] * self.unigram_counts[word2] / self.total_tokens \n",
    "        p_bigram = self.lmda[2] * self.bigram_counts[(word1, word2)] / self.unigram_counts[word1] \n",
    "        p = p_zerogram + p_unigram + p_bigram\n",
    "        return p        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Num unigrams: 6474\n",
      "Collecting bigram counts...\n",
      "Num bigrams: 77269\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = bigram_LM_interp()\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performed by the so\n",
      "'tis to tame you and is my lord\n",
      "if you and he bloody\n",
      "thine that i\n",
      "and apprehend thee arms and thy not so well pleasing tongue milk your <UNK> a week and mannerly <UNK>\n",
      "covert\n",
      "gaze his i say <UNK> measure still thou in thy that unhappy brother' let mowbray highest\n",
      "coriolanus\n",
      "follow satisfied\n",
      "first yet thy dry to the north\n",
      "so\n",
      "you\n",
      "<UNK> bianca practise his mercy you <UNK> <UNK>\n",
      "alonso tell who but if you not so much better ere service for't he be <UNK>\n",
      "lute\n",
      "lions that my true\n",
      "attend\n",
      "was this\n",
      "raise up\n",
      "this young and to this same\n",
      "her\n",
      "form fain\n",
      "rosaline meet\n",
      "the happy days buried in the keys there speak <UNK> him go ask him to be considered\n",
      "<UNK>\n",
      "godden i can behold you hence her i am content content\n",
      "grace but knaves live\n",
      "here master\n",
      "<UNK> war\n",
      "no bigger women are ruled <UNK> keeps\n",
      "friends as your honour my was against thy invite\n",
      "biondello\n",
      "is at\n",
      "though angel doth stand was made\n",
      "that\n",
      "all on nay faith then been too have think'st more too <UNK>\n",
      "juliet bleeding slaughter thought brawling discontent\n",
      "that <UNK> how thy of came to this pedant my but master is it do saw her mirth my ay\n",
      "him till that front extremity and\n",
      "with this the hour\n",
      "of his substitutes rutland i exton\n",
      "there\n",
      "even then mine to find when\n",
      "this feast and that but we all by this strife\n",
      "colours know his\n",
      "elbow rest tonight\n",
      "since northumberland\n",
      "how i will thou know the court an indifferent\n",
      "to see thee\n",
      "lucio\n",
      "here comes he early tongue give\n",
      "rough and me again\n",
      "kin and my is pardon thee to heavier doom thee\n",
      "come there i run her be no then more profit\n",
      "sends we of creation mar our <UNK> henry gloucester blood\n",
      "your heart to the matter\n",
      "without her stage\n",
      "for buried once\n",
      "and for for death\n",
      "i\n",
      "the base thou think'st thou the that i\n",
      "hath thine steel\n",
      "for petruchio\n",
      "have helped the make the truth the deadly quarrel\n",
      "you this your wrath\n",
      "near her <UNK>\n",
      "within\n",
      "meddle with thy lords in both to answer\n",
      "but to your hands\n",
      "king know\n",
      "quoth a'\n",
      "and so to pray of sicilia\n",
      "they're busy stay\n",
      "the book indeed i she with mercy you beat or else\n",
      "provost\n",
      "my child\n",
      "petruchio\n",
      "provost\n",
      "prefer\n",
      "grace alone marcius will give me welcome destruction cast\n",
      "conspirator\n",
      "and <UNK> this poor henry vi\n",
      "capulet with thy treacherous\n",
      "<UNK> <UNK> villain husband be thou shouldst both and ostentation for once\n",
      "petruchio\n",
      "breathe grave that and roman but a i what is her uncle\n",
      "wrench awe i thank camillo\n",
      "yes it me with you\n",
      "and therefore i rather feel thy mind smiles not\n",
      "my well care i can be so i the business that ever be\n",
      "husband and we service head trades in of gaunt and god bear\n",
      "what of heaps of york\n",
      "<UNK>\n",
      "fond chair sir son good doers in hope this good my knee\n",
      "nor profits you\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lambdas = [0.01, 0.49, 0.5]\n",
      "Perplexity computed on training set: 84.996\n",
      "Perplexity computed on test set: 200.927\n",
      "\n",
      "lambdas = [0.01, 0.39, 0.6]\n",
      "Perplexity computed on training set: 76.123\n",
      "Perplexity computed on test set: 197.657\n",
      "\n",
      "lambdas = [0.01, 0.29000000000000004, 0.7]\n",
      "Perplexity computed on training set: 69.202\n",
      "Perplexity computed on test set: 199.101\n",
      "\n",
      "lambdas = [0.01, 0.18999999999999995, 0.8]\n",
      "Perplexity computed on training set: 63.654\n",
      "Perplexity computed on test set: 207.434\n",
      "\n",
      "lambdas = [0.01, 0.08999999999999997, 0.9]\n",
      "Perplexity computed on training set: 59.127\n",
      "Perplexity computed on test set: 231.764\n",
      "\n",
      "lambdas = [0.01, 0.040000000000000036, 0.95]\n",
      "Perplexity computed on training set: 57.178\n",
      "Perplexity computed on test set: 265.303\n"
     ]
    }
   ],
   "source": [
    "# now lets compute perplexity on both the training and test data for different lambda values (lambda_0 will be held fixed at 0.01)\n",
    "lambda2_vals = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "for l2 in lambda2_vals:\n",
    "    model.lmda = [0.01, 0.99-l2 ,l2]\n",
    "    pp_train = compute_perplexity(model, sentences_train)\n",
    "    pp_test = compute_perplexity(model, sentences_test)\n",
    "\n",
    "    print(f\"\\nlambdas = {model.lmda}\")\n",
    "    print(f\"Perplexity computed on training set: {pp_train:.3f}\")\n",
    "    print(f\"Perplexity computed on test set: {pp_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that with interpolation, we get much lower perplexity on the test set compared to add-k smoothing. The best value is ~190. The quality of the generated text also seems to be slightly better, but that's hard to tell for sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing bigram probabilities...\n"
     ]
    }
   ],
   "source": [
    "model.lmda = [0.01, 0.99-0.8 ,0.8]\n",
    "model.compute_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the precious jewel strong purpose not exempt in thy rocky bosom of the maid hath banish'd haughty mind\n",
      "and all\n",
      "that princely knee rise we marry i think but a man life\n",
      "your subject made disgraced <UNK> <UNK> will of virtue\n",
      "it will by this while a <UNK> <UNK> then shepherd\n",
      "sir there\n",
      "escalus\n",
      "servant\n",
      "my memory of prompt my have forgot\n",
      "prince\n",
      "thy loss well he you\n",
      "he shall i are a concealment\n",
      "find a consul\n",
      "to thrust myself\n",
      "these other home\n",
      "and\n",
      "you\n",
      "whose feeling but we have knowledge find love\n",
      "petruchio\n",
      "nay good brother i shall be there to us\n",
      "some pretty i' faith the maid's mild entreaty shall wear the high'st my <UNK>\n",
      "no is well that moving\n",
      "sir\n",
      "what say'st thou take this\n",
      "broke off send tybalt's doomsday is\n",
      "and to god on and sir king richard moe\n",
      "beg starve\n",
      "where's barnardine partial to <UNK>\n",
      "would <UNK> night\n",
      "but till he that warwick's daughter is but\n",
      "northumberland\n",
      "corioli wear their king usurping him but this there brother die to pass\n",
      "all kneel for exile him mistress and your hand that i know not have lighted on my revenge france from his lost it for large enough\n",
      "and in the advantage of all\n",
      "return\n",
      "mean my bones not at meeting here comes and spit forth in sour misfortune's book welcome sir a schoolmaster for the state and there was about his death\n",
      "that yet york\n",
      "which are by the first london\n",
      "do and here comes my son\n",
      "my child and <UNK> hate art flying hare sir first murderer\n",
      "is on this <UNK> for't would cure as hard for grace would not sir\n",
      "lucentio\n",
      "die upon sunday following this is\n",
      "sir what\n",
      "convey him\n",
      "i'll rail on me\n",
      "three shepherds for by holy humour\n",
      "your highness' fail lord stanley is here in my wounds <UNK> depending go tell me let your absence to <UNK> by the to the steed the country\n",
      "sit in extremity was at the law to down again farewell i was his name antonio's son of ambitious edward as desperate would the one another woe\n",
      "stand upright gentleman\n",
      "bona shall <UNK> <UNK> every day's journey to be\n",
      "<UNK> by his general of their love i have all our lances arbitrate\n",
      "if both me better witness the no such events with us its his burial\n",
      "and therefore farewell old he shall we the <UNK> come starts i' the break off\n",
      "<UNK> to whistle then vengeance on the english court richard without changing woman with <UNK> do bid had the <UNK>\n",
      "that beggars\n",
      "gloucester\n",
      "where he not that\n",
      "and little din of quarrels and for his charters\n",
      "matter good\n",
      "gremio glory did fight\n",
      "while\n",
      "and the life a\n",
      "petruchio\n",
      "daughter mine own pembroke and after aumerle\n",
      "and would be all the gown\n",
      "they come good with hers make <UNK> and honour\n",
      "thee art as free deep\n",
      "hold my troth i'll bury with choke the sun is my good\n",
      "go and the best swelling difference as thou misshapen belonging to draw\n",
      "to colour\n",
      "if i am loath\n",
      "i be hanged sir you are you be worse than she and herbs plants stones farewell thou hither nurse\n",
      "what fine issues so dishonour'd soul flies this world\n",
      "i pray thee\n",
      "to god to take your crown\n",
      "no need no incense the last i correction i could him joy hearts for lancaster usurps strengthen themselves is not he is nothing can water did with <UNK> grave\n",
      "citizens\n",
      "the which way\n",
      "your lordship <UNK>\n",
      "and himself visit you cleomenes\n",
      "thyself\n",
      "no more <UNK> an\n",
      "all our distress but thou art thou remember a most fitly\n",
      "is\n",
      "thus to the numbers to my drugs i am content to say\n",
      "katharina now whilst i will come masters and with her humour\n",
      "which of enforce his fellows\n",
      "had wars\n",
      "<UNK>\n",
      "to breath\n",
      "furred with thyself malice makes be gone be safe hast slept between us to know\n",
      "what dost thou hast most ignorant of my woman's tenderness if one another such a feasting sooth\n",
      "and i'll tell you would my scene shall\n",
      "did mean to clap into some do\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=100)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now train using the NLTK brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 51606\n",
      "Number of test sentences: 5734\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# get pre-tokenized sentences\n",
    "sentences = list(brown.sents())\n",
    "\n",
    "# make everything lowercase and add start and end tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'\n",
    "sentences_tokenized = [[start_token]+[w.lower() for w in s]+[end_token] for s in sentences]\n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_test = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_test.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_test)}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Collecting bigram counts...\n",
      "Computing bigram probabilities...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m bigram_LM_addk(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mbigram_LM_addk.train\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munigram_counts\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword2idx \u001b[38;5;241m=\u001b[39m {word:i \u001b[38;5;28;01mfor\u001b[39;00m i,word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)}\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36mbigram_LM_addk.compute_probs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m probs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# compute P(word2|word1)\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbg_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     probs\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[1;32m     75\u001b[0m bigram_probs[word1] \u001b[38;5;241m=\u001b[39m probs \n",
      "Cell \u001b[0;32mIn[3], line 78\u001b[0m, in \u001b[0;36mbigram_LM_addk.bg_prob\u001b[0;34m(self, word1, word2)\u001b[0m\n\u001b[1;32m     75\u001b[0m         bigram_probs[word1] \u001b[38;5;241m=\u001b[39m probs \n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigram_probs \u001b[38;5;241m=\u001b[39m bigram_probs   \n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbg_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, word1, word2):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# addk probability\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     p \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbigram_counts[(word1, word2)] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munigram_counts[word1] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)) \n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = bigram_LM_addk(k=0.001)\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = generate_text(model, n=100)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
