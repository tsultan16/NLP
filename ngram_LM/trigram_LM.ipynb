{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/tanzid/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, we will train on the NLTK brown corpus, keeping all the punctutation, but still use lowercase folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pre-tokenized sentences\n",
    "sentences = list(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 51606\n",
      "Number of test sentences: 5734\n"
     ]
    }
   ],
   "source": [
    "# make everything lowercase and add start and end tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'\n",
    "sentences_tokenized = [[start_token]*2+[w.lower() for w in s]+[end_token] for s in sentences]\n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_test = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_test.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_test)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.', '</s>']\n",
      "['<s>', '<s>', 'the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.', '</s>']\n",
      "['<s>', '<s>', 'the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr.', '.', '</s>']\n",
      "['<s>', '<s>', '``', 'only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.', '</s>']\n",
      "['<s>', '<s>', 'the', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(sentences_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trigram_LM_addk():\n",
    "\n",
    "    def __init__(self, count_threshold=2, k=0.1):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.k = k\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.trigram_counts = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.num_sentences = None\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<s>'        \n",
    "        self.end_token = '</s>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.num_sentences = len(sentences)\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts, self.trigram_counts = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        print(f\"Total num unigrams: {len(unigram_counts)}\")        \n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "        print(f\"Total num bigrams: {len(bigram_counts)}\")        \n",
    "    \n",
    "        # collect trigram counts\n",
    "        print(\"Collecting trigram counts...\")\n",
    "        trigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for trigram in zip(s[:-2], s[1:-1], s[2:]):\n",
    "                trigram_counts[trigram] += 1     \n",
    "        print(f\"Total num trigrams: {len(trigram_counts)}\")                \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts, trigram_counts\n",
    "    \n",
    "    def compute_probs(self, word1, word2):\n",
    "        probs = []\n",
    "        for word3 in self.vocab:\n",
    "            # compute P(word3|word1,word2)\n",
    "            p = self.tg_prob(word1, word2, word3)\n",
    "            probs.append(p)\n",
    "        return probs      \n",
    "    \n",
    "    def tg_prob(self, word1, word2, word3):\n",
    "        # addk probability\n",
    "        p = (self.trigram_counts[(word1,word2,word3)] + self.k) / (self.bigram_counts[(word1,word2)] + self.k*len(self.vocab)) \n",
    "        return p        \n",
    "    \n",
    "\n",
    "class trigram_LM_interp():\n",
    "\n",
    "    def __init__(self, count_threshold=2, lmda = [0.01, 0.2, 0.3, 0.49]):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.lmda = lmda\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.trigram_counts = None\n",
    "        self.total_tokens = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.num_sentences = None\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<s>'        \n",
    "        self.end_token = '</s>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.num_sentences = len(sentences)\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts, self.trigram_counts, self.total_tokens = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        total_tokens = 0\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "                total_tokens += 1\n",
    "        print(f\"Total num unigrams: {len(unigram_counts)}\")        \n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "        print(f\"Total num bigrams: {len(bigram_counts)}\")        \n",
    "    \n",
    "        # collect trigram counts\n",
    "        print(\"Collecting trigram counts...\")\n",
    "        trigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for trigram in zip(s[:-2], s[1:-1], s[2:]):\n",
    "                trigram_counts[trigram] += 1     \n",
    "        print(f\"Total num trigrams: {len(trigram_counts)}\")                \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts, trigram_counts, total_tokens\n",
    "    \n",
    "    def compute_probs(self, word1, word2):\n",
    "        probs = []\n",
    "        for word3 in self.vocab:\n",
    "            # compute P(word3|word1,word2)\n",
    "            p = self.tg_prob(word1, word2, word3)\n",
    "            probs.append(p)\n",
    "        return probs\n",
    "\n",
    "    def tg_prob(self, word1, word2, word3):\n",
    "        # linearly interpolated probability\n",
    "        p_zerogram = self.lmda[0] / len(self.vocab)\n",
    "        p_unigram  = self.lmda[1] * self.unigram_counts[word3] / self.total_tokens \n",
    "        p_bigram   = self.lmda[2] * self.bigram_counts[(word2, word3)] / self.unigram_counts[word2] \n",
    "        if self.bigram_counts[(word1, word2)] > 0:\n",
    "            p_trigram  = self.lmda[3] * self.trigram_counts[(word1, word2, word3)] / self.bigram_counts[(word1, word2)]  \n",
    "        else:\n",
    "            p_trigram = 0\n",
    "        p = p_zerogram + p_unigram + p_bigram + p_trigram\n",
    "        return p        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, n=10, max_len=200):\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    for i in range(n):\n",
    "        context_w1 = '<s>'\n",
    "        context_w2 = '<s>'\n",
    "        words = []    \n",
    "        while True:\n",
    "            # get probabilities of next word given current context, i.e P(w|context_w1, context_w2)\n",
    "            probs = model.compute_probs(context_w1, context_w2)\n",
    "            # now sample from the vocabulry according to this distribution\n",
    "            next_word = random.choices(model.vocab, weights=probs, k=1)[0]\n",
    "            if next_word == '</s>' or len(words) == max_len:\n",
    "                break\n",
    "            if next_word == '<s>':\n",
    "                continue    \n",
    "            words.append(next_word)\n",
    "            context_w1 = context_w2\n",
    "            context_w2 = next_word\n",
    "        if len(words) > 0:    \n",
    "            sentences.append(\" \".join(words))\n",
    "        i += 1\n",
    "         \n",
    "        \n",
    "    return \"\\n\".join(sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Total num unigrams: 26362\n",
      "Collecting bigram counts...\n",
      "Total num bigrams: 369300\n",
      "Collecting trigram counts...\n",
      "Total num trigrams: 770624\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "model = trigram_LM_addk(k=0.0000001)\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a description of what their neighbors , studying interviews with members of the cake with chocolate <UNK> .\n",
      "in spite of this abuse , he was sympathetic to the boarding ladder together .\n",
      "grabski looked at lawrence with a heavy list to dictionary forms and shapes and made a world-shaking contribution to hollywood , who gave hope and courage to the private detective ( at the right tone of kindly <UNK> : term of office buildings in the air\n",
      "the check , mercer collaborated with many early theologians , especially since he was `` stung '' with the <UNK> sexual coming of the dead people , their murderers dismembered the bodies are missing , for hosses had <UNK> from saratoga up to the editor of the new spirit , these blocks were set one within the system increases , and she hoped he was still there and waited for it is important here is truly majestic and an orchestra will take a look at penny .\n",
      "he hated them too .\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=5)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_prob(model, s):\n",
    "    sum_log_probs = 0.0\n",
    "    n = 0\n",
    "    for w1,w2,w3 in zip(s[:-2], s[1:-1], s[2:]):\n",
    "        # replace any oov token with <UNK>\n",
    "        if w1 not in model.vocab:\n",
    "            w1 = model.unk_token    \n",
    "        if w2 not in model.vocab:\n",
    "            w2 = model.unk_token\n",
    "        if w3 not in model.vocab:\n",
    "            w3 = model.unk_token\n",
    "        sum_log_probs += np.log(model.tg_prob(w1, w2, w3))\n",
    "        n += 1\n",
    "    return sum_log_probs, n\n",
    "\n",
    "def compute_perplexity(model, test_sentences, num_procs=8):\n",
    "    # create a partial function with model argument fixed\n",
    "    func = partial(compute_log_prob, model)\n",
    "    # distribute computation across parallel processes\n",
    "    with Pool(num_procs) as p:\n",
    "        results = p.map(func, test_sentences)\n",
    "    sum_log_probs = sum(result[0] for result in results)\n",
    "    n = sum(result[1] for result in results)\n",
    "    sum_log_probs *= (-1/n) \n",
    "    perplexity = np.exp(sum_log_probs)\n",
    "    return perplexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate the add-k trigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Total num unigrams: 26362\n",
      "Collecting bigram counts...\n",
      "Total num bigrams: 369300\n",
      "Collecting trigram counts...\n",
      "Total num trigrams: 770624\n",
      "Training complete!\n",
      "\n",
      "k = 1.0\n",
      "Perplexity computed on training set: 6423.655\n",
      "Perplexity computed on test set: 9944.344\n",
      "\n",
      "k = 0.1\n",
      "Perplexity computed on training set: 1164.288\n",
      "Perplexity computed on test set: 5172.969\n",
      "\n",
      "k = 0.01\n",
      "Perplexity computed on training set: 177.566\n",
      "Perplexity computed on test set: 3058.485\n",
      "\n",
      "k = 0.001\n",
      "Perplexity computed on training set: 36.185\n",
      "Perplexity computed on test set: 2512.401\n",
      "\n",
      "k = 0.0001\n",
      "Perplexity computed on training set: 12.569\n",
      "Perplexity computed on test set: 3249.263\n",
      "\n",
      "k = 1e-05\n",
      "Perplexity computed on training set: 8.315\n",
      "Perplexity computed on test set: 6355.444\n"
     ]
    }
   ],
   "source": [
    "model = trigram_LM_addk(k=0.01)\n",
    "model.train(sentences_train)\n",
    "\n",
    "# now lets compute perplexity on both the training and test data for different k values\n",
    "kvals = [1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "for k in kvals:\n",
    "    model.k = k\n",
    "    pp_train = compute_perplexity(model, sentences_train)\n",
    "    pp_test = compute_perplexity(model, sentences_test)\n",
    "\n",
    "    print(f\"\\nk = {k}\")\n",
    "    print(f\"Perplexity computed on training set: {pp_train:.3f}\")\n",
    "    print(f\"Perplexity computed on test set: {pp_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that the perplexity on the training set gets really low, below 10 indicating that the model is able to fit the training data quite well. However also note that the test set perplexity values are very large (much larger than what we saw for the bigram model) which indicates that the model may be severely overfitting to the training set and does not generalize well to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n",
      "Re-collecting unigram counts...\n",
      "Total num unigrams: 26362\n",
      "Collecting bigram counts...\n",
      "Total num bigrams: 369300\n",
      "Collecting trigram counts...\n",
      "Total num trigrams: 770624\n",
      "Training complete!\n",
      "\n",
      "lambdas = [0.01, 0.1, 0.5900000000000001, 0.3]\n",
      "Perplexity computed on training set: 16.401\n",
      "Perplexity computed on test set: 299.602\n",
      "\n",
      "lambdas = [0.01, 0.1, 0.39, 0.5]\n",
      "Perplexity computed on training set: 12.249\n",
      "Perplexity computed on test set: 326.992\n",
      "\n",
      "lambdas = [0.01, 0.1, 0.19000000000000006, 0.7]\n",
      "Perplexity computed on training set: 9.949\n",
      "Perplexity computed on test set: 398.139\n",
      "\n",
      "lambdas = [0.01, 0.1, 0.08999999999999997, 0.8]\n",
      "Perplexity computed on training set: 9.134\n",
      "Perplexity computed on test set: 490.479\n",
      "\n",
      "lambdas = [0.01, 0.1, 0.040000000000000036, 0.85]\n",
      "Perplexity computed on training set: 8.782\n",
      "Perplexity computed on test set: 600.247\n"
     ]
    }
   ],
   "source": [
    "model = trigram_LM_interp()\n",
    "model.train(sentences_train)\n",
    "\n",
    "lambda3_vals = [0.3, 0.5, 0.7, 0.8, 0.85]\n",
    "for l3 in lambda3_vals:\n",
    "    model.lmda = [0.01, 0.1, 0.89-l3 ,l3]\n",
    "    pp_train = compute_perplexity(model, sentences_train)\n",
    "    pp_test = compute_perplexity(model, sentences_test)\n",
    "\n",
    "    print(f\"\\nlambdas = {model.lmda}\")\n",
    "    print(f\"Perplexity computed on training set: {pp_train:.3f}\")\n",
    "    print(f\"Perplexity computed on test set: {pp_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With interpolation smoothing, the test set perplexities are substantially lower, indicating that the model is not severely overfitting to the training set which happened with the add-k smoothing. It seems to do a better job at generalizing to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unigram counts...\n",
      "Constructing vocab...\n",
      "Replacing with oov tokens in training data...\n"
     ]
    }
   ],
   "source": [
    "model = trigram_LM_interp(lmda=[0.01, 0.1, 0.39 ,0.5])\n",
    "model.train(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathing the itching parts of the painting , unless it made only presidential appointees to concern its watershed lands from erosion and <UNK> showed that operations continued through the hymen .\n",
      "about noon they came out durability peculiar to it , did the heated air , organized to furnish a statement was also its weakness .\n",
      "`` from its dullest season .\n",
      "she became intelligible .\n",
      "yes '' area at least be safe to assume a center of the see considerably narrower ground of the open scaffold .\n"
     ]
    }
   ],
   "source": [
    "text = generate_text(model, n=10)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
