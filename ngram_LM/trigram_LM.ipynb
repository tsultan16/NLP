{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/tanzid/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, we will train on the NLTK brown corpus, keeping all the punctutation, but still use lowercase folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pre-tokenized sentences\n",
    "sentences = list(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 51606\n",
      "Number of test sentences: 5734\n"
     ]
    }
   ],
   "source": [
    "# make everything lowercase and add start and end tokens\n",
    "start_token = '<s>'        \n",
    "end_token = '</s>'\n",
    "sentences_tokenized = [[start_token]+[w.lower() for w in s]+[end_token] for s in sentences]\n",
    "\n",
    "# now we split the data into train and test sentences\n",
    "num_sent = len(sentences_tokenized)\n",
    "num_test = int(0.1 * num_sent)\n",
    "test_idx = random.sample(range(num_sent), num_test)\n",
    "\n",
    "sentences_train = []\n",
    "sentences_test = []\n",
    "for i in range(num_sent):\n",
    "    if i not in test_idx:\n",
    "        sentences_train.append(sentences_tokenized[i])\n",
    "    else:\n",
    "        sentences_test.append(sentences_tokenized[i])    \n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")        \n",
    "print(f\"Number of test sentences: {len(sentences_test)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'the', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'city', 'executive', 'committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'city', 'of', 'atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.', '</s>']\n",
      "['<s>', 'the', 'september-october', 'term', 'jury', 'had', 'been', 'charged', 'by', 'fulton', 'superior', 'court', 'judge', 'durwood', 'pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'mayor-nominate', 'ivan', 'allen', 'jr.', '.', '</s>']\n",
      "['<s>', '``', 'only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.', '</s>']\n",
      "['<s>', 'the', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', \"georgia's\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.', '</s>']\n",
      "['<s>', 'it', 'recommended', 'that', 'fulton', 'legislators', 'act', '``', 'to', 'have', 'these', 'laws', 'studied', 'and', 'revised', 'to', 'the', 'end', 'of', 'modernizing', 'and', 'improving', 'them', \"''\", '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(sentences_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trigram_LM_addk():\n",
    "\n",
    "    def __init__(self, count_threshold=2, k=0.1):\n",
    "        self.count_threshold = count_threshold \n",
    "        self.k = k\n",
    "        self.bigram_counts = None\n",
    "        self.unigram_counts = None\n",
    "        self.vocab = None\n",
    "        self.word2idx = None\n",
    "        self.bigram_probs = None\n",
    "        self.num_sentences = None\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<s>'        \n",
    "        self.end_token = '</s>'\n",
    "\n",
    "    def train(self, sentences):\n",
    "        self.num_sentences = len(sentences)\n",
    "        self.vocab, self.unigram_counts, self.bigram_counts = self.get_counts(sentences)\n",
    "        self.vocab = list(self.unigram_counts.keys())\n",
    "        self.word2idx = {word:i for i,word in enumerate(self.vocab)}\n",
    "        self.compute_probs()\n",
    "        print(\"Training complete!\")         \n",
    "\n",
    "    def get_counts(self, sentences):\n",
    "        # collect unigram counts \n",
    "        print(\"Collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        \n",
    "        # remove all words that have count below the threshold    \n",
    "        print(\"Constructing vocab...\")     \n",
    "        for w in list(unigram_counts.keys()):\n",
    "            if unigram_counts[w] < self.count_threshold:\n",
    "                unigram_counts.pop(w)\n",
    "        # construct vocab \n",
    "        vocab = [self.unk_token] + sorted(list(unigram_counts.keys()))            \n",
    "        \n",
    "        # replace all oov tokens in training sentences with <UNK>\n",
    "        print(\"Replacing with oov tokens in training data...\")\n",
    "        sentences_unk = []\n",
    "        for s in sentences:\n",
    "            sent = []\n",
    "            for word in s:\n",
    "                if word in vocab:\n",
    "                    sent.append(word)\n",
    "                else:\n",
    "                    sent.append(self.unk_token)\n",
    "            sentences_unk.append(sent)            \n",
    "\n",
    "        # re-collect unigram counts \n",
    "        print(\"Re-collecting unigram counts...\")\n",
    "        unigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for word in s:\n",
    "                unigram_counts[word] += 1 \n",
    "        print(f\"Total num unigrams: {len(unigram_counts)}\")        \n",
    "\n",
    "        # collect bigram counts\n",
    "        print(\"Collecting bigram counts...\")\n",
    "        bigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for bigram in zip(s[:-1], s[1:]):\n",
    "                bigram_counts[bigram] += 1     \n",
    "        print(f\"Total num bigrams: {len(bigram_counts)}\")        \n",
    "\n",
    "        # add an extra start token at the beginning of all sentences\n",
    "        sentences_unk = [[self.start_token]+s for s in sentences_unk]        \n",
    "        # collect trigram counts\n",
    "        print(\"Collecting trigram counts...\")\n",
    "        trigram_counts = Counter()\n",
    "        for s in sentences_unk:\n",
    "            for trigram in zip(s[:-2], s[1:-1], s[2:]):\n",
    "                trigram_counts[trigram] += 1     \n",
    "        print(f\"Total num trigrams: {len(trigram_counts)}\")                \n",
    "\n",
    "        return vocab, unigram_counts, bigram_counts, trigram_counts\n",
    "    \n",
    "    def compute_probs(self):\n",
    "        print(\"Computing trigram probabilities...\")\n",
    "        trigram_probs = Counter()\n",
    "        for word1 in self.vocab:\n",
    "            probs = []\n",
    "            for word2 in self.vocab:\n",
    "                # compute P(word2|word1)\n",
    "                p = self.bg_prob(word1, word2)\n",
    "                probs.append(p)\n",
    "            trigram_probs[word1] = probs \n",
    "        self.trigram_probs = trigram_probs   \n",
    "\n",
    "    def bg_prob(self, word1, word2):\n",
    "        # addk probability\n",
    "        p = (self.bigram_counts[(word1, word2)] + self.k) / (self.unigram_counts[word1] + self.k*len(self.vocab)) \n",
    "        return p        \n",
    "    \n",
    "    def tg_prob(self, word1, word2, word3):\n",
    "        # addk probability\n",
    "        p = (self.trigram_counts[(word1,word2,word3)] + self.k) / (self.bigram_counts[(word1,word2)] + self.k*len(self.vocab)) \n",
    "        return p        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
