{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMM)\n",
    "\n",
    "Hidden Markov Models can help us solve the problem of sequence labelling, e.g. given a senquence of words, finding a corresponding sequence of parts-of-speech(POS) tags. HMMs are an extension of markov chains. The state (random) variables $T=\\{t_1, t_2,...,t_N\\}$ (e.g. $N$ different POS tags) are considered `hidden`. Given a sequence of these hidden states, e.g. $t_1,...,t_{i-1}$, we invoke the `markov assumption` (a.k.a. same as bigram LM), i.e. the next state in the sequence depends only on the previous state, so $P(t_i|t_1,..,t_{i-1}) = P(t_i|t_{i-1})$, these are also called `transition probabilities`. Since the state variables are drawn from an identical categorial distribution (because we have a finite number of possible values for the state), we can use `maximum likelihood estimation` to estimate the transition probabilities from a training corpus:\n",
    "\n",
    "$P(t_i|t_{i-1}) = \\frac{count(t_{i-1},t_i)}{count(t_{i-1})}$\n",
    "\n",
    "Instead of observing a sequence of states directly, we observe a sequence of `observations` $O=\\{w_1, .., w_T\\}$, which are a different set of random variables. In the case of POS tagging, these are a sequence of $T$ words and each $w_i$ is drawn from the same vocabulary $V$. Here we invoke the `output independence assumption` according to which the $ith$ observation in the sequence only depends on the corresponding $ith$ hidden state (i.e. the $ith$ state generates the $ith$ observation) which means that $P(w_i|t_1, ..t_T, w_1, ..,w_T) = P(w_i|t_i)$, these are called `observation likelihoods`/`emission probabilites` and can also be estimated using MLE:\n",
    "\n",
    "$P(w_i|t_i) = \\frac{count(w_i,t_i)}{count(t_i)}$\n",
    "\n",
    "\n",
    "\n",
    "Our main goal is to `infer`/`decode` the most likely sequence of hidden states that could have generated the observed sequence of words:   \n",
    "\n",
    "$\\hat{t}_{1:T} = \\text{argmax}_{t_{1:T}} \\text{ } P(t_1,..,t_T|w_1,..,w_T) =  \\text{argmax}_{t_{1:T}} \\frac{P(w_1,..,w_T|t_1,..,t_T) P(t_1,..,t_T)}{P(w_1,..,w_T)} = \\text{argmax}_{t_{1:T}} P(w_1,..,w_T|t_1,..,t_T) P(t_1,..,t_T)$\n",
    "\n",
    "where we used Baye's rule. Invoking the output independence assumptions for the observations, we can write: $P(w_1,..,w_T|t_1,..,t_T) = \\prod_{i=1}^T P(w_i|t_i)$ and invoking the markov assumption for the states, we can write: $P(t_1,..,t_T) = \\prod_{i=1}^T P(t_i|t_{i-1})$ which leads to the following:\n",
    "\n",
    "$\\hat{t}_{1:T} = \\text{argmax}_{t_{1:T}} \\text{ } \\prod_{i=1}^T P(t_i|t_{i-1})  P(w_i|t_i)$\n",
    "\n",
    "Instead of computing this product exhaustively for all possible sequence and then finding the maximum, we can note that there is an optimal substructure, i.e. the subsequence $\\hat{t}_{1:i}$ is an optimal solution for the observed subsequence up to the $ith$ word, therefore we can use `dynamic programming` to obtain the solution more efficiently, aka the `Viterbi algorithm`.\n",
    "\n",
    "In the Viterbi algorithm, we can set up a matrix whose columns represent the observations at each step and rows represent each possible hidden state. Then defining $v_t(j)$ as the cell in column $t$ and row $j$ which represents the probability of the HMM being in the state $j$ after seeing the first $t$ observations and passing through the most probable state subsequence $\\{t_1,..t_{i-1}\\}$, i.e. the most probable path to reach that cell. We can compute the value at each cell in column $t$, given that we've already computed the vaules in the preceding column $t-1$, using the following recurrence relation:\n",
    "\n",
    "$v_t(j) = max_{i=1}^N \\text{ } v_{t-1}(i) P(t_j|t_i) P(w_t|t_j)$\n",
    "\n",
    "Note that we choose the tag that gives us the most probable extension of the path up to $t_i$ in the previous column. In addition, inside each cell, we also store a `backpointer` to that $t_i$.\n",
    "\n",
    "(this is very similar to the Dijkstra shortest path algorithm!)\n",
    "\n",
    "To run this algorithm, we need to initialize all the cells in the first column which are the probabilities for each possible hidden state given the first observation. We can compute these using the distribution over initial hidden states:\n",
    "\n",
    "$v_1(j) = P(t_j|<s>) P(w_1|t_j) = \\pi_j P(w_1|t_j)$, where $<s>$ denotes a special start of sequence hidden state (like a start of sentence token) and $\\pi_j = P(t_j|<s>)$ denotes the probability distribution over all possible starting states. Then using the recurrence relation, we can fill out the remaining columns one by one. Finally once we've computed the column $v_T(j)$, we can pick the cell with the largest probability which is the final state along the optimal path, $\\hat{t}_N = \\text{argmax}_j v_T(j)$ and trace backward along the optimal path using the backpointers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    A simple example from the Jurafsky textbook (section 8.4.6). We assume that the transition and emission probabilities have already been obtained from pretraining.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# seven different POS tags\n",
    "tags = [\"NNP\", \"MD\", \"VB\", \"JJ\", \"NN\", \"RB\", \"DT\"]\n",
    "tag_dict = {0: 'NNP', 1: 'MD', 2: 'VB', 3: 'JJ', 4: 'NN', 5: 'RB', 6: 'DT'}\n",
    "\n",
    "# observation sequence\n",
    "words = [\"Janet\", \"will\", \"back\", \"the\", \"bill\"]\n",
    "word2idx = {\"Janet\": 0, \"will\": 1, \"back\": 2, \"the\": 3, \"bill\": 4}\n",
    "\n",
    "# transition probabilities: A_ij = P(t_j|t_i)\n",
    "A = np.array([\n",
    "    [0.3777, 0.0110, 0.0009, 0.0084, 0.0584, 0.0090, 0.0025],\n",
    "    [0.0008, 0.0002, 0.7968, 0.0005, 0.0008, 0.1698, 0.0041],\n",
    "    [0.0322, 0.0005, 0.0050, 0.0837, 0.0615, 0.0514, 0.2231],\n",
    "    [0.0366, 0.0004, 0.0001, 0.0733, 0.4509, 0.0036, 0.0036],\n",
    "    [0.0096, 0.0176, 0.0014, 0.0086, 0.1216, 0.0177, 0.0068],\n",
    "    [0.0068, 0.0102, 0.1011, 0.1012, 0.0120, 0.0728, 0.0479],\n",
    "    [0.1147, 0.0021, 0.0002, 0.2157, 0.4744, 0.0102, 0.0017]\n",
    "    ])\n",
    "\n",
    "# initial state probabilities: Pi_j =  P(t_j|<s>)\n",
    "pi = np.array([0.2767, 0.0006, 0.0031, 0.0453, 0.0449, 0.0510, 0.2026])\n",
    "\n",
    "# emission probabilities B_jt = P(w_t|t_j)\n",
    "B = np.array([\n",
    "    [0.000032, 0, 0, 0.000048, 0],\n",
    "    [0, 0.308431, 0, 0, 0],\n",
    "    [0, 0.000028, 0.000672, 0, 0.000028],\n",
    "    [0, 0, 0.000340, 0.000097, 0],\n",
    "    [0, 0.000200, 0.000223, 0.000006, 0.002337],\n",
    "    [0, 0, 0.010446, 0, 0],\n",
    "    [0, 0, 0, 0.506099, 0]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of Viterbi Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_tagger(A, B, pi, words, word2idx, tags, tag_dict):\n",
    "    \n",
    "    # initialize the viterbi matrix\n",
    "    T = len(words)\n",
    "    N = len(tag_dict)\n",
    "    V = np.zeros(shape=(N, T))\n",
    "    V[:,0] = B[:,word2idx[words[0]]] * pi\n",
    "    backptr = np.zeros_like(V)\n",
    "\n",
    "    #np.set_printoptions(precision=2)\n",
    "    # apply recurrence relation\n",
    "    for t in range(1,T):\n",
    "        for j in range(N):\n",
    "            P_wt_tj = B[j,word2idx[words[t]]]\n",
    "            max_i = 0\n",
    "            max_v = 0\n",
    "            for i in range(N):\n",
    "                P_tj_ti = A[i,j]\n",
    "                v_t_j = V[i,t-1] * P_tj_ti * P_wt_tj\n",
    "                if v_t_j > max_v:\n",
    "                    max_v = v_t_j\n",
    "                    max_i = i\n",
    "            V[j,t] = max_v\n",
    "            backptr[j,t] = max_i        \n",
    "\n",
    "\n",
    "    # get the final state in potimal path\n",
    "    t_hat = []\n",
    "    t_hat_T = np.argmax(V[:,-1])\n",
    "    t_hat.insert(0,t_hat_T)\n",
    "    # back trace to get rest of the optimal path\n",
    "    t_hat_prev = t_hat_T\n",
    "    for t in range(T-1, 0, -1):\n",
    "        t_hat_prev = backptr[int(t_hat_prev), t]\n",
    "        t_hat.insert(0,int(t_hat_prev))\n",
    "    t_hat = [tags[t] for t in t_hat]\n",
    "    print(f\"Word sequence; {words}\")\n",
    "    print(f\"Tag sequence: {t_hat}\")\n",
    "\n",
    "    return t_hat    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word sequence; ['Janet', 'will', 'back', 'the', 'bill']\n",
      "Tag sequence: ['NNP', 'MD', 'VB', 'DT', 'NN']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NNP', 'MD', 'VB', 'DT', 'NN']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_tagger(A,B,pi,words,word2idx,tags,tag_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
