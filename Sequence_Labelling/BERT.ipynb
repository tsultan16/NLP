{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT based Sequence Labeller\n",
    "\n",
    "We've explored training HMM (Viterbi) and RNN-based POS (part of speech) taggers on tagged sentences from the Stanford treebank dataset. We saw that the HMM tagger had a validation accuracy of about 90% and the RNN based tagger had about 91%. We will now try a different type of neural approach. For the RNN, recall that we used pretrained GloVe embeddings to represent the words in a sentence. Since the meaning of words in a sentence can be ambiguous, we should use contextualized vector representations of words instead of fixed GloVe word embeddings to overcome this problem of word sense. Pretrained BERT models are perfect for this task because they can be used to extract contextualized word embedding. \n",
    "\n",
    "In this notebook, we will finetune a BERT model on the POS tagging task. Since BERT uses subword tokenization and POS labels are assigned to whole words, we need to figure out a way of assigning labels to the subword tokens. A simple approach is to assign the POS label of a word to the first subword in the sequence of subwords corresponding to that word, then assign a special tag 'X' to the remaining subwords, which indicates a continuation of the preceding POS label. e.g.\n",
    "\n",
    "`(spokesman, NN)` --> {`(spokes, NN)`, `(##man, X)`}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizerFast, RobertaModel, get_linear_schedule_with_warmup\n",
    "from nltk.corpus import treebank\n",
    "from tqdm import tqdm\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n",
      "[0, 11300, 468, 4291, 225, 2156, 5659, 107, 793, 2156, 40, 1962, 5, 792, 25, 10, 47554, 3204, 19172, 736, 1442, 4, 1132, 479, 2]\n",
      "['<s>', 'ĠPierre', 'ĠV', 'ink', 'en', 'Ġ,', 'Ġ61', 'Ġyears', 'Ġold', 'Ġ,', 'Ġwill', 'Ġjoin', 'Ġthe', 'Ġboard', 'Ġas', 'Ġa', 'Ġnonex', 'ec', 'utive', 'Ġdirector', 'ĠNov', '.', 'Ġ29', 'Ġ.', '</s>']\n",
      "[0, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 13, 14, 15, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = [elem[0] for elem in corpus[0]]\n",
    "labels = [elem[1] for elem in corpus[0]]\n",
    "print(test_sentence)\n",
    "print(labels)\n",
    "encoding = tokenizer.encode_plus(test_sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "subword_tokens = encoding.tokens()\n",
    "word_ids = encoding.word_ids()[1:-1]\n",
    "\n",
    "print(encoding['input_ids'])\n",
    "print(subword_tokens)\n",
    "print(word_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'NNP', 'NNP', 'X', 'X', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'X', 'X', 'NN', 'NNP', 'X', 'CD', '.', -100]\n"
     ]
    }
   ],
   "source": [
    "labels_subword = [-100]\n",
    "for i in range(len(word_ids)):\n",
    "    if word_ids[i] != word_ids[i-1]:\n",
    "        labels_subword.append(labels[word_ids[i]])\n",
    "    else:\n",
    "        labels_subword.append('X')    \n",
    "\n",
    "labels_subword.append(-100)\n",
    "print(labels_subword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  3914\n",
      "Longest sentence length: 271\n"
     ]
    }
   ],
   "source": [
    "# get the POS tagged corpus, 3914 tagged sentences\n",
    "corpus = treebank.tagged_sents()\n",
    "print(\"Number of sentences: \", len(corpus))\n",
    "print(f\"Longest sentence length: {max([len(s) for s in corpus])}\")\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Treebank(Dataset):\n",
    "    def __init__(self, corpus):\n",
    "        # get the sentences and labels\n",
    "        self.sentences = [[elem[0] for elem in s] for s in corpus]\n",
    "        self.pos_labels = [[elem[1] for elem in s] for s in corpus]\n",
    "        # define special tag\n",
    "        self.continuation_tag = \"X\"\n",
    "        # get tag set\n",
    "        self.tags = sorted([self.continuation_tag] + list(set([elem[1] for s in corpus for elem in s])))\n",
    "        # get tag set\n",
    "        tags = sorted(list(set([elem[1] for s in corpus for elem in s])))\n",
    "        # tag to idx mapping\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.tags)}\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get sentence and labels\n",
    "        sentence = self.sentences[idx]\n",
    "        labels = self.pos_labels[idx]\n",
    "        # tokenize the sentence\n",
    "        input_encoding = self.tokenizer.encode_plus(test_sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "        input_idx = input_encoding['input_ids']\n",
    "        # assign labels to subword tokens\n",
    "        labels_subword = [-100]\n",
    "        for i in range(len(word_ids)):\n",
    "            if word_ids[i] != word_ids[i-1]:\n",
    "                labels_subword.append(self.tag2idx[labels[word_ids[i]]])\n",
    "            else:\n",
    "                labels_subword.append(self.tag2idx[\"X\"])    \n",
    "\n",
    "        labels_subword.append(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
