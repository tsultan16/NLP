{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN-based Sequence Labeller\n",
    "\n",
    "We've seen how to implement an HMM bigram POS tagger and saw that it can achieve over 90% test accuracy on the Stanford treebank. The main limitation of this model is the limited context size (bigram context) which is used for making the sequence label predictions. Naively extending the algorithm to handle larger n-gram contexts may result in exponential increase in memory consumption (for vocab size $|V|$, the number of possible n-grams is on the order of $|V|^n$). A more natural way to handle larger contexts is using a `recurrent neural network (RNN)` to perform the sequence labelling task, where we use a sequence of pre-trained word embeddings as input and predict POS tags corrresponding to each word. We will experiment with some simple RNN architectures for this task and compare performance with the unigram and Viterbi bigram taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first download pretrained GloVe embeddings (api.load() returns a 'KeyedVectors' object)\n",
    "# we're getting the 50 dimensional words vectors, 400k vocab size\n",
    "embeddings = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  3914\n",
      "Longest sentence length: 271\n",
      "Vocab size: 12409\n",
      "['<s>', '#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get the POS tagged corpus, 3914 tagged sentences\n",
    "corpus = treebank.tagged_sents()\n",
    "print(\"Number of sentences: \", len(corpus))\n",
    "print(f\"Longest sentence length: {max([len(s) for s in corpus])}\")\n",
    "\n",
    "\n",
    "# lets get the vocabulary and tag set\n",
    "pad_token = \"<PAD>\"\n",
    "vocab = [pad_token] + sorted(list(set([elem[0] for s in corpus for elem in s])))\n",
    "vocab_size = len(vocab)\n",
    "start_tag = \"<s>\"\n",
    "tags = [start_tag] + sorted(list(set([elem[1] for s in corpus for elem in s])))\n",
    "num_tags = len(tags)\n",
    "\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "tag2idx = {t:i for i,t in enumerate(tags)}\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words for which embeddings not available: 0\n"
     ]
    }
   ],
   "source": [
    "# now lets get the embeddings for all words in our vocab\n",
    "embedding_dim = 50\n",
    "embedding_vectors = np.zeros(shape=(vocab_size,embedding_dim))\n",
    "\n",
    "oov_words = []\n",
    "for i, word in enumerate(vocab):\n",
    "    if word.lower() in embeddings.key_to_index:\n",
    "        embedding_vectors[i] = embeddings[word.lower()]\n",
    "    else:\n",
    "        #print(f\"'{word.lower()}' not in GloVe vocab!\")    \n",
    "        # if the word is hyphenated, then split it and see if the sub-words have embeddings\n",
    "        if \"-\" in word:\n",
    "            split_hyphen = word.lower().split(\"-\")\n",
    "            # compute average word embedding across split words\n",
    "            emb = np.zeros(shape=(embedding_dim))\n",
    "            found = False\n",
    "            for i, w in enumerate(split_hyphen):\n",
    "                if w in embeddings.key_to_index:\n",
    "                    emb += embeddings[w]\n",
    "                    found = True\n",
    "            if found:        \n",
    "                emb += (len(split_hyphen)-1) * embeddings[\"-\"]\n",
    "                emb = emb / (2*len(split_hyphen)-1)\n",
    "                embedding_vectors[i] = emb   \n",
    "            else:\n",
    "                oov_words.append(word)     \n",
    "\n",
    "print(f\"Number of words for which embeddings not available: {len(oov_words)}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences and tags to get the inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[word2idx[word] for word,tag in s] for s in corpus]\n",
    "y = [[tag2idx[tag] for word,tag in s] for s in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest train sentence length: 271\n",
      "Longest val sentence length: 58\n"
     ]
    }
   ],
   "source": [
    "num_train = int(0.9 * len(x))\n",
    "x_train, y_train = x[:num_train], y[:num_train]\n",
    "x_val, y_val = x[num_train:], y[num_train:]\n",
    "\n",
    "print(f\"Longest train sentence length: {max([len(s) for s in x_train])}\")\n",
    "print(f\"Longest val sentence length: {max([len(s) for s in x_val])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Treebank(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = pad_sequence([torch.tensor(x, dtype=torch.long) for x in inputs], batch_first=True, padding_value=0)\n",
    "        self.targets = pad_sequence([torch.tensor(y, dtype=torch.long) for y in targets], batch_first=True, padding_value=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Treebank(x_train, y_train)\n",
    "val_dataset = Treebank(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create our RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_tags, embedding_dims, pretrained_embeddings, num_rnn_layers=1, hidden_dims=64, dropout_rate=0.1, padding_idx=-1):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dims)        \n",
    "        # intialize with pretrained embedding vectors\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        # freeze the embedding layer (i.e. make non-trainable)\n",
    "        self.emb.weight.requires_grad = False\n",
    "        # create rnn layers (we will use bidirectional LSTM so the output hidden states will have dims=2*hidden_dims)\n",
    "        self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dims, num_layers=num_rnn_layers, bidirectional=True, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(2*hidden_dims, num_tags)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # compute rnn hidden states\n",
    "        x, _ = self.rnn_layers(x) # shape: (B,L,2*H)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,num_tags)\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,num_tags)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y, ignore_index=self.padding_idx)\n",
    "\n",
    "        return x, loss\n",
    "        \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            \n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "        train_acc = num_correct / num_total        \n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "\n",
    "        #if epoch % 5 == 0:\n",
    "        #    save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'rnntagger_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('rnntagger_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 0.685905 M\n",
      "RAM used: 1248.61 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "H = 64\n",
    "num_rnn_layers = 1\n",
    "learning_rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = RNNTagger(vocab_size, num_tags, embedding_dim, embedding_vectors, hidden_dims=H, num_rnn_layers=num_rnn_layers).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 3.611, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 28/28 [00:00<00:00, 56.83it/s]\n",
      "Epoch 2, EMA Train Loss: 3.724, Train Accuracy:  0.049, Val Loss:  3.790, Val Accuracy:  0.108: 100%|██████████| 28/28 [00:00<00:00, 97.72it/s] \n",
      "Epoch 3, EMA Train Loss: 3.625, Train Accuracy:  0.113, Val Loss:  3.699, Val Accuracy:  0.117: 100%|██████████| 28/28 [00:00<00:00, 104.25it/s]\n",
      "Epoch 4, EMA Train Loss: 3.454, Train Accuracy:  0.119, Val Loss:  3.578, Val Accuracy:  0.117: 100%|██████████| 28/28 [00:00<00:00, 117.22it/s]\n",
      "Epoch 5, EMA Train Loss: 3.235, Train Accuracy:  0.120, Val Loss:  3.364, Val Accuracy:  0.119: 100%|██████████| 28/28 [00:00<00:00, 115.50it/s]\n",
      "Epoch 6, EMA Train Loss: 3.094, Train Accuracy:  0.128, Val Loss:  3.154, Val Accuracy:  0.142: 100%|██████████| 28/28 [00:00<00:00, 117.83it/s]\n",
      "Epoch 7, EMA Train Loss: 3.022, Train Accuracy:  0.153, Val Loss:  2.991, Val Accuracy:  0.199: 100%|██████████| 28/28 [00:00<00:00, 117.52it/s]\n",
      "Epoch 8, EMA Train Loss: 2.971, Train Accuracy:  0.172, Val Loss:  2.942, Val Accuracy:  0.191: 100%|██████████| 28/28 [00:00<00:00, 112.19it/s]\n",
      "Epoch 9, EMA Train Loss: 2.932, Train Accuracy:  0.174, Val Loss:  2.929, Val Accuracy:  0.196: 100%|██████████| 28/28 [00:00<00:00, 116.78it/s]\n",
      "Epoch 10, EMA Train Loss: 2.894, Train Accuracy:  0.184, Val Loss:  2.864, Val Accuracy:  0.214: 100%|██████████| 28/28 [00:00<00:00, 116.82it/s]\n",
      "Epoch 11, EMA Train Loss: 2.841, Train Accuracy:  0.199, Val Loss:  2.803, Val Accuracy:  0.235: 100%|██████████| 28/28 [00:00<00:00, 111.22it/s]\n",
      "Epoch 12, EMA Train Loss: 2.793, Train Accuracy:  0.219, Val Loss:  2.772, Val Accuracy:  0.254: 100%|██████████| 28/28 [00:00<00:00, 115.68it/s]\n",
      "Epoch 13, EMA Train Loss: 2.743, Train Accuracy:  0.238, Val Loss:  2.708, Val Accuracy:  0.286: 100%|██████████| 28/28 [00:00<00:00, 117.13it/s]\n",
      "Epoch 14, EMA Train Loss: 2.698, Train Accuracy:  0.265, Val Loss:  2.674, Val Accuracy:  0.318: 100%|██████████| 28/28 [00:00<00:00, 116.39it/s]\n",
      "Epoch 15, EMA Train Loss: 2.648, Train Accuracy:  0.290, Val Loss:  2.615, Val Accuracy:  0.344: 100%|██████████| 28/28 [00:00<00:00, 113.80it/s]\n",
      "Epoch 16, EMA Train Loss: 2.592, Train Accuracy:  0.313, Val Loss:  2.590, Val Accuracy:  0.362: 100%|██████████| 28/28 [00:00<00:00, 117.15it/s]\n",
      "Epoch 17, EMA Train Loss: 2.543, Train Accuracy:  0.333, Val Loss:  2.527, Val Accuracy:  0.376: 100%|██████████| 28/28 [00:00<00:00, 115.29it/s]\n",
      "Epoch 18, EMA Train Loss: 2.482, Train Accuracy:  0.356, Val Loss:  2.482, Val Accuracy:  0.393: 100%|██████████| 28/28 [00:00<00:00, 112.31it/s]\n",
      "Epoch 19, EMA Train Loss: 2.431, Train Accuracy:  0.372, Val Loss:  2.435, Val Accuracy:  0.413: 100%|██████████| 28/28 [00:00<00:00, 114.35it/s]\n",
      "Epoch 20, EMA Train Loss: 2.376, Train Accuracy:  0.389, Val Loss:  2.363, Val Accuracy:  0.430: 100%|██████████| 28/28 [00:00<00:00, 99.75it/s] \n",
      "Epoch 21, EMA Train Loss: 2.333, Train Accuracy:  0.409, Val Loss:  2.313, Val Accuracy:  0.450: 100%|██████████| 28/28 [00:00<00:00, 111.94it/s]\n",
      "Epoch 22, EMA Train Loss: 2.275, Train Accuracy:  0.426, Val Loss:  2.262, Val Accuracy:  0.467: 100%|██████████| 28/28 [00:00<00:00, 106.57it/s]\n",
      "Epoch 23, EMA Train Loss: 2.223, Train Accuracy:  0.445, Val Loss:  2.183, Val Accuracy:  0.479: 100%|██████████| 28/28 [00:00<00:00, 101.80it/s]\n",
      "Epoch 24, EMA Train Loss: 2.166, Train Accuracy:  0.460, Val Loss:  2.160, Val Accuracy:  0.497: 100%|██████████| 28/28 [00:00<00:00, 103.86it/s]\n",
      "Epoch 25, EMA Train Loss: 2.121, Train Accuracy:  0.474, Val Loss:  2.099, Val Accuracy:  0.508: 100%|██████████| 28/28 [00:00<00:00, 113.64it/s]\n",
      "Epoch 26, EMA Train Loss: 2.078, Train Accuracy:  0.488, Val Loss:  2.050, Val Accuracy:  0.520: 100%|██████████| 28/28 [00:00<00:00, 110.20it/s]\n",
      "Epoch 27, EMA Train Loss: 2.023, Train Accuracy:  0.499, Val Loss:  2.020, Val Accuracy:  0.528: 100%|██████████| 28/28 [00:00<00:00, 110.66it/s]\n",
      "Epoch 28, EMA Train Loss: 1.981, Train Accuracy:  0.510, Val Loss:  1.955, Val Accuracy:  0.539: 100%|██████████| 28/28 [00:00<00:00, 114.16it/s]\n",
      "Epoch 29, EMA Train Loss: 1.937, Train Accuracy:  0.519, Val Loss:  1.897, Val Accuracy:  0.547: 100%|██████████| 28/28 [00:00<00:00, 114.98it/s]\n",
      "Epoch 30, EMA Train Loss: 1.890, Train Accuracy:  0.528, Val Loss:  1.883, Val Accuracy:  0.553: 100%|██████████| 28/28 [00:00<00:00, 114.03it/s]\n",
      "Epoch 31, EMA Train Loss: 1.850, Train Accuracy:  0.536, Val Loss:  1.840, Val Accuracy:  0.561: 100%|██████████| 28/28 [00:00<00:00, 114.55it/s]\n",
      "Epoch 32, EMA Train Loss: 1.817, Train Accuracy:  0.542, Val Loss:  1.766, Val Accuracy:  0.568: 100%|██████████| 28/28 [00:00<00:00, 111.83it/s]\n",
      "Epoch 33, EMA Train Loss: 1.778, Train Accuracy:  0.550, Val Loss:  1.718, Val Accuracy:  0.574: 100%|██████████| 28/28 [00:00<00:00, 111.28it/s]\n",
      "Epoch 34, EMA Train Loss: 1.744, Train Accuracy:  0.557, Val Loss:  1.697, Val Accuracy:  0.578: 100%|██████████| 28/28 [00:00<00:00, 107.33it/s]\n",
      "Epoch 35, EMA Train Loss: 1.702, Train Accuracy:  0.563, Val Loss:  1.654, Val Accuracy:  0.585: 100%|██████████| 28/28 [00:00<00:00, 106.20it/s]\n",
      "Epoch 36, EMA Train Loss: 1.668, Train Accuracy:  0.571, Val Loss:  1.670, Val Accuracy:  0.590: 100%|██████████| 28/28 [00:00<00:00, 111.97it/s]\n",
      "Epoch 37, EMA Train Loss: 1.640, Train Accuracy:  0.578, Val Loss:  1.615, Val Accuracy:  0.596: 100%|██████████| 28/28 [00:00<00:00, 114.38it/s]\n",
      "Epoch 38, EMA Train Loss: 1.610, Train Accuracy:  0.584, Val Loss:  1.599, Val Accuracy:  0.602: 100%|██████████| 28/28 [00:00<00:00, 110.63it/s]\n",
      "Epoch 39, EMA Train Loss: 1.581, Train Accuracy:  0.591, Val Loss:  1.510, Val Accuracy:  0.609: 100%|██████████| 28/28 [00:00<00:00, 114.49it/s]\n",
      "Epoch 40, EMA Train Loss: 1.555, Train Accuracy:  0.597, Val Loss:  1.480, Val Accuracy:  0.615: 100%|██████████| 28/28 [00:00<00:00, 114.79it/s]\n",
      "Epoch 41, EMA Train Loss: 1.525, Train Accuracy:  0.603, Val Loss:  1.489, Val Accuracy:  0.622: 100%|██████████| 28/28 [00:00<00:00, 115.05it/s]\n",
      "Epoch 42, EMA Train Loss: 1.497, Train Accuracy:  0.609, Val Loss:  1.456, Val Accuracy:  0.628: 100%|██████████| 28/28 [00:00<00:00, 110.88it/s]\n",
      "Epoch 43, EMA Train Loss: 1.470, Train Accuracy:  0.615, Val Loss:  1.463, Val Accuracy:  0.632: 100%|██████████| 28/28 [00:00<00:00, 113.94it/s]\n",
      "Epoch 44, EMA Train Loss: 1.447, Train Accuracy:  0.621, Val Loss:  1.491, Val Accuracy:  0.638: 100%|██████████| 28/28 [00:00<00:00, 113.33it/s]\n",
      "Epoch 45, EMA Train Loss: 1.429, Train Accuracy:  0.628, Val Loss:  1.411, Val Accuracy:  0.643: 100%|██████████| 28/28 [00:00<00:00, 112.77it/s]\n",
      "Epoch 46, EMA Train Loss: 1.398, Train Accuracy:  0.634, Val Loss:  1.367, Val Accuracy:  0.650: 100%|██████████| 28/28 [00:00<00:00, 113.36it/s]\n",
      "Epoch 47, EMA Train Loss: 1.386, Train Accuracy:  0.641, Val Loss:  1.386, Val Accuracy:  0.658: 100%|██████████| 28/28 [00:00<00:00, 113.41it/s]\n",
      "Epoch 48, EMA Train Loss: 1.360, Train Accuracy:  0.646, Val Loss:  1.324, Val Accuracy:  0.664: 100%|██████████| 28/28 [00:00<00:00, 109.23it/s]\n",
      "Epoch 49, EMA Train Loss: 1.340, Train Accuracy:  0.652, Val Loss:  1.339, Val Accuracy:  0.669: 100%|██████████| 28/28 [00:00<00:00, 114.87it/s]\n",
      "Epoch 50, EMA Train Loss: 1.321, Train Accuracy:  0.658, Val Loss:  1.254, Val Accuracy:  0.676: 100%|██████████| 28/28 [00:00<00:00, 114.06it/s]\n",
      "Epoch 51, EMA Train Loss: 1.302, Train Accuracy:  0.665, Val Loss:  1.282, Val Accuracy:  0.681: 100%|██████████| 28/28 [00:00<00:00, 113.91it/s]\n",
      "Epoch 52, EMA Train Loss: 1.287, Train Accuracy:  0.669, Val Loss:  1.212, Val Accuracy:  0.686: 100%|██████████| 28/28 [00:00<00:00, 114.39it/s]\n",
      "Epoch 53, EMA Train Loss: 1.266, Train Accuracy:  0.675, Val Loss:  1.218, Val Accuracy:  0.692: 100%|██████████| 28/28 [00:00<00:00, 112.48it/s]\n",
      "Epoch 54, EMA Train Loss: 1.244, Train Accuracy:  0.680, Val Loss:  1.206, Val Accuracy:  0.695: 100%|██████████| 28/28 [00:00<00:00, 114.05it/s]\n",
      "Epoch 55, EMA Train Loss: 1.240, Train Accuracy:  0.684, Val Loss:  1.155, Val Accuracy:  0.700: 100%|██████████| 28/28 [00:00<00:00, 114.33it/s]\n",
      "Epoch 56, EMA Train Loss: 1.214, Train Accuracy:  0.686, Val Loss:  1.194, Val Accuracy:  0.705: 100%|██████████| 28/28 [00:00<00:00, 111.78it/s]\n",
      "Epoch 57, EMA Train Loss: 1.198, Train Accuracy:  0.693, Val Loss:  1.144, Val Accuracy:  0.708: 100%|██████████| 28/28 [00:00<00:00, 114.79it/s]\n",
      "Epoch 58, EMA Train Loss: 1.176, Train Accuracy:  0.695, Val Loss:  1.159, Val Accuracy:  0.711: 100%|██████████| 28/28 [00:00<00:00, 107.51it/s]\n",
      "Epoch 59, EMA Train Loss: 1.172, Train Accuracy:  0.699, Val Loss:  1.155, Val Accuracy:  0.715: 100%|██████████| 28/28 [00:00<00:00, 113.03it/s]\n",
      "Epoch 60, EMA Train Loss: 1.157, Train Accuracy:  0.701, Val Loss:  1.125, Val Accuracy:  0.716: 100%|██████████| 28/28 [00:00<00:00, 110.21it/s]\n",
      "Epoch 61, EMA Train Loss: 1.142, Train Accuracy:  0.707, Val Loss:  1.143, Val Accuracy:  0.720: 100%|██████████| 28/28 [00:00<00:00, 115.44it/s]\n",
      "Epoch 62, EMA Train Loss: 1.127, Train Accuracy:  0.709, Val Loss:  1.066, Val Accuracy:  0.723: 100%|██████████| 28/28 [00:00<00:00, 113.94it/s]\n",
      "Epoch 63, EMA Train Loss: 1.114, Train Accuracy:  0.712, Val Loss:  1.071, Val Accuracy:  0.725: 100%|██████████| 28/28 [00:00<00:00, 111.74it/s]\n",
      "Epoch 64, EMA Train Loss: 1.092, Train Accuracy:  0.716, Val Loss:  1.076, Val Accuracy:  0.727: 100%|██████████| 28/28 [00:00<00:00, 114.40it/s]\n",
      "Epoch 65, EMA Train Loss: 1.084, Train Accuracy:  0.718, Val Loss:  1.020, Val Accuracy:  0.730: 100%|██████████| 28/28 [00:00<00:00, 111.10it/s]\n",
      "Epoch 66, EMA Train Loss: 1.076, Train Accuracy:  0.721, Val Loss:  1.056, Val Accuracy:  0.731: 100%|██████████| 28/28 [00:00<00:00, 109.07it/s]\n",
      "Epoch 67, EMA Train Loss: 1.061, Train Accuracy:  0.723, Val Loss:  1.013, Val Accuracy:  0.734: 100%|██████████| 28/28 [00:00<00:00, 115.75it/s]\n",
      "Epoch 68, EMA Train Loss: 1.064, Train Accuracy:  0.725, Val Loss:  1.030, Val Accuracy:  0.736: 100%|██████████| 28/28 [00:00<00:00, 114.18it/s]\n",
      "Epoch 69, EMA Train Loss: 1.044, Train Accuracy:  0.728, Val Loss:  0.995, Val Accuracy:  0.740: 100%|██████████| 28/28 [00:00<00:00, 112.15it/s]\n",
      "Epoch 70, EMA Train Loss: 1.033, Train Accuracy:  0.730, Val Loss:  0.970, Val Accuracy:  0.742: 100%|██████████| 28/28 [00:00<00:00, 111.94it/s]\n",
      "Epoch 71, EMA Train Loss: 1.019, Train Accuracy:  0.733, Val Loss:  0.945, Val Accuracy:  0.744: 100%|██████████| 28/28 [00:00<00:00, 109.55it/s]\n",
      "Epoch 72, EMA Train Loss: 1.024, Train Accuracy:  0.735, Val Loss:  0.971, Val Accuracy:  0.747: 100%|██████████| 28/28 [00:00<00:00, 114.04it/s]\n",
      "Epoch 73, EMA Train Loss: 1.002, Train Accuracy:  0.738, Val Loss:  0.987, Val Accuracy:  0.748: 100%|██████████| 28/28 [00:00<00:00, 114.46it/s]\n",
      "Epoch 74, EMA Train Loss: 1.001, Train Accuracy:  0.739, Val Loss:  0.969, Val Accuracy:  0.751: 100%|██████████| 28/28 [00:00<00:00, 113.40it/s]\n",
      "Epoch 75, EMA Train Loss: 0.979, Train Accuracy:  0.740, Val Loss:  0.964, Val Accuracy:  0.752: 100%|██████████| 28/28 [00:00<00:00, 113.90it/s]\n",
      "Epoch 76, EMA Train Loss: 0.976, Train Accuracy:  0.743, Val Loss:  0.942, Val Accuracy:  0.754: 100%|██████████| 28/28 [00:00<00:00, 106.55it/s]\n",
      "Epoch 77, EMA Train Loss: 0.966, Train Accuracy:  0.745, Val Loss:  0.944, Val Accuracy:  0.756: 100%|██████████| 28/28 [00:00<00:00, 110.43it/s]\n",
      "Epoch 78, EMA Train Loss: 0.962, Train Accuracy:  0.747, Val Loss:  0.906, Val Accuracy:  0.758: 100%|██████████| 28/28 [00:00<00:00, 113.84it/s]\n",
      "Epoch 79, EMA Train Loss: 0.950, Train Accuracy:  0.748, Val Loss:  0.941, Val Accuracy:  0.759: 100%|██████████| 28/28 [00:00<00:00, 114.59it/s]\n",
      "Epoch 80, EMA Train Loss: 0.935, Train Accuracy:  0.751, Val Loss:  0.879, Val Accuracy:  0.762: 100%|██████████| 28/28 [00:00<00:00, 113.67it/s]\n",
      "Epoch 81, EMA Train Loss: 0.930, Train Accuracy:  0.752, Val Loss:  0.890, Val Accuracy:  0.764: 100%|██████████| 28/28 [00:00<00:00, 111.81it/s]\n",
      "Epoch 82, EMA Train Loss: 0.929, Train Accuracy:  0.754, Val Loss:  0.933, Val Accuracy:  0.767: 100%|██████████| 28/28 [00:00<00:00, 114.26it/s]\n",
      "Epoch 83, EMA Train Loss: 0.927, Train Accuracy:  0.756, Val Loss:  0.866, Val Accuracy:  0.769: 100%|██████████| 28/28 [00:00<00:00, 100.24it/s]\n",
      "Epoch 84, EMA Train Loss: 0.903, Train Accuracy:  0.757, Val Loss:  0.858, Val Accuracy:  0.770: 100%|██████████| 28/28 [00:00<00:00, 112.44it/s]\n",
      "Epoch 85, EMA Train Loss: 0.901, Train Accuracy:  0.759, Val Loss:  0.861, Val Accuracy:  0.772: 100%|██████████| 28/28 [00:00<00:00, 112.11it/s]\n",
      "Epoch 86, EMA Train Loss: 0.899, Train Accuracy:  0.761, Val Loss:  0.862, Val Accuracy:  0.774: 100%|██████████| 28/28 [00:00<00:00, 109.87it/s]\n",
      "Epoch 87, EMA Train Loss: 0.887, Train Accuracy:  0.762, Val Loss:  0.868, Val Accuracy:  0.775: 100%|██████████| 28/28 [00:00<00:00, 111.72it/s]\n",
      "Epoch 88, EMA Train Loss: 0.884, Train Accuracy:  0.764, Val Loss:  0.876, Val Accuracy:  0.776: 100%|██████████| 28/28 [00:00<00:00, 113.31it/s]\n",
      "Epoch 89, EMA Train Loss: 0.872, Train Accuracy:  0.765, Val Loss:  0.833, Val Accuracy:  0.776: 100%|██████████| 28/28 [00:00<00:00, 112.94it/s]\n",
      "Epoch 90, EMA Train Loss: 0.862, Train Accuracy:  0.767, Val Loss:  0.821, Val Accuracy:  0.777: 100%|██████████| 28/28 [00:00<00:00, 109.84it/s]\n",
      "Epoch 91, EMA Train Loss: 0.864, Train Accuracy:  0.768, Val Loss:  0.836, Val Accuracy:  0.778: 100%|██████████| 28/28 [00:00<00:00, 114.00it/s]\n",
      "Epoch 92, EMA Train Loss: 0.856, Train Accuracy:  0.769, Val Loss:  0.848, Val Accuracy:  0.780: 100%|██████████| 28/28 [00:00<00:00, 113.04it/s]\n",
      "Epoch 93, EMA Train Loss: 0.846, Train Accuracy:  0.771, Val Loss:  0.838, Val Accuracy:  0.781: 100%|██████████| 28/28 [00:00<00:00, 113.07it/s]\n",
      "Epoch 94, EMA Train Loss: 0.848, Train Accuracy:  0.772, Val Loss:  0.827, Val Accuracy:  0.782: 100%|██████████| 28/28 [00:00<00:00, 112.45it/s]\n",
      "Epoch 95, EMA Train Loss: 0.835, Train Accuracy:  0.774, Val Loss:  0.802, Val Accuracy:  0.784: 100%|██████████| 28/28 [00:00<00:00, 107.79it/s]\n",
      "Epoch 96, EMA Train Loss: 0.832, Train Accuracy:  0.775, Val Loss:  0.799, Val Accuracy:  0.785: 100%|██████████| 28/28 [00:00<00:00, 112.68it/s]\n",
      "Epoch 97, EMA Train Loss: 0.823, Train Accuracy:  0.776, Val Loss:  0.774, Val Accuracy:  0.787: 100%|██████████| 28/28 [00:00<00:00, 107.22it/s]\n",
      "Epoch 98, EMA Train Loss: 0.817, Train Accuracy:  0.778, Val Loss:  0.778, Val Accuracy:  0.789: 100%|██████████| 28/28 [00:00<00:00, 112.44it/s]\n",
      "Epoch 99, EMA Train Loss: 0.813, Train Accuracy:  0.780, Val Loss:  0.803, Val Accuracy:  0.790: 100%|██████████| 28/28 [00:00<00:00, 107.66it/s]\n",
      "Epoch 100, EMA Train Loss: 0.807, Train Accuracy:  0.781, Val Loss:  0.766, Val Accuracy:  0.791: 100%|██████████| 28/28 [00:00<00:00, 114.29it/s]\n",
      "Epoch 101, EMA Train Loss: 0.803, Train Accuracy:  0.783, Val Loss:  0.805, Val Accuracy:  0.793: 100%|██████████| 28/28 [00:00<00:00, 114.93it/s]\n",
      "Epoch 102, EMA Train Loss: 0.795, Train Accuracy:  0.783, Val Loss:  0.800, Val Accuracy:  0.794: 100%|██████████| 28/28 [00:00<00:00, 111.51it/s]\n",
      "Epoch 103, EMA Train Loss: 0.782, Train Accuracy:  0.784, Val Loss:  0.734, Val Accuracy:  0.796: 100%|██████████| 28/28 [00:00<00:00, 109.27it/s]\n",
      "Epoch 104, EMA Train Loss: 0.788, Train Accuracy:  0.787, Val Loss:  0.761, Val Accuracy:  0.796: 100%|██████████| 28/28 [00:00<00:00, 113.72it/s]\n",
      "Epoch 105, EMA Train Loss: 0.775, Train Accuracy:  0.788, Val Loss:  0.757, Val Accuracy:  0.798: 100%|██████████| 28/28 [00:00<00:00, 110.03it/s]\n",
      "Epoch 106, EMA Train Loss: 0.776, Train Accuracy:  0.790, Val Loss:  0.743, Val Accuracy:  0.799: 100%|██████████| 28/28 [00:00<00:00, 112.59it/s]\n",
      "Epoch 107, EMA Train Loss: 0.767, Train Accuracy:  0.790, Val Loss:  0.729, Val Accuracy:  0.800: 100%|██████████| 28/28 [00:00<00:00, 107.98it/s]\n",
      "Epoch 108, EMA Train Loss: 0.760, Train Accuracy:  0.791, Val Loss:  0.726, Val Accuracy:  0.802: 100%|██████████| 28/28 [00:00<00:00, 115.14it/s]\n",
      "Epoch 109, EMA Train Loss: 0.764, Train Accuracy:  0.793, Val Loss:  0.742, Val Accuracy:  0.803: 100%|██████████| 28/28 [00:00<00:00, 114.66it/s]\n",
      "Epoch 110, EMA Train Loss: 0.762, Train Accuracy:  0.793, Val Loss:  0.699, Val Accuracy:  0.805: 100%|██████████| 28/28 [00:00<00:00, 110.42it/s]\n",
      "Epoch 111, EMA Train Loss: 0.756, Train Accuracy:  0.794, Val Loss:  0.762, Val Accuracy:  0.806: 100%|██████████| 28/28 [00:00<00:00, 113.58it/s]\n",
      "Epoch 112, EMA Train Loss: 0.752, Train Accuracy:  0.796, Val Loss:  0.716, Val Accuracy:  0.807: 100%|██████████| 28/28 [00:00<00:00, 113.68it/s]\n",
      "Epoch 113, EMA Train Loss: 0.742, Train Accuracy:  0.797, Val Loss:  0.739, Val Accuracy:  0.808: 100%|██████████| 28/28 [00:00<00:00, 113.04it/s]\n",
      "Epoch 114, EMA Train Loss: 0.736, Train Accuracy:  0.798, Val Loss:  0.737, Val Accuracy:  0.808: 100%|██████████| 28/28 [00:00<00:00, 107.00it/s]\n",
      "Epoch 115, EMA Train Loss: 0.730, Train Accuracy:  0.801, Val Loss:  0.685, Val Accuracy:  0.809: 100%|██████████| 28/28 [00:00<00:00, 113.38it/s]\n",
      "Epoch 116, EMA Train Loss: 0.732, Train Accuracy:  0.800, Val Loss:  0.736, Val Accuracy:  0.810: 100%|██████████| 28/28 [00:00<00:00, 111.05it/s]\n",
      "Epoch 117, EMA Train Loss: 0.723, Train Accuracy:  0.801, Val Loss:  0.730, Val Accuracy:  0.810: 100%|██████████| 28/28 [00:00<00:00, 106.49it/s]\n",
      "Epoch 118, EMA Train Loss: 0.721, Train Accuracy:  0.802, Val Loss:  0.692, Val Accuracy:  0.811: 100%|██████████| 28/28 [00:00<00:00, 110.87it/s]\n",
      "Epoch 119, EMA Train Loss: 0.725, Train Accuracy:  0.803, Val Loss:  0.705, Val Accuracy:  0.813: 100%|██████████| 28/28 [00:00<00:00, 110.85it/s]\n",
      "Epoch 120, EMA Train Loss: 0.718, Train Accuracy:  0.804, Val Loss:  0.700, Val Accuracy:  0.813: 100%|██████████| 28/28 [00:00<00:00, 114.05it/s]\n",
      "Epoch 121, EMA Train Loss: 0.714, Train Accuracy:  0.804, Val Loss:  0.663, Val Accuracy:  0.814: 100%|██████████| 28/28 [00:00<00:00, 114.07it/s]\n",
      "Epoch 122, EMA Train Loss: 0.706, Train Accuracy:  0.805, Val Loss:  0.737, Val Accuracy:  0.816: 100%|██████████| 28/28 [00:00<00:00, 108.27it/s]\n",
      "Epoch 123, EMA Train Loss: 0.710, Train Accuracy:  0.806, Val Loss:  0.663, Val Accuracy:  0.816: 100%|██████████| 28/28 [00:00<00:00, 110.86it/s]\n",
      "Epoch 124, EMA Train Loss: 0.699, Train Accuracy:  0.808, Val Loss:  0.714, Val Accuracy:  0.818: 100%|██████████| 28/28 [00:00<00:00, 112.31it/s]\n",
      "Epoch 125, EMA Train Loss: 0.693, Train Accuracy:  0.808, Val Loss:  0.664, Val Accuracy:  0.818: 100%|██████████| 28/28 [00:00<00:00, 108.92it/s]\n",
      "Epoch 126, EMA Train Loss: 0.692, Train Accuracy:  0.809, Val Loss:  0.696, Val Accuracy:  0.818: 100%|██████████| 28/28 [00:00<00:00, 106.08it/s]\n",
      "Epoch 127, EMA Train Loss: 0.684, Train Accuracy:  0.809, Val Loss:  0.654, Val Accuracy:  0.819: 100%|██████████| 28/28 [00:00<00:00, 113.88it/s]\n",
      "Epoch 128, EMA Train Loss: 0.685, Train Accuracy:  0.810, Val Loss:  0.670, Val Accuracy:  0.820: 100%|██████████| 28/28 [00:00<00:00, 110.77it/s]\n",
      "Epoch 129, EMA Train Loss: 0.678, Train Accuracy:  0.812, Val Loss:  0.664, Val Accuracy:  0.820: 100%|██████████| 28/28 [00:00<00:00, 108.68it/s]\n",
      "Epoch 130, EMA Train Loss: 0.675, Train Accuracy:  0.813, Val Loss:  0.692, Val Accuracy:  0.820: 100%|██████████| 28/28 [00:00<00:00, 112.97it/s]\n",
      "Epoch 131, EMA Train Loss: 0.676, Train Accuracy:  0.814, Val Loss:  0.685, Val Accuracy:  0.821: 100%|██████████| 28/28 [00:00<00:00, 113.08it/s]\n",
      "Epoch 132, EMA Train Loss: 0.670, Train Accuracy:  0.816, Val Loss:  0.651, Val Accuracy:  0.821: 100%|██████████| 28/28 [00:00<00:00, 108.43it/s]\n",
      "Epoch 133, EMA Train Loss: 0.671, Train Accuracy:  0.814, Val Loss:  0.646, Val Accuracy:  0.822: 100%|██████████| 28/28 [00:00<00:00, 112.28it/s]\n",
      "Epoch 134, EMA Train Loss: 0.662, Train Accuracy:  0.815, Val Loss:  0.645, Val Accuracy:  0.824: 100%|██████████| 28/28 [00:00<00:00, 112.74it/s]\n",
      "Epoch 135, EMA Train Loss: 0.665, Train Accuracy:  0.816, Val Loss:  0.592, Val Accuracy:  0.823: 100%|██████████| 28/28 [00:00<00:00, 111.30it/s]\n",
      "Epoch 136, EMA Train Loss: 0.660, Train Accuracy:  0.818, Val Loss:  0.610, Val Accuracy:  0.824: 100%|██████████| 28/28 [00:00<00:00, 106.32it/s]\n",
      "Epoch 137, EMA Train Loss: 0.657, Train Accuracy:  0.818, Val Loss:  0.661, Val Accuracy:  0.824: 100%|██████████| 28/28 [00:00<00:00, 112.54it/s]\n",
      "Epoch 138, EMA Train Loss: 0.650, Train Accuracy:  0.819, Val Loss:  0.628, Val Accuracy:  0.825: 100%|██████████| 28/28 [00:00<00:00, 112.91it/s]\n",
      "Epoch 139, EMA Train Loss: 0.643, Train Accuracy:  0.820, Val Loss:  0.626, Val Accuracy:  0.825: 100%|██████████| 28/28 [00:00<00:00, 110.91it/s]\n",
      "Epoch 140, EMA Train Loss: 0.646, Train Accuracy:  0.821, Val Loss:  0.646, Val Accuracy:  0.826: 100%|██████████| 28/28 [00:00<00:00, 111.00it/s]\n",
      "Epoch 141, EMA Train Loss: 0.634, Train Accuracy:  0.821, Val Loss:  0.624, Val Accuracy:  0.827: 100%|██████████| 28/28 [00:00<00:00, 113.16it/s]\n",
      "Epoch 142, EMA Train Loss: 0.638, Train Accuracy:  0.823, Val Loss:  0.617, Val Accuracy:  0.827: 100%|██████████| 28/28 [00:00<00:00, 111.81it/s]\n",
      "Epoch 143, EMA Train Loss: 0.636, Train Accuracy:  0.822, Val Loss:  0.622, Val Accuracy:  0.827: 100%|██████████| 28/28 [00:00<00:00, 109.01it/s]\n",
      "Epoch 144, EMA Train Loss: 0.633, Train Accuracy:  0.822, Val Loss:  0.617, Val Accuracy:  0.828: 100%|██████████| 28/28 [00:00<00:00, 106.43it/s]\n",
      "Epoch 145, EMA Train Loss: 0.622, Train Accuracy:  0.824, Val Loss:  0.659, Val Accuracy:  0.828: 100%|██████████| 28/28 [00:00<00:00, 111.79it/s]\n",
      "Epoch 146, EMA Train Loss: 0.628, Train Accuracy:  0.824, Val Loss:  0.613, Val Accuracy:  0.830: 100%|██████████| 28/28 [00:00<00:00, 108.85it/s]\n",
      "Epoch 147, EMA Train Loss: 0.629, Train Accuracy:  0.825, Val Loss:  0.615, Val Accuracy:  0.830: 100%|██████████| 28/28 [00:00<00:00, 114.49it/s]\n",
      "Epoch 148, EMA Train Loss: 0.630, Train Accuracy:  0.825, Val Loss:  0.622, Val Accuracy:  0.830: 100%|██████████| 28/28 [00:00<00:00, 112.70it/s]\n",
      "Epoch 149, EMA Train Loss: 0.622, Train Accuracy:  0.827, Val Loss:  0.624, Val Accuracy:  0.830: 100%|██████████| 28/28 [00:00<00:00, 113.27it/s]\n",
      "Epoch 150, EMA Train Loss: 0.613, Train Accuracy:  0.828, Val Loss:  0.605, Val Accuracy:  0.831: 100%|██████████| 28/28 [00:00<00:00, 114.07it/s]\n",
      "Epoch 151, EMA Train Loss: 0.610, Train Accuracy:  0.828, Val Loss:  0.597, Val Accuracy:  0.832: 100%|██████████| 28/28 [00:00<00:00, 113.18it/s]\n",
      "Epoch 152, EMA Train Loss: 0.613, Train Accuracy:  0.828, Val Loss:  0.607, Val Accuracy:  0.832: 100%|██████████| 28/28 [00:00<00:00, 107.61it/s]\n",
      "Epoch 153, EMA Train Loss: 0.612, Train Accuracy:  0.830, Val Loss:  0.595, Val Accuracy:  0.834: 100%|██████████| 28/28 [00:00<00:00, 112.59it/s]\n",
      "Epoch 154, EMA Train Loss: 0.604, Train Accuracy:  0.830, Val Loss:  0.613, Val Accuracy:  0.834: 100%|██████████| 28/28 [00:00<00:00, 108.75it/s]\n",
      "Epoch 155, EMA Train Loss: 0.602, Train Accuracy:  0.830, Val Loss:  0.597, Val Accuracy:  0.836: 100%|██████████| 28/28 [00:00<00:00, 109.78it/s]\n",
      "Epoch 156, EMA Train Loss: 0.601, Train Accuracy:  0.832, Val Loss:  0.593, Val Accuracy:  0.835: 100%|██████████| 28/28 [00:00<00:00, 113.90it/s]\n",
      "Epoch 157, EMA Train Loss: 0.609, Train Accuracy:  0.832, Val Loss:  0.612, Val Accuracy:  0.836: 100%|██████████| 28/28 [00:00<00:00, 114.66it/s]\n",
      "Epoch 158, EMA Train Loss: 0.599, Train Accuracy:  0.832, Val Loss:  0.575, Val Accuracy:  0.837: 100%|██████████| 28/28 [00:00<00:00, 115.42it/s]\n",
      "Epoch 159, EMA Train Loss: 0.596, Train Accuracy:  0.833, Val Loss:  0.558, Val Accuracy:  0.838: 100%|██████████| 28/28 [00:00<00:00, 110.13it/s]\n",
      "Epoch 160, EMA Train Loss: 0.589, Train Accuracy:  0.834, Val Loss:  0.572, Val Accuracy:  0.839: 100%|██████████| 28/28 [00:00<00:00, 113.53it/s]\n",
      "Epoch 161, EMA Train Loss: 0.587, Train Accuracy:  0.835, Val Loss:  0.574, Val Accuracy:  0.839: 100%|██████████| 28/28 [00:00<00:00, 113.19it/s]\n",
      "Epoch 162, EMA Train Loss: 0.584, Train Accuracy:  0.835, Val Loss:  0.573, Val Accuracy:  0.839: 100%|██████████| 28/28 [00:00<00:00, 113.45it/s]\n",
      "Epoch 163, EMA Train Loss: 0.582, Train Accuracy:  0.836, Val Loss:  0.543, Val Accuracy:  0.840: 100%|██████████| 28/28 [00:00<00:00, 113.59it/s]\n",
      "Epoch 164, EMA Train Loss: 0.582, Train Accuracy:  0.836, Val Loss:  0.574, Val Accuracy:  0.840: 100%|██████████| 28/28 [00:00<00:00, 108.94it/s]\n",
      "Epoch 165, EMA Train Loss: 0.582, Train Accuracy:  0.836, Val Loss:  0.579, Val Accuracy:  0.841: 100%|██████████| 28/28 [00:00<00:00, 110.80it/s]\n",
      "Epoch 166, EMA Train Loss: 0.576, Train Accuracy:  0.837, Val Loss:  0.534, Val Accuracy:  0.841: 100%|██████████| 28/28 [00:00<00:00, 112.45it/s]\n",
      "Epoch 167, EMA Train Loss: 0.569, Train Accuracy:  0.838, Val Loss:  0.592, Val Accuracy:  0.841: 100%|██████████| 28/28 [00:00<00:00, 107.77it/s]\n",
      "Epoch 168, EMA Train Loss: 0.570, Train Accuracy:  0.840, Val Loss:  0.577, Val Accuracy:  0.842: 100%|██████████| 28/28 [00:00<00:00, 112.25it/s]\n",
      "Epoch 169, EMA Train Loss: 0.565, Train Accuracy:  0.840, Val Loss:  0.579, Val Accuracy:  0.842: 100%|██████████| 28/28 [00:00<00:00, 111.22it/s]\n",
      "Epoch 170, EMA Train Loss: 0.562, Train Accuracy:  0.840, Val Loss:  0.571, Val Accuracy:  0.843: 100%|██████████| 28/28 [00:00<00:00, 111.04it/s]\n",
      "Epoch 171, EMA Train Loss: 0.569, Train Accuracy:  0.841, Val Loss:  0.535, Val Accuracy:  0.843: 100%|██████████| 28/28 [00:00<00:00, 113.08it/s]\n",
      "Epoch 172, EMA Train Loss: 0.565, Train Accuracy:  0.840, Val Loss:  0.569, Val Accuracy:  0.844: 100%|██████████| 28/28 [00:00<00:00, 111.65it/s]\n",
      "Epoch 173, EMA Train Loss: 0.567, Train Accuracy:  0.842, Val Loss:  0.568, Val Accuracy:  0.844: 100%|██████████| 28/28 [00:00<00:00, 108.87it/s]\n",
      "Epoch 174, EMA Train Loss: 0.556, Train Accuracy:  0.843, Val Loss:  0.525, Val Accuracy:  0.845: 100%|██████████| 28/28 [00:00<00:00, 112.94it/s]\n",
      "Epoch 175, EMA Train Loss: 0.548, Train Accuracy:  0.843, Val Loss:  0.568, Val Accuracy:  0.845: 100%|██████████| 28/28 [00:00<00:00, 111.41it/s]\n",
      "Epoch 176, EMA Train Loss: 0.554, Train Accuracy:  0.843, Val Loss:  0.521, Val Accuracy:  0.845: 100%|██████████| 28/28 [00:00<00:00, 109.27it/s]\n",
      "Epoch 177, EMA Train Loss: 0.553, Train Accuracy:  0.845, Val Loss:  0.524, Val Accuracy:  0.845: 100%|██████████| 28/28 [00:00<00:00, 112.55it/s]\n",
      "Epoch 178, EMA Train Loss: 0.553, Train Accuracy:  0.845, Val Loss:  0.533, Val Accuracy:  0.846: 100%|██████████| 28/28 [00:00<00:00, 113.63it/s]\n",
      "Epoch 179, EMA Train Loss: 0.554, Train Accuracy:  0.844, Val Loss:  0.547, Val Accuracy:  0.847: 100%|██████████| 28/28 [00:00<00:00, 106.98it/s]\n",
      "Epoch 180, EMA Train Loss: 0.548, Train Accuracy:  0.846, Val Loss:  0.595, Val Accuracy:  0.848: 100%|██████████| 28/28 [00:00<00:00, 110.04it/s]\n",
      "Epoch 181, EMA Train Loss: 0.546, Train Accuracy:  0.845, Val Loss:  0.534, Val Accuracy:  0.849: 100%|██████████| 28/28 [00:00<00:00, 108.53it/s]\n",
      "Epoch 182, EMA Train Loss: 0.541, Train Accuracy:  0.846, Val Loss:  0.532, Val Accuracy:  0.849: 100%|██████████| 28/28 [00:00<00:00, 113.49it/s]\n",
      "Epoch 183, EMA Train Loss: 0.545, Train Accuracy:  0.847, Val Loss:  0.513, Val Accuracy:  0.849: 100%|██████████| 28/28 [00:00<00:00, 114.02it/s]\n",
      "Epoch 184, EMA Train Loss: 0.539, Train Accuracy:  0.849, Val Loss:  0.528, Val Accuracy:  0.850: 100%|██████████| 28/28 [00:00<00:00, 100.74it/s]\n",
      "Epoch 185, EMA Train Loss: 0.539, Train Accuracy:  0.848, Val Loss:  0.575, Val Accuracy:  0.851: 100%|██████████| 28/28 [00:00<00:00, 113.57it/s]\n",
      "Epoch 186, EMA Train Loss: 0.538, Train Accuracy:  0.849, Val Loss:  0.547, Val Accuracy:  0.851: 100%|██████████| 28/28 [00:00<00:00, 114.76it/s]\n",
      "Epoch 187, EMA Train Loss: 0.530, Train Accuracy:  0.849, Val Loss:  0.567, Val Accuracy:  0.852: 100%|██████████| 28/28 [00:00<00:00, 108.30it/s]\n",
      "Epoch 188, EMA Train Loss: 0.526, Train Accuracy:  0.849, Val Loss:  0.515, Val Accuracy:  0.853: 100%|██████████| 28/28 [00:00<00:00, 110.58it/s]\n",
      "Epoch 189, EMA Train Loss: 0.532, Train Accuracy:  0.850, Val Loss:  0.563, Val Accuracy:  0.853: 100%|██████████| 28/28 [00:00<00:00, 106.22it/s]\n",
      "Epoch 190, EMA Train Loss: 0.526, Train Accuracy:  0.850, Val Loss:  0.567, Val Accuracy:  0.853: 100%|██████████| 28/28 [00:00<00:00, 100.52it/s]\n",
      "Epoch 191, EMA Train Loss: 0.521, Train Accuracy:  0.851, Val Loss:  0.504, Val Accuracy:  0.854: 100%|██████████| 28/28 [00:00<00:00, 111.84it/s]\n",
      "Epoch 192, EMA Train Loss: 0.519, Train Accuracy:  0.852, Val Loss:  0.506, Val Accuracy:  0.854: 100%|██████████| 28/28 [00:00<00:00, 108.99it/s]\n",
      "Epoch 193, EMA Train Loss: 0.522, Train Accuracy:  0.851, Val Loss:  0.565, Val Accuracy:  0.855: 100%|██████████| 28/28 [00:00<00:00, 111.88it/s]\n",
      "Epoch 194, EMA Train Loss: 0.518, Train Accuracy:  0.853, Val Loss:  0.492, Val Accuracy:  0.855: 100%|██████████| 28/28 [00:00<00:00, 112.94it/s]\n",
      "Epoch 195, EMA Train Loss: 0.519, Train Accuracy:  0.853, Val Loss:  0.519, Val Accuracy:  0.856: 100%|██████████| 28/28 [00:00<00:00, 113.08it/s]\n",
      "Epoch 196, EMA Train Loss: 0.517, Train Accuracy:  0.854, Val Loss:  0.521, Val Accuracy:  0.856: 100%|██████████| 28/28 [00:00<00:00, 110.46it/s]\n",
      "Epoch 197, EMA Train Loss: 0.507, Train Accuracy:  0.855, Val Loss:  0.545, Val Accuracy:  0.857: 100%|██████████| 28/28 [00:00<00:00, 110.67it/s]\n",
      "Epoch 198, EMA Train Loss: 0.513, Train Accuracy:  0.856, Val Loss:  0.497, Val Accuracy:  0.857: 100%|██████████| 28/28 [00:00<00:00, 113.91it/s]\n",
      "Epoch 199, EMA Train Loss: 0.509, Train Accuracy:  0.855, Val Loss:  0.493, Val Accuracy:  0.857: 100%|██████████| 28/28 [00:00<00:00, 112.15it/s]\n",
      "Epoch 200, EMA Train Loss: 0.510, Train Accuracy:  0.856, Val Loss:  0.536, Val Accuracy:  0.858: 100%|██████████| 28/28 [00:00<00:00, 113.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
