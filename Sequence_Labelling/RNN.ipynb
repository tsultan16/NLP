{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN-based Sequence Labeller\n",
    "\n",
    "We've seen how to implement an HMM bigram POS tagger and saw that it can achieve over 90% test accuracy on the Stanford treebank. The main limitation of this model is the limited context size (bigram context) which is used for making the sequence label predictions. Naively extending the algorithm to handle larger n-gram contexts may result in exponential increase in memory consumption (for vocab size $|V|$, the number of possible n-grams is on the order of $|V|^n$). A more natural way to handle larger contexts is using a `recurrent neural network (RNN)` to perform the sequence labelling task, where we use a sequence of pre-trained word embeddings as input and predict POS tags corrresponding to each word. We will experiment with some simple RNN architectures for this task and compare performance with the unigram and Viterbi bigram taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first download pretrained GloVe embeddings (api.load() returns a 'KeyedVectors' object)\n",
    "# we're getting the 50 dimensional words vectors, 400k vocab size\n",
    "embeddings = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  3914\n",
      "Longest sentence length: 271\n",
      "Vocab size: 12409\n",
      "['<s>', '#', '$', \"''\", ',', '-LRB-', '-NONE-', '-RRB-', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get the POS tagged corpus, 3914 tagged sentences\n",
    "corpus = treebank.tagged_sents()\n",
    "print(\"Number of sentences: \", len(corpus))\n",
    "print(f\"Longest sentence length: {max([len(s) for s in corpus])}\")\n",
    "\n",
    "\n",
    "# lets get the vocabulary and tag set\n",
    "pad_token = \"<PAD>\"\n",
    "vocab = [pad_token] + sorted(list(set([elem[0] for s in corpus for elem in s])))\n",
    "vocab_size = len(vocab)\n",
    "start_tag = \"<s>\"\n",
    "tags = [start_tag] + sorted(list(set([elem[1] for s in corpus for elem in s])))\n",
    "num_tags = len(tags)\n",
    "\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "tag2idx = {t:i for i,t in enumerate(tags)}\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words for which embeddings not available: 0\n"
     ]
    }
   ],
   "source": [
    "# now lets get the embeddings for all words in our vocab\n",
    "embedding_dim = 50\n",
    "embedding_vectors = np.zeros(shape=(vocab_size,embedding_dim))\n",
    "\n",
    "oov_words = []\n",
    "for i, word in enumerate(vocab):\n",
    "    if word.lower() in embeddings.key_to_index:\n",
    "        embedding_vectors[i] = embeddings[word.lower()]\n",
    "    else:\n",
    "        #print(f\"'{word.lower()}' not in GloVe vocab!\")    \n",
    "        # if the word is hyphenated, then split it and see if the sub-words have embeddings\n",
    "        if \"-\" in word:\n",
    "            split_hyphen = word.lower().split(\"-\")\n",
    "            # compute average word embedding across split words\n",
    "            emb = np.zeros(shape=(embedding_dim))\n",
    "            found = False\n",
    "            for i, w in enumerate(split_hyphen):\n",
    "                if w in embeddings.key_to_index:\n",
    "                    emb += embeddings[w]\n",
    "                    found = True\n",
    "            if found:        \n",
    "                emb += (len(split_hyphen)-1) * embeddings[\"-\"]\n",
    "                emb = emb / (2*len(split_hyphen)-1)\n",
    "                embedding_vectors[i] = emb   \n",
    "            else:\n",
    "                oov_words.append(word)     \n",
    "\n",
    "print(f\"Number of words for which embeddings not available: {len(oov_words)}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences and tags to get the inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[word2idx[word] for word,tag in s] for s in corpus]\n",
    "y = [[tag2idx[tag] for word,tag in s] for s in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train-validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest train sentence length: 271\n",
      "Longest val sentence length: 58\n"
     ]
    }
   ],
   "source": [
    "num_train = int(0.9 * len(x))\n",
    "x_train, y_train = x[:num_train], y[:num_train]\n",
    "x_val, y_val = x[num_train:], y[num_train:]\n",
    "\n",
    "print(f\"Longest train sentence length: {max([len(s) for s in x_train])}\")\n",
    "print(f\"Longest val sentence length: {max([len(s) for s in x_val])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Treebank(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = pad_sequence([torch.tensor(x, dtype=torch.long) for x in inputs], batch_first=True, padding_value=0)\n",
    "        self.targets = pad_sequence([torch.tensor(y, dtype=torch.long) for y in targets], batch_first=True, padding_value=-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Treebank(x_train, y_train)\n",
    "val_dataset = Treebank(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's create our RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_tags, embedding_dims, pretrained_embeddings, num_rnn_layers=1, hidden_dims=64, dropout_rate=0.1, padding_idx=-1):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(vocab_size, embedding_dims)        \n",
    "        # intialize with pretrained embedding vectors\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        # freeze the embedding layer (i.e. make non-trainable)\n",
    "        self.emb.weight.requires_grad = False\n",
    "        # create rnn layers (we will use bidirectional LSTM so the output hidden states will have dims=2*hidden_dims)\n",
    "        self.rnn_layers = torch.nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dims, num_layers=num_rnn_layers, bidirectional=True, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # create output layer (computes output class logits for each item in sequence)\n",
    "        self.output_layer =  torch.nn.Linear(2*hidden_dims, num_tags)\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, x, y):\n",
    "        # get embeddings for batch of input sequences of length L\n",
    "        x = self.emb(x) # shape: (B,L,D)\n",
    "        # compute rnn hidden states\n",
    "        x, _ = self.rnn_layers(x) # shape: (B,L,2*H)\n",
    "        # apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # compute output logits\n",
    "        x = self.output_layer(x) # shape: (B,L,num_tags)\n",
    "        # reshape\n",
    "        x = x.view(-1,x.shape[-1]) # shape: (B*L,num_tags)\n",
    "        y = y.view(-1) # shape: (B*L,)\n",
    "        # compute cross entropy loss\n",
    "        loss = F.cross_entropy(x, y, ignore_index=self.padding_idx)\n",
    "\n",
    "        return x, loss\n",
    "        \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=\"cpu\", num_epochs=10, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for batch in pbar:\n",
    "            inputs, targets = batch\n",
    "            # move batch to device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            logits, loss = model(inputs, targets)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            \n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            \n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "        scheduler.step()\n",
    "        train_acc = num_correct / num_total        \n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "\n",
    "        #if epoch % 5 == 0:\n",
    "        #    save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            inputs, targets = batch = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            y_pred = logits.argmax(dim=-1) # shape (B*L)\n",
    "            y = targets.view(-1) # shape (B*L)\n",
    "            mask = (y != -1)\n",
    "            num_correct += (torch.eq(y[mask], y_pred[mask])).sum().item()\n",
    "            num_total += len(y[mask])\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, 'rnntagger_checkpoint.pth')\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer):\n",
    "    checkpoint = torch.load('rnntagger_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.train()\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 0.685905 M\n",
      "RAM used: 1257.09 MB\n"
     ]
    }
   ],
   "source": [
    "B = 128\n",
    "H = 64\n",
    "num_rnn_layers = 1\n",
    "learning_rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = RNNTagger(vocab_size, num_tags, embedding_dim, embedding_vectors, hidden_dims=H, num_rnn_layers=num_rnn_layers).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, shuffle=True, pin_memory=True, num_workers=2)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 3.590, Train Accuracy:  0.000, Val Loss:  0.000, Val Accuracy:  0.000: 100%|██████████| 28/28 [00:00<00:00, 62.91it/s]\n",
      "Epoch 2, EMA Train Loss: 3.708, Train Accuracy:  0.064, Val Loss:  3.763, Val Accuracy:  0.117: 100%|██████████| 28/28 [00:00<00:00, 99.69it/s] \n",
      "Epoch 3, EMA Train Loss: 3.613, Train Accuracy:  0.118, Val Loss:  3.682, Val Accuracy:  0.143: 100%|██████████| 28/28 [00:00<00:00, 109.79it/s]\n",
      "Epoch 4, EMA Train Loss: 3.428, Train Accuracy:  0.131, Val Loss:  3.550, Val Accuracy:  0.147: 100%|██████████| 28/28 [00:00<00:00, 117.53it/s]\n",
      "Epoch 5, EMA Train Loss: 3.191, Train Accuracy:  0.130, Val Loss:  3.322, Val Accuracy:  0.147: 100%|██████████| 28/28 [00:00<00:00, 116.89it/s]\n",
      "Epoch 6, EMA Train Loss: 3.069, Train Accuracy:  0.129, Val Loss:  3.091, Val Accuracy:  0.147: 100%|██████████| 28/28 [00:00<00:00, 115.23it/s]\n",
      "Epoch 7, EMA Train Loss: 3.006, Train Accuracy:  0.132, Val Loss:  2.992, Val Accuracy:  0.150: 100%|██████████| 28/28 [00:00<00:00, 117.55it/s]\n",
      "Epoch 8, EMA Train Loss: 2.958, Train Accuracy:  0.149, Val Loss:  2.934, Val Accuracy:  0.162: 100%|██████████| 28/28 [00:00<00:00, 115.21it/s]\n",
      "Epoch 9, EMA Train Loss: 2.919, Train Accuracy:  0.164, Val Loss:  2.882, Val Accuracy:  0.170: 100%|██████████| 28/28 [00:00<00:00, 116.82it/s]\n",
      "Epoch 10, EMA Train Loss: 2.877, Train Accuracy:  0.173, Val Loss:  2.847, Val Accuracy:  0.187: 100%|██████████| 28/28 [00:00<00:00, 114.60it/s]\n",
      "Epoch 11, EMA Train Loss: 2.837, Train Accuracy:  0.194, Val Loss:  2.802, Val Accuracy:  0.219: 100%|██████████| 28/28 [00:00<00:00, 114.17it/s]\n",
      "Epoch 12, EMA Train Loss: 2.781, Train Accuracy:  0.222, Val Loss:  2.788, Val Accuracy:  0.265: 100%|██████████| 28/28 [00:00<00:00, 106.52it/s]\n",
      "Epoch 13, EMA Train Loss: 2.734, Train Accuracy:  0.248, Val Loss:  2.704, Val Accuracy:  0.285: 100%|██████████| 28/28 [00:00<00:00, 111.28it/s]\n",
      "Epoch 14, EMA Train Loss: 2.692, Train Accuracy:  0.271, Val Loss:  2.673, Val Accuracy:  0.305: 100%|██████████| 28/28 [00:00<00:00, 117.17it/s]\n",
      "Epoch 15, EMA Train Loss: 2.634, Train Accuracy:  0.293, Val Loss:  2.631, Val Accuracy:  0.324: 100%|██████████| 28/28 [00:00<00:00, 114.94it/s]\n",
      "Epoch 16, EMA Train Loss: 2.585, Train Accuracy:  0.313, Val Loss:  2.565, Val Accuracy:  0.358: 100%|██████████| 28/28 [00:00<00:00, 106.51it/s]\n",
      "Epoch 17, EMA Train Loss: 2.537, Train Accuracy:  0.336, Val Loss:  2.493, Val Accuracy:  0.382: 100%|██████████| 28/28 [00:00<00:00, 101.04it/s]\n",
      "Epoch 18, EMA Train Loss: 2.481, Train Accuracy:  0.359, Val Loss:  2.446, Val Accuracy:  0.409: 100%|██████████| 28/28 [00:00<00:00, 117.53it/s]\n",
      "Epoch 19, EMA Train Loss: 2.428, Train Accuracy:  0.382, Val Loss:  2.413, Val Accuracy:  0.430: 100%|██████████| 28/28 [00:00<00:00, 116.62it/s]\n",
      "Epoch 20, EMA Train Loss: 2.375, Train Accuracy:  0.404, Val Loss:  2.364, Val Accuracy:  0.449: 100%|██████████| 28/28 [00:00<00:00, 120.55it/s]\n",
      "Epoch 21, EMA Train Loss: 2.315, Train Accuracy:  0.421, Val Loss:  2.300, Val Accuracy:  0.468: 100%|██████████| 28/28 [00:00<00:00, 109.16it/s]\n",
      "Epoch 22, EMA Train Loss: 2.264, Train Accuracy:  0.442, Val Loss:  2.252, Val Accuracy:  0.481: 100%|██████████| 28/28 [00:00<00:00, 116.70it/s]\n",
      "Epoch 23, EMA Train Loss: 2.199, Train Accuracy:  0.456, Val Loss:  2.222, Val Accuracy:  0.495: 100%|██████████| 28/28 [00:00<00:00, 113.95it/s]\n",
      "Epoch 24, EMA Train Loss: 2.153, Train Accuracy:  0.471, Val Loss:  2.136, Val Accuracy:  0.504: 100%|██████████| 28/28 [00:00<00:00, 119.40it/s]\n",
      "Epoch 25, EMA Train Loss: 2.109, Train Accuracy:  0.481, Val Loss:  2.085, Val Accuracy:  0.512: 100%|██████████| 28/28 [00:00<00:00, 118.63it/s]\n",
      "Epoch 26, EMA Train Loss: 2.059, Train Accuracy:  0.492, Val Loss:  2.026, Val Accuracy:  0.522: 100%|██████████| 28/28 [00:00<00:00, 117.47it/s]\n",
      "Epoch 27, EMA Train Loss: 2.006, Train Accuracy:  0.502, Val Loss:  1.976, Val Accuracy:  0.533: 100%|██████████| 28/28 [00:00<00:00, 116.60it/s]\n",
      "Epoch 28, EMA Train Loss: 1.956, Train Accuracy:  0.510, Val Loss:  1.943, Val Accuracy:  0.537: 100%|██████████| 28/28 [00:00<00:00, 118.65it/s]\n",
      "Epoch 29, EMA Train Loss: 1.920, Train Accuracy:  0.519, Val Loss:  1.868, Val Accuracy:  0.544: 100%|██████████| 28/28 [00:00<00:00, 113.74it/s]\n",
      "Epoch 30, EMA Train Loss: 1.880, Train Accuracy:  0.524, Val Loss:  1.869, Val Accuracy:  0.551: 100%|██████████| 28/28 [00:00<00:00, 119.44it/s]\n",
      "Epoch 31, EMA Train Loss: 1.837, Train Accuracy:  0.533, Val Loss:  1.818, Val Accuracy:  0.555: 100%|██████████| 28/28 [00:00<00:00, 115.80it/s]\n",
      "Epoch 32, EMA Train Loss: 1.793, Train Accuracy:  0.539, Val Loss:  1.797, Val Accuracy:  0.561: 100%|██████████| 28/28 [00:00<00:00, 121.33it/s]\n",
      "Epoch 33, EMA Train Loss: 1.777, Train Accuracy:  0.545, Val Loss:  1.755, Val Accuracy:  0.565:   0%|          | 0/28 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, scheduler, train_dataloader, val_dataloader, device=DEVICE, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
