{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT augmented CKY variant\n",
    "\n",
    "Previously we looked at implementing the CKY algorithm to generate parse trees for sentences conforming to a context-free grammar in Chomsky-normal-form. We found that sentences generated by a CFG suffer from syntax ambiguities, i.e. a sentence can have multiple valid parse trees, each with a different meaning. However, we know that out of the different possible parses, typically only one of them is the \"correct\" one, i.e. captures the intended meaning of the sentence. \n",
    "\n",
    "In this notebook, we will look an a variant of the CKY algorithm in which each possible span of a sentence is assigned a score, and these scores can be used to arrive at the correct parse tree. These scores can be computed with the help of a neural network trained on an annotated treebank dataset. The training task simply involves predicting a distribution of scores over all possible non-terminal labels for each valid span/constituent of a sentence. After training this neural model, it is expected to assign large score to the correct label for each constituents of any given sentence, which will then help with the downstream task of disambiguating the correct parse tree (which will be done using a slightly modified CKY algorithm). A BERT model is powerful and well-suited for the score prediction task. The diagram below (borrowed from the Jurafsky-Martin textbook) summarizes the model architecture:\n",
    "\n",
    "<img src=\"neural_parser.png\" width=\"600\" height=\"450\">\n",
    "\n",
    "We define the spans in the same way as we did for the vanilla CKY parser, i.e. using the \"fencepost\" positions. We also use the same upper-triangular matrix that which we used previously. Instead of using a pre-defined CFG in CNF to assign non-terminal labels to each element in this matrix (which represent the different possible spans), this time we will instead use BERT model to compute a distribution of scores over all possible terminals for each possible span. First, we create a fixed-size vector representation of the span and then feed it into an MLP classifier, as shown in the diagram. We outline the steps in more detail:\n",
    "\n",
    "1) Convert words to subword tokens\n",
    "2) Get `BERT embeddings for subwords`\n",
    "3) Compute the `embeddings for full words` (many ways to do this, e.g. we could just assign the BERT em,bedding for the first subword of that word, or we could take the sum.average of the embedding of all the subwords or we could take element-wise max across all the subword embeddings, etc.)  \n",
    "4) Compute `embeddings of fence posts` (shown as 0,1,2,3.. in the diagram above). Since each fence-post can represent the beginning or end of a span, we will create two separate representations. We first split the embedding vector $y_t$ of the $t$-th word in the sentence into two halves, $\\overleftarrow{y_t}$ and $\\overrightarrow{y_t}$ such that the concatentaion $[\\overleftarrow{y_t}; \\overrightarrow{y_t}] = y_t$. Then the `start-of-span representation` of the fencepost at position $i$ is defined as $\\overrightarrow{y_i}$ and the `end-of-span representation` is defined as $\\overleftarrow{y}_{i+1}$\n",
    "5) Construct `embedding for a span` `(i,j)` using the fencepost embeddings as the following concatenation between the difference in start-of span and end-of-span embeddings for the bounding fenceposts: $v(i,j) = [\\overrightarrow{y_j}-\\overrightarrow{y_i}; \\overleftarrow{y}_{j+1}-\\overleftarrow{y}_{i+1}]$\n",
    "6) Pass $v(i,j)$ through the MLP to get a distribution of scores over all possible non-terminal labels.\n",
    "\n",
    "One really important thing to note here is that we are no longer using a pre-defined context-free grammar. The supervised training of the neural network model will implicitly induce/\"learn\" the grammar. Also, one downside is that the model may sometimes fail to get a grammatically correct parse of a sentence, because it does not have access to the \"true grammar\", only some form of statistical approximation of it.\n",
    "\n",
    "After we implement and train this model, we will iplement the CKY variant that will perform the actual parsing using the scores computed by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk import Tree\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "random.seed(10)\n",
    "import psutil\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstm let's load the data. We will use the NLTK treebank, which is a subset of the original Penn Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parsed sentences: 3910\n"
     ]
    }
   ],
   "source": [
    "# get all parsed sentences acrsoo all the files\n",
    "sentences = treebank.sents()\n",
    "parse_trees = treebank.parsed_sents()\n",
    "\n",
    "# only keep sentences that are at most 100 words\n",
    "sentences_parses = zip(sentences, parse_trees)\n",
    "sentences_parses = [(s,p) for s,p in sentences_parses if len(s) <= 100]\n",
    "\n",
    "sentences = [s for s,p in sentences_parses]\n",
    "parse_trees = [p for s,p in sentences_parses]\n",
    "\n",
    "print(f\"Number of parsed sentences: {len(parse_trees)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 92 25.58618925831202\n"
     ]
    }
   ],
   "source": [
    "sentences_lengths = [len(s) for s in sentences]\n",
    "print(min(sentences_lengths), max(sentences_lengths), sum(sentences_lengths)/len(sentences_lengths))\n",
    "\n",
    "# plot a histogram of sentence lengths\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.hist(sentences_lengths, bins=30)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "# print example of a parsed sentence\n",
    "example_tree = parse_trees[0]\n",
    "print(example_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for extracting all the non-terminal labels for all possible spans from each sentence\n",
    "def get_span_labels(parse_tree, verbose=False):\n",
    "    span_labels = {}\n",
    "    leaves = parse_tree.leaves()\n",
    "    if verbose:\n",
    "        parse_tree.pretty_print()\n",
    "    # iterate over all subtrees in level-order traversal\n",
    "    for subtree in parse_tree.subtrees():\n",
    "        subtree_leaves = subtree.leaves()\n",
    "        start_index = leaves.index(subtree_leaves[0])\n",
    "        end_index = start_index + len(subtree_leaves)\n",
    "        span = (start_index, end_index)\n",
    "        if span[1] < span[0]:\n",
    "            parse_tree.pretty_print()\n",
    "            raise ValueError(f\"Span start index is greater than or equal to span end index: {span}\")\n",
    "        span_labels[span] = subtree.label()\n",
    "        if verbose:\n",
    "            print(f\"\\nsubtree label: {subtree.label()}\")\n",
    "            print(f\"subtree leaves: {subtree.leaves()}\")\n",
    "            print(f\"Start fence-post: {start_index}\")\n",
    "            print(f\"End fence-post: {end_index}\")\n",
    "            print(f\"span: {span}\")\n",
    "    return span_labels           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 18): 'S',\n",
       " (0, 7): 'NP-SBJ',\n",
       " (0, 2): 'NP',\n",
       " (0, 1): 'NNP',\n",
       " (1, 2): 'NNP',\n",
       " (2, 3): ',',\n",
       " (3, 6): 'ADJP',\n",
       " (3, 5): 'NP',\n",
       " (3, 4): 'CD',\n",
       " (4, 5): 'NNS',\n",
       " (5, 6): 'JJ',\n",
       " (7, 17): 'VP',\n",
       " (7, 8): 'MD',\n",
       " (8, 17): 'VP',\n",
       " (8, 9): 'VB',\n",
       " (9, 11): 'NP',\n",
       " (9, 10): 'DT',\n",
       " (10, 11): 'NN',\n",
       " (11, 15): 'PP-CLR',\n",
       " (11, 12): 'IN',\n",
       " (12, 15): 'NP',\n",
       " (12, 13): 'DT',\n",
       " (13, 14): 'JJ',\n",
       " (14, 15): 'NN',\n",
       " (15, 17): 'NP-TMP',\n",
       " (15, 16): 'NNP',\n",
       " (16, 17): 'CD',\n",
       " (17, 18): '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_span_labels(example_tree, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because CKY requires the parse trees to be binary, we need to convert all n-ary trees from our treebank dataset to binary form. We use a simple scheme for binarization, we recursively traverse down the tree starting from the root, and everytime we find a node with more than two children, we create a new node with the 'Null' label and make it the parent of all children except for the leftmost child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursively binarize a parse tree\n",
    "def binarize_tree(parse_tree, empty_label='Null'):\n",
    "    if isinstance(parse_tree, Tree):\n",
    "        if len(parse_tree) == 1:\n",
    "            return Tree(parse_tree.label(), [parse_tree[0]])\n",
    "        elif len(parse_tree) == 2:\n",
    "            return Tree(parse_tree.label(), [binarize_tree(parse_tree[0]), binarize_tree(parse_tree[1])])\n",
    "        elif len(parse_tree) > 2:\n",
    "            return Tree(parse_tree.label(), [parse_tree[0], binarize_tree(Tree(empty_label, parse_tree[1:]))])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tree_binarized = binarize_tree(example_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     S                                                                               \n",
      "                         ____________________________|______________________                                                          \n",
      "                        |                                                  Null                                                      \n",
      "                        |                             ______________________|______________________________________________________   \n",
      "                        |                            VP                                                                            | \n",
      "                        |                        ____|_____________                                                                |  \n",
      "                        |                       |                  VP                                                              | \n",
      "                        |                       |     _____________|________                                                       |  \n",
      "                        |                       |    |                     Null                                                    | \n",
      "                        |                       |    |         _____________|_______________                                       |  \n",
      "                        |                       |    |        |                            Null                                    | \n",
      "                        |                       |    |        |                    _________|____________________________          |  \n",
      "                      NP-SBJ                    |    |        |                 PP-CLR                                   |         | \n",
      "         _______________|___________________    |    |        |          _________|_________                             |         |  \n",
      "        |          |              ADJP      |   |    |        |         |                   NP                           |         | \n",
      "        |          |           ____|____    |   |    |        |         |    _______________|________                    |         |  \n",
      "        NP         |          NP        |   |   |    |        NP        |   |                       Null               NP-TMP      | \n",
      "   _____|____      |     _____|____     |   |   |    |     ___|____     |   |                ________|______        _____|_____    |  \n",
      " NNP        NNP    ,    CD        NNS   JJ  ,   MD   VB   DT       NN   IN  DT              JJ              NN    NNP          CD  . \n",
      "  |          |     |    |          |    |   |   |    |    |        |    |   |               |               |      |           |   |  \n",
      "Pierre     Vinken  ,    61       years old  ,  will join the     board  as  a          nonexecutive      director Nov.         29  . \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 18): 'S',\n",
       " (0, 7): 'NP-SBJ',\n",
       " (0, 2): 'NP',\n",
       " (0, 1): 'NNP',\n",
       " (1, 2): 'NNP',\n",
       " (2, 3): ',',\n",
       " (3, 6): 'ADJP',\n",
       " (3, 5): 'NP',\n",
       " (3, 4): 'CD',\n",
       " (4, 5): 'NNS',\n",
       " (5, 6): 'JJ',\n",
       " (7, 18): 'Null',\n",
       " (7, 17): 'VP',\n",
       " (7, 8): 'MD',\n",
       " (8, 17): 'VP',\n",
       " (8, 9): 'VB',\n",
       " (9, 17): 'Null',\n",
       " (9, 11): 'NP',\n",
       " (9, 10): 'DT',\n",
       " (10, 11): 'NN',\n",
       " (11, 17): 'Null',\n",
       " (11, 15): 'PP-CLR',\n",
       " (11, 12): 'IN',\n",
       " (12, 15): 'NP',\n",
       " (12, 13): 'DT',\n",
       " (13, 15): 'Null',\n",
       " (13, 14): 'JJ',\n",
       " (14, 15): 'NN',\n",
       " (15, 17): 'NP-TMP',\n",
       " (15, 16): 'NNP',\n",
       " (16, 17): 'CD',\n",
       " (17, 18): '.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tree_binarized.pretty_print()\n",
    "get_span_labels(example_tree_binarized, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score-based CKY variant: For the Vanilla CKY, we constructed parse trees by filling an upper triangular matrix in a left to right bottom-up fashion, then traced the back pointers to recover the parse. We will use a variant of CKY which uses span span scores and the following recursion relation for computing the highest scoring subtree rooted at a given span.\n",
    "\n",
    "Let $s_{best}(i,j)$ denote the score of the best sub-tree for the span `(i,j)`. Let $score(i,j,l)$ denote the score of the span `(i,j)` for label $l$. Then the base case of the recusion relation, i.e. for the spans of length 1, is given by:\n",
    "\n",
    "$s_{best}(i,i+1) = \\max_l score(i,i+1,l)$\n",
    "\n",
    "and for the general case:\n",
    "\n",
    "$s_{best}(i,j) = \\max_l score(i,j,l) + \\max_k (s_{best}(i,k) + s_{best}(k,j))$\n",
    "\n",
    "Now, we can fill the upper triangular matrix by computing the value of $s_{best}(i,j)$ inside each cell using this equation (initializing the super-diagonal cell values using the base case, and initializing the remaining cell values as -$\\infty$).\n",
    "\n",
    "At each split, we also store the label $l$ and the split position $k$ in our back pointers, which will then allow us to recover the parse tree.\n",
    "\n",
    "Then the score of the entire tree is computed by summing up the scores for each node as follows:\n",
    "\n",
    "$S(T) = \\sum_{(i,j,l) \\in T} score(i,j,l)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# lets implement this CKY variant and demonstrate with a simple example\\n\\n# define a score function (can be anything) and assume we have only 3 different labels and 4 words in our sentence\\nL = 3\\nn = 4\\nscores = torch.rand((n+1,n+1,L)) \\n\\nspans = [(i,j+1) for i in range(n) for j in range(i,n)]\\ns_best = {span: float(\\'-inf\\') for span in spans}\\nback = {}\\n\\n# base case initialization\\nfor i in range(n):\\n    s_best[(i,i+1)] = scores[i,i+1,:].max().item()\\n    best_label = scores[i,i+1,:].argmax().item()\\n    back[(i,i+1)] = (i,best_label)\\n\\n# gerenal case bottom up\\nfor j in range(1, n+1):\\n    for i in range(j-2, -1, -1):\\n        print(f\"span: ({i},{j})\")\\n        # get best label and score\\n        best_label_score = scores[i,j,:].max().item()\\n        best_label = scores[i,j,:].argmax().item()\\n        # get best split\\n        best_split_score = float(\\'-inf\\') \\n        for k in range(i+1, j):\\n            split_score = s_best[(i,k)] + s_best[(k,j)]\\n            if split_score > best_split_score:\\n                best_split_score = split_score\\n                best_k = k\\n        # score of best subtree for this span\\n        s_best[(i,j)] = best_label_score + best_split_score \\n        # back pointer\\n        back[(i,j)] = (best_k,best_label) \\n        print(f\"Best label: {best_label}, Best label score: {best_label_score}, Best split score: {best_split_score}, Best split: {best_k}\")       \\n\\n\\n# now let\\'s recursively retreive the best tree\\ndef get_tree(i, j, back):\\n    if i == j-1:\\n        return Tree(f\"w{i}\", [f\"w{i}\"])\\n    else:\\n        k, label = back[(i,j)]\\n        return Tree(f\"nt{label}\", [get_tree(i,k,back), get_tree(k,j,back)])\\n\\nbest_tree = get_tree(0, n, back)\\nbest_tree.pretty_print()        \\n\\n# recursively retriev the (span, label) tuples for all nodes in the best tree\\ndef get_tree_spans_labels(i, j, back):\\n    if i == j-1:\\n        k, label = back[(i,j)]\\n        return [(i,j,label)]\\n    else:\\n        k, label = back[(i,j)]\\n        return [(i,j,label)] + get_tree_spans_labels(i, k, back) + get_tree_spans_labels(k, j, back)   \\n\\nspan_labels = get_tree_spans_labels(0, n, back) \\nprint(span_labels)\\n\\n# compute total score of the tree\\ntot_score = 0.0\\nfor (i,j,label) in span_labels:\\n    tot_score += scores[i,j,label].item()\\nprint(f\"Total score: {tot_score}\")    \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# lets implement this CKY variant and demonstrate with a simple example\n",
    "\n",
    "# define a score function (can be anything) and assume we have only 3 different labels and 4 words in our sentence\n",
    "L = 3\n",
    "n = 4\n",
    "scores = torch.rand((n+1,n+1,L)) \n",
    "\n",
    "spans = [(i,j+1) for i in range(n) for j in range(i,n)]\n",
    "s_best = {span: float('-inf') for span in spans}\n",
    "back = {}\n",
    "\n",
    "# base case initialization\n",
    "for i in range(n):\n",
    "    s_best[(i,i+1)] = scores[i,i+1,:].max().item()\n",
    "    best_label = scores[i,i+1,:].argmax().item()\n",
    "    back[(i,i+1)] = (i,best_label)\n",
    "\n",
    "# gerenal case bottom up\n",
    "for j in range(1, n+1):\n",
    "    for i in range(j-2, -1, -1):\n",
    "        print(f\"span: ({i},{j})\")\n",
    "        # get best label and score\n",
    "        best_label_score = scores[i,j,:].max().item()\n",
    "        best_label = scores[i,j,:].argmax().item()\n",
    "        # get best split\n",
    "        best_split_score = float('-inf') \n",
    "        for k in range(i+1, j):\n",
    "            split_score = s_best[(i,k)] + s_best[(k,j)]\n",
    "            if split_score > best_split_score:\n",
    "                best_split_score = split_score\n",
    "                best_k = k\n",
    "        # score of best subtree for this span\n",
    "        s_best[(i,j)] = best_label_score + best_split_score \n",
    "        # back pointer\n",
    "        back[(i,j)] = (best_k,best_label) \n",
    "        print(f\"Best label: {best_label}, Best label score: {best_label_score}, Best split score: {best_split_score}, Best split: {best_k}\")       \n",
    "\n",
    "\n",
    "# now let's recursively retreive the best tree\n",
    "def get_tree(i, j, back):\n",
    "    if i == j-1:\n",
    "        return Tree(f\"w{i}\", [f\"w{i}\"])\n",
    "    else:\n",
    "        k, label = back[(i,j)]\n",
    "        return Tree(f\"nt{label}\", [get_tree(i,k,back), get_tree(k,j,back)])\n",
    "\n",
    "best_tree = get_tree(0, n, back)\n",
    "best_tree.pretty_print()        \n",
    "\n",
    "# recursively retriev the (span, label) tuples for all nodes in the best tree\n",
    "def get_tree_spans_labels(i, j, back):\n",
    "    if i == j-1:\n",
    "        k, label = back[(i,j)]\n",
    "        return [(i,j,label)]\n",
    "    else:\n",
    "        k, label = back[(i,j)]\n",
    "        return [(i,j,label)] + get_tree_spans_labels(i, k, back) + get_tree_spans_labels(k, j, back)   \n",
    "\n",
    "span_labels = get_tree_spans_labels(0, n, back) \n",
    "print(span_labels)\n",
    "\n",
    "# compute total score of the tree\n",
    "tot_score = 0.0\n",
    "for (i,j,label) in span_labels:\n",
    "    tot_score += scores[i,j,label].item()\n",
    "print(f\"Total score: {tot_score}\")    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a pytorch dataset for creating the (input, target) instances for our BERT span score prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null label idx: 327\n",
      "Number of training sentences: 3519\n",
      "Number of validation sentences: 391\n"
     ]
    }
   ],
   "source": [
    "# first, let's binarize all the parse trees\n",
    "parse_trees_binarized = [binarize_tree(t) for t in parse_trees]\n",
    "\n",
    "# get span labels for all sentences\n",
    "span_labels = [get_span_labels(p) for p in parse_trees_binarized]\n",
    "    \n",
    "# create mapping of span labels to unique ids\n",
    "unique_span_labels = list(set([label for span in span_labels for label in span.values()]))\n",
    "label2idx = {label: i for i, label in enumerate(unique_span_labels)}    \n",
    "null_label_idx = label2idx['Null']\n",
    "print(f\"Null label idx: {null_label_idx}\")\n",
    "\n",
    "# create train-val splits\n",
    "n_train = int(0.9 * len(sentences))\n",
    "sentences_train = sentences[:n_train]\n",
    "parse_trees_train = parse_trees_binarized[:n_train]\n",
    "span_labels_train = span_labels[:n_train]\n",
    "sentences_val = sentences[n_train:]\n",
    "parse_trees_val = parse_trees_binarized[n_train:]\n",
    "span_labels_val = span_labels[n_train:]\n",
    "\n",
    "print(f\"Number of training sentences: {len(sentences_train)}\")\n",
    "print(f\"Number of validation sentences: {len(sentences_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseTreeDataset(Dataset):\n",
    "    def __init__(self, sentences, span_labels, label2idx, block_size=256, max_spans=1024):\n",
    "        self.sentences = sentences\n",
    "        self.span_labels = span_labels\n",
    "        self.label2idx = label2idx\n",
    "        self.block_size = block_size\n",
    "        self.max_spans = max_spans\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get sentence and span labels\n",
    "        sentence = self.sentences[idx]\n",
    "        span_labels = self.span_labels[idx]\n",
    "        # convert span labels to indices\n",
    "        span_labels_idx = {span:self.label2idx[label] for span, label in span_labels.items()}\n",
    "\n",
    "        # tokenize the sentence\n",
    "        input_encoding = self.tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "        input_idx = input_encoding['input_ids']\n",
    "        word_ids = input_encoding.word_ids()\n",
    "\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise ValueError(f\"Tokenized sentence {idx} is too long: {len(input_idx)}. Truncation unsupported.\")\n",
    "\n",
    "        # add padding \n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "        # create attention mask \n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask) \n",
    "        \n",
    "        return input_idx, input_attn_mask, word_ids, span_labels_idx   \n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the tensors and the dictionaries\n",
    "    input_idxs, input_attn_masks, word_ids, span_labels_idx = zip(*batch)\n",
    "\n",
    "    # Default collate the tensors\n",
    "    input_idxs = torch.stack(input_idxs)\n",
    "    input_attn_masks = torch.stack(input_attn_masks)\n",
    "\n",
    "    # Handle the dictionaries\n",
    "    spans_labels = []\n",
    "    for d in span_labels_idx:\n",
    "        # Convert the dictionary to two lists: one for spans and one for labels\n",
    "        spans, labels = zip(*d.items())\n",
    "        spans_labels.append((spans, labels))\n",
    "\n",
    "    return input_idxs, input_attn_masks, word_ids, spans_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implement the BERT model for span score prediction, which uses `margin-based training`. For margin-based training, we use an `SVM/hinge loss` function of the following form:\n",
    "\n",
    "$\\text{SVM Loss} = \\max (0, Hamming(T, T^*) + S(T) - S(T^*))$ \n",
    "\n",
    "where $S(T)$ and $S(T^*)$ are the total scores of the predicted and ground-truth parese trees respectively and we use the `Hamming distance` between $T$ and $T^*$ (i.e. the proportion of labeled spans that are different between the predicted and actual trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CKY(torch.nn.Module):\n",
    "    def __init__(self, num_classes, null_label_idx, dropout_rate=0.1, mlp_hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.null_label_idx = null_label_idx\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head (2 layer MLP)\n",
    "        self.classifier_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.bert_encoder.config.hidden_size, mlp_hidden_size),\n",
    "            torch.nn.LayerNorm(mlp_hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(mlp_hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def get_word_embeddings(self, bert_output, word_ids, i):\n",
    "        word_embeddings = [bert_output[i,0,:]] # add CLS token embedding\n",
    "        # construct embeddings for each word in the original (untokenized) sequence\n",
    "        for j in range(word_ids[-2]+1):\n",
    "            y_j = bert_output[i,word_ids.index(j),:] # gets first subword token embedding for word j\n",
    "            word_embeddings.append(y_j)        \n",
    "        word_embeddings.append(bert_output[i,len(word_ids)-1,:]) # add SEP token embedding\n",
    "        return word_embeddings\n",
    "\n",
    "    def compute_span_scores(self, word_embeddings, spans):\n",
    "        span_scores = {}\n",
    "        for i,j in spans:\n",
    "            # get right and left fencepost embeddings for the bounding fenceposts of this span\n",
    "            y_j_right = word_embeddings[j][768//2:]\n",
    "            y_i_right = word_embeddings[i][768//2:]\n",
    "            y_jplus1_left = word_embeddings[j+1][:768//2]\n",
    "            y_iplus1_left = word_embeddings[i+1][:768//2]\n",
    "            # concatenate difference vectors to get span embedding\n",
    "            span_embedding = torch.cat([y_j_right-y_i_right, y_jplus1_left-y_iplus1_left], dim=0) # shape: (hidden_size,)\n",
    "            # compute logits for this span\n",
    "            span_scores[(i,j)] = self.classifier_head(span_embedding) # shape: (num_classes,)\n",
    "            # zero out the score for the null label\n",
    "            span_scores[(i,j)][self.null_label_idx] = 0\n",
    "        return span_scores\n",
    "    \n",
    "    def get_tree_spans_labels(self, i, j, back):\n",
    "        if i == j-1:\n",
    "            _, label = back[(i,j)]\n",
    "            return [(i,j,label)]\n",
    "        else:\n",
    "            k, label = back[(i,j)]\n",
    "            return [(i,j,label)] + self.get_tree_spans_labels(i, k, back) + self.get_tree_spans_labels(k, j, back)  \n",
    "\n",
    "    def cky(self, spans, span_scores, num_words):\n",
    "        s_best = {span: float('-inf') for span in spans}\n",
    "        back = {}\n",
    "        # base case initialization\n",
    "        for i in range(num_words):\n",
    "            s_best[(i,i+1)] = span_scores[(i,i+1)].max()\n",
    "            best_label = span_scores[(i,i+1)].argmax().item()\n",
    "            back[(i,i+1)] = (i,best_label)\n",
    "\n",
    "        # gerenal case bottom up\n",
    "        for j in range(1, num_words+1):\n",
    "            for i in range(j-2, -1, -1):\n",
    "                #print(f\"span: ({i},{j})\")\n",
    "                # get best label and score\n",
    "                best_label_score = span_scores[(i,j)].max()\n",
    "                best_label = span_scores[(i,j)].argmax().item()\n",
    "                # get best split\n",
    "                best_split_score = float('-inf') \n",
    "                for k in range(i+1, j):\n",
    "                    split_score = s_best[(i,k)] + s_best[(k,j)]\n",
    "                    if split_score.item() > best_split_score:\n",
    "                        best_split_score = split_score\n",
    "                        best_k = k\n",
    "                # score of best subtree for this span\n",
    "                s_best[(i,j)] = best_label_score + best_split_score \n",
    "                # back pointer\n",
    "                back[(i,j)] = (best_k,best_label) \n",
    "                #print(f\"Best label: {best_label}, Best label score: {best_label_score}, Best split score: {best_split_score}, Best split: {best_k}\")   \n",
    "        return s_best, back\n",
    "    \n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, batch_word_ids, targets=None):\n",
    "        # compute BERT embeddings for input tokens\n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask)\n",
    "        bert_output = self.dropout(bert_output.last_hidden_state) # shape: (batch_size, block_size, hidden_size)\n",
    "\n",
    "        loss = 0.0\n",
    "        batch_back_pointers = []\n",
    "        batch_span_labels = []\n",
    "        # for each sequence in batch, get word embeddings for each word by using the BERT embeddings for first subword token for that word\n",
    "        for batch_idx, word_ids in enumerate(batch_word_ids):\n",
    "            word_embeddings = self.get_word_embeddings(bert_output, word_ids, batch_idx)\n",
    "            n = len(word_embeddings)-2\n",
    "            # now that we have the word embeddings, we need to construct span embeddings and compute scores for all possible spans\n",
    "            all_spans = [(i,j+1) for i in range(n) for j in range(i,n)]\n",
    "            #print(f\"Lenght of word embeddings list: {len(word_embeddings)}\")\n",
    "            #print(f\"num fenceposts: {n}, num spans: {len(all_spans)}\")\n",
    "            #print(f\"spans: {all_spans}\")\n",
    "            span_scores = self.compute_span_scores(word_embeddings, all_spans)\n",
    "\n",
    "            # now apply CKY algorithm to get the best tree for this sequence\n",
    "            s_best , back = self.cky(all_spans, span_scores, n)\n",
    "            batch_back_pointers.append(back)\n",
    "              \n",
    "            # recursively retrieve the (span, label) tuples for all nodes in the best predicted parse tree\n",
    "            span_labels = self.get_tree_spans_labels(0, n, back) \n",
    "            batch_span_labels.append(span_labels)\n",
    "            #print(span_labels)\n",
    "\n",
    "            if targets is None:\n",
    "                continue\n",
    "\n",
    "            # compute total score of the predicted parse tree by summing up scores of all the constituent spans\n",
    "            tree_score = 0.0\n",
    "            for (i,j,label) in span_labels:\n",
    "                tree_score += span_scores[(i,j)][label]\n",
    "            #print(f\"Total score: {tot_score}\")   \n",
    "                \n",
    "            # compute the total score of the gold standard parse tree and Hamming loss\n",
    "            gold_spans, gold_labels = targets[batch_idx]\n",
    "            gold_score = 0.0    \n",
    "            hamming = 0.0\n",
    "            for (i,j),label in zip(gold_spans, gold_labels):\n",
    "                gold_score += span_scores[(i,j)][label]\n",
    "                if (i,j,label) not in span_labels:\n",
    "                    hamming += 1\n",
    "            hamming = hamming/len(gold_spans) # normalize by number of spans\n",
    "\n",
    "            # accumulate maximum-margin/hinge loss for this sequence\n",
    "            loss += max(0, hamming + tree_score - gold_score)\n",
    "\n",
    "        loss = loss / len(input_idx) # average loss over batch\n",
    "        return loss, batch_span_labels, batch_back_pointers\n",
    "  \n",
    "\n",
    "    \"\"\"    \n",
    "    def forward(self, input_idx, input_attn_mask, batch_word_ids, targets=None):\n",
    "        # compute BERT embeddings for input tokens\n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask)\n",
    "        bert_output = self.dropout(bert_output.last_hidden_state) # shape: (batch_size, block_size, hidden_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            # for each sequence, get word embeddings for each word by using the BERT embeddings for first subword token for that word\n",
    "            logits = []\n",
    "            loss = 0.0\n",
    "            for i, word_ids in enumerate(batch_word_ids):\n",
    "                word_embeddings = [bert_output[i,0,:]] # add CLS token embedding\n",
    "                # construct embeddings for each word in the original (untokenized) sequence\n",
    "                for j in range(word_ids[-2]+1):\n",
    "                    y_j = bert_output[i,word_ids.index(j),:] # gets first subword token embedding for word j\n",
    "                    word_embeddings.append(y_j)        \n",
    "                word_embeddings.append(bert_output[i,len(word_ids)-1,:]) # add SEP token embedding\n",
    "\n",
    "                # now that we have the word embeddings, we need to construct the span embeddings\n",
    "                spans, labels = targets[i]\n",
    "                span_logits = []\n",
    "                for i,j in spans:\n",
    "                    # get right and left fencepost embeddings for the bounding fenceposts of this span\n",
    "                    y_j_right = word_embeddings[j][768//2:]\n",
    "                    y_i_right = word_embeddings[i][768//2:]\n",
    "                    y_jplus1_left = word_embeddings[j+1][:768//2]\n",
    "                    y_iplus1_left = word_embeddings[i+1][:768//2]\n",
    "                    # concatenate difference vectors to get span embedding\n",
    "                    span_embedding = torch.cat([y_j_right-y_i_right, y_jplus1_left-y_iplus1_left], dim=0) # shape: (hidden_size,)\n",
    "                    # compute logits for this span\n",
    "                    span_logits.append(self.classifier_head(span_embedding)) # shape: (num_classes,)\n",
    "\n",
    "                span_logits = torch.stack(span_logits, dim=0) # shape: (num_spans, num_classes)\n",
    "                logits.append(span_logits)\n",
    "                # accumulate loss for the spans in this sequence (note that we only compute losses for labeled spans)\n",
    "                loss += F.cross_entropy(span_logits, torch.tensor(labels, device=input_idx.device))\n",
    "\n",
    "            loss = loss / len(input_idx) # average loss over batch\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            loss = None\n",
    "            logits = []\n",
    "            batch_spans = []\n",
    "            for i, word_ids in enumerate(batch_word_ids):\n",
    "                word_embeddings = [bert_output[i,0,:]] # add CLS token embedding\n",
    "                # construct embeddings for each word in the original (untokenized) sequence\n",
    "                for j in range(word_ids[-2]+1):\n",
    "                    y_j = bert_output[i,word_ids.index(j),:] # gets first subword token embedding for word j\n",
    "                    word_embeddings.append(y_j)        \n",
    "                word_embeddings.append(bert_output[i,len(word_ids)-1,:]) # add SEP token embedding\n",
    "\n",
    "                # in inference mode, we will compute scores for all possible spans\n",
    "                spans = [(i,j+1) for i in range(len(word_embeddings)) for j in range(i, len(word_embeddings))]\n",
    "                # compute logits\n",
    "                span_logits = []\n",
    "                for i,j in spans:\n",
    "                    # get right and left fencepost embeddings for the bounding fenceposts of this span\n",
    "                    y_j_right = word_embeddings[j][768//2:]\n",
    "                    y_i_right = word_embeddings[i][768//2:]\n",
    "                    y_jplus1_left = word_embeddings[j+1][:768//2]\n",
    "                    y_iplus1_left = word_embeddings[i+1][:768//2]\n",
    "                    span_embedding = torch.cat([y_j_right-y_i_right, y_jplus1_left-y_iplus1_left], dim=0) # shape: (hidden_size,)                    \n",
    "                    # compute logits for each span\n",
    "                    span_logits.append(self.classifier_head(span_embedding)) # shape: (num_classes,)  \n",
    "                span_logits = torch.stack(span_logits, dim=0) # shape: (num_spans, num_classes)\n",
    "                logits.append(span_logits)\n",
    "                batch_spans.append(spans)\n",
    "            return logits, batch_spans\n",
    "            \"\"\"\n",
    "        \n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, word_ids, targets = batch\n",
    "            # move batch to device\n",
    "            input_idx, input_attn_mask = input_idx.to(device), input_attn_mask.to(device)\n",
    "            # forward pass\n",
    "            loss, batch_span_labels, batch_back_pointers = model(input_idx, input_attn_mask, word_ids, targets)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            for i, (gold_spans, gold_labels) in enumerate(targets):\n",
    "                span_labels = batch_span_labels[i]\n",
    "                for (i,j),label in zip(gold_spans, gold_labels):\n",
    "                    if (i,j,label) in span_labels:\n",
    "                        num_correct += 1\n",
    "                num_total += len(gold_spans)\n",
    "            train_acc = num_correct / num_total        \n",
    "\n",
    "            if val_every is not None:\n",
    "                if i%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                    pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\":loss.item(), \"Moving Avg Loss\":avg_loss, \"Train Accuracy\":train_acc, \"Val Loss\": val_loss, \"Val Accuracy\":val_acc}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, word_ids, targets = batch\n",
    "            input_idx, input_attn_mask = input_idx.to(device), input_attn_mask.to(device)\n",
    "            loss, batch_span_labels, batch_back_pointers = model(input_idx, input_attn_mask, word_ids, targets)\n",
    "            for i, (gold_spans, gold_labels) in enumerate(targets):\n",
    "                span_labels = batch_span_labels[i]\n",
    "                for (i,j),label in zip(gold_spans, gold_labels):\n",
    "                    if (i,j,label) in span_labels:\n",
    "                        num_correct += 1\n",
    "                num_total += len(gold_spans)\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename='BERT_CKY_checkpoint.pth'):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer=None,  filename='BERT_CKY_checkpoint.pth'):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 1e-5\n",
    "epochs = 1\n",
    "\n",
    "train_dataset = ParseTreeDataset(sentences_train, span_labels_train, label2idx)\n",
    "val_dataset = ParseTreeDataset(sentences_val, span_labels_val, label2idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 66.513684 M\n",
      "RAM used: 2632.34 MB\n"
     ]
    }
   ],
   "source": [
    "# model with finetuning disabled\n",
    "model = BERT_CKY(num_classes=len(label2idx), null_label_idx=null_label_idx).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "warmup_steps = int(len(train_dataloader) * 0.1 *  epochs) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/220 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=epochs, scheduler=scheduler, save_every=None, val_every=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
