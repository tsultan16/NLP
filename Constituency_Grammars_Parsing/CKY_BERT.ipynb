{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT augmented CKY variant\n",
    "\n",
    "Previously we looked at implementing the CKY algorithm to generate parse trees for sentences conforming to a context-free grammar in Chomsky-normal-form. We found that sentences generated by a CFG suffer from syntax ambiguities, i.e. a sentence can have multiple valid parse trees, each with a different meaning. However, we know that out of the different possible parses, typically only one of them is the \"correct\" one, i.e. captures the intended meaning of the sentence. \n",
    "\n",
    "In this notebook, we will look an a variant of the CKY algorithm in which each possible span of a sentence is assigned a score, and these scores can be used to arrive at the correct parse tree. These scores can be computed with the help of a neural network trained on an annotated treebank dataset. The training task simply involves predicting a distribution of scores over all possible non-terminal labels for each valid span/constituent of a sentence. After training this neural model, it is expected to assign large score to the correct label for each constituents of any given sentence, which will then help with the downstream task of disambiguating the correct parse tree (which will be done using a slightly modified CKY algorithm). A BERT model is powerful and well-suited for the score prediction task. The diagram below (borrowed from the Jurafsky-Martin textbook) summarizes the model architecture:\n",
    "\n",
    "<img src=\"neural_parser.png\" width=\"600\" height=\"450\">\n",
    "\n",
    "We define the spans in the same way as we did for the vanilla CKY parser, i.e. using the \"fencepost\" positions. We also use the same upper-triangular matrix that which we used previously. Instead of using a pre-defined CFG in CNF to assign non-terminal labels to each element in this matrix (which represent the different possible spans), this time we will instead use BERT model to compute a distribution of scores over all possible terminals for each possible span. First, we create a fixed-size vector representation of the span and then feed it into an MLP classifier, as shown in the diagram. We outline the steps in more detail:\n",
    "\n",
    "1) Convert words to subword tokens\n",
    "2) Get `BERT embeddings for subwords`\n",
    "3) Compute the `embeddings for full words` (many ways to do this, e.g. we could just assign the BERT em,bedding for the first subword of that word, or we could take the sum.average of the embedding of all the subwords or we could take element-wise max across all the subword embeddings, etc.)  \n",
    "4) Compute `embeddings of fence posts` (shown as 0,1,2,3.. in the diagram above). Since each fence-post can represent the beginning or end of a span, we will create two separate representations. We first split the embedding vector $y_t$ of the $t$-th word in the sentence into two halves, $\\overleftarrow{y_t}$ and $\\overrightarrow{y_t}$ such that the concatentaion $[\\overleftarrow{y_t}; \\overrightarrow{y_t}] = y_t$. Then the `start-of-span representation` of the fencepost at position $i$ is defined as $\\overrightarrow{y_i}$ and the `end-of-span representation` is defined as $\\overleftarrow{y}_{i+1}$\n",
    "5) Construct `embedding for a span` `(i,j)` using the fencepost embeddings as the following concatenation between the difference in start-of span and end-of-span embeddings for the bounding fenceposts: $v(i,j) = [\\overrightarrow{y_j}-\\overrightarrow{y_i}; \\overleftarrow{y}_{j+1}-\\overleftarrow{y}_{i+1}]$\n",
    "6) Pass $v(i,j)$ through the MLP to get a distribution of scores over all possible non-terminal labels.\n",
    "\n",
    "One really important thing to note here is that we are no longer using a pre-defined context-free grammar. The supervised training of the neural network model will implicitly induce/\"learn\" the grammar. Also, one downside is that the model may sometimes fail to get a grammatically correct parse of a sentence, because it does not have access to the \"true grammar\", only some form of statistical approximation of it.\n",
    "\n",
    "After we implement and train this model, we will iplement the CKY variant that will perform the actual parsing using the scores computed by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertModel, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "random.seed(10)\n",
    "import psutil\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstm let's load the data. We will use the NLTK treebank, which is a subset of the original Penn Treebank dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parsed sentences: 3910\n"
     ]
    }
   ],
   "source": [
    "# get all parsed sentences acrsoo all the files\n",
    "sentences = treebank.sents()\n",
    "parse_trees = treebank.parsed_sents()\n",
    "\n",
    "# only keep sentences that are at most 100 words\n",
    "sentences_parses = zip(sentences, parse_trees)\n",
    "sentences_parses = [(s,p) for s,p in sentences_parses if len(s) <= 100]\n",
    "\n",
    "sentences = [s for s,p in sentences_parses]\n",
    "parse_trees = [p for s,p in sentences_parses]\n",
    "\n",
    "print(f\"Number of parsed sentences: {len(parse_trees)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 92 25.58618925831202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences_lengths = [len(s) for s in sentences]\n",
    "print(min(sentences_lengths), max(sentences_lengths), sum(sentences_lengths)/len(sentences_lengths))\n",
    "\n",
    "# plot a histogram of sentence lengths\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sentences_lengths, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "# print example of a parsed sentence\n",
    "example_tree = parse_trees[0]\n",
    "print(example_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciton for extracting all the non-terminal labels for all possible spans from each sentence\n",
    "def get_span_labels(parse_tree, verbose=False):\n",
    "    span_labels = {}\n",
    "    if verbose:\n",
    "        parse_tree.pretty_print()\n",
    "    # iterate over all subtrees in level-order traversal\n",
    "    for subtree in parse_tree.subtrees():       \n",
    "        span = (parse_tree.leaves().index(subtree.leaves()[0]), parse_tree.leaves().index(subtree.leaves()[-1])+1)\n",
    "        span_labels[span] = subtree.label()\n",
    "        if verbose:\n",
    "            print(f\"\\nsubtree label: {subtree.label()}\")\n",
    "            print(f\"subtree leaves: {subtree.leaves()}\")\n",
    "            print(f\"Start fence-post: {parse_tree.leaves().index(subtree.leaves()[0])}\")\n",
    "            print(f\"End fence-post: {parse_tree.leaves().index(subtree.leaves()[-1])+1}\")\n",
    "            print(f\"span: {span}\")\n",
    "    return span_labels            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     S                                                                         \n",
      "                         ____________________________|_______________________________________________________________________   \n",
      "                        |                                               VP                                                   | \n",
      "                        |                        _______________________|___                                                 |  \n",
      "                      NP-SBJ                    |                           VP                                               | \n",
      "         _______________|___________________    |     ______________________|______________________________________          |  \n",
      "        |          |              ADJP      |   |    |        |                PP-CLR                              |         | \n",
      "        |          |           ____|____    |   |    |        |          ________|_________                        |         |  \n",
      "        NP         |          NP        |   |   |    |        NP        |                  NP                    NP-TMP      | \n",
      "   _____|____      |     _____|____     |   |   |    |     ___|____     |    ______________|__________        _____|_____    |  \n",
      " NNP        NNP    ,    CD        NNS   JJ  ,   MD   VB   DT       NN   IN  DT             JJ         NN    NNP          CD  . \n",
      "  |          |     |    |          |    |   |   |    |    |        |    |   |              |          |      |           |   |  \n",
      "Pierre     Vinken  ,    61       years old  ,  will join the     board  as  a         nonexecutive director Nov.         29  . \n",
      "\n",
      "\n",
      "subtree label: S\n",
      "subtree leaves: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "Start fence-post: 0\n",
      "End fence-post: 18\n",
      "span: (0, 18)\n",
      "\n",
      "subtree label: NP-SBJ\n",
      "subtree leaves: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',']\n",
      "Start fence-post: 0\n",
      "End fence-post: 3\n",
      "span: (0, 3)\n",
      "\n",
      "subtree label: NP\n",
      "subtree leaves: ['Pierre', 'Vinken']\n",
      "Start fence-post: 0\n",
      "End fence-post: 2\n",
      "span: (0, 2)\n",
      "\n",
      "subtree label: NNP\n",
      "subtree leaves: ['Pierre']\n",
      "Start fence-post: 0\n",
      "End fence-post: 1\n",
      "span: (0, 1)\n",
      "\n",
      "subtree label: NNP\n",
      "subtree leaves: ['Vinken']\n",
      "Start fence-post: 1\n",
      "End fence-post: 2\n",
      "span: (1, 2)\n",
      "\n",
      "subtree label: ,\n",
      "subtree leaves: [',']\n",
      "Start fence-post: 2\n",
      "End fence-post: 3\n",
      "span: (2, 3)\n",
      "\n",
      "subtree label: ADJP\n",
      "subtree leaves: ['61', 'years', 'old']\n",
      "Start fence-post: 3\n",
      "End fence-post: 6\n",
      "span: (3, 6)\n",
      "\n",
      "subtree label: NP\n",
      "subtree leaves: ['61', 'years']\n",
      "Start fence-post: 3\n",
      "End fence-post: 5\n",
      "span: (3, 5)\n",
      "\n",
      "subtree label: CD\n",
      "subtree leaves: ['61']\n",
      "Start fence-post: 3\n",
      "End fence-post: 4\n",
      "span: (3, 4)\n",
      "\n",
      "subtree label: NNS\n",
      "subtree leaves: ['years']\n",
      "Start fence-post: 4\n",
      "End fence-post: 5\n",
      "span: (4, 5)\n",
      "\n",
      "subtree label: JJ\n",
      "subtree leaves: ['old']\n",
      "Start fence-post: 5\n",
      "End fence-post: 6\n",
      "span: (5, 6)\n",
      "\n",
      "subtree label: ,\n",
      "subtree leaves: [',']\n",
      "Start fence-post: 2\n",
      "End fence-post: 3\n",
      "span: (2, 3)\n",
      "\n",
      "subtree label: VP\n",
      "subtree leaves: ['will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29']\n",
      "Start fence-post: 7\n",
      "End fence-post: 17\n",
      "span: (7, 17)\n",
      "\n",
      "subtree label: MD\n",
      "subtree leaves: ['will']\n",
      "Start fence-post: 7\n",
      "End fence-post: 8\n",
      "span: (7, 8)\n",
      "\n",
      "subtree label: VP\n",
      "subtree leaves: ['join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29']\n",
      "Start fence-post: 8\n",
      "End fence-post: 17\n",
      "span: (8, 17)\n",
      "\n",
      "subtree label: VB\n",
      "subtree leaves: ['join']\n",
      "Start fence-post: 8\n",
      "End fence-post: 9\n",
      "span: (8, 9)\n",
      "\n",
      "subtree label: NP\n",
      "subtree leaves: ['the', 'board']\n",
      "Start fence-post: 9\n",
      "End fence-post: 11\n",
      "span: (9, 11)\n",
      "\n",
      "subtree label: DT\n",
      "subtree leaves: ['the']\n",
      "Start fence-post: 9\n",
      "End fence-post: 10\n",
      "span: (9, 10)\n",
      "\n",
      "subtree label: NN\n",
      "subtree leaves: ['board']\n",
      "Start fence-post: 10\n",
      "End fence-post: 11\n",
      "span: (10, 11)\n",
      "\n",
      "subtree label: PP-CLR\n",
      "subtree leaves: ['as', 'a', 'nonexecutive', 'director']\n",
      "Start fence-post: 11\n",
      "End fence-post: 15\n",
      "span: (11, 15)\n",
      "\n",
      "subtree label: IN\n",
      "subtree leaves: ['as']\n",
      "Start fence-post: 11\n",
      "End fence-post: 12\n",
      "span: (11, 12)\n",
      "\n",
      "subtree label: NP\n",
      "subtree leaves: ['a', 'nonexecutive', 'director']\n",
      "Start fence-post: 12\n",
      "End fence-post: 15\n",
      "span: (12, 15)\n",
      "\n",
      "subtree label: DT\n",
      "subtree leaves: ['a']\n",
      "Start fence-post: 12\n",
      "End fence-post: 13\n",
      "span: (12, 13)\n",
      "\n",
      "subtree label: JJ\n",
      "subtree leaves: ['nonexecutive']\n",
      "Start fence-post: 13\n",
      "End fence-post: 14\n",
      "span: (13, 14)\n",
      "\n",
      "subtree label: NN\n",
      "subtree leaves: ['director']\n",
      "Start fence-post: 14\n",
      "End fence-post: 15\n",
      "span: (14, 15)\n",
      "\n",
      "subtree label: NP-TMP\n",
      "subtree leaves: ['Nov.', '29']\n",
      "Start fence-post: 15\n",
      "End fence-post: 17\n",
      "span: (15, 17)\n",
      "\n",
      "subtree label: NNP\n",
      "subtree leaves: ['Nov.']\n",
      "Start fence-post: 15\n",
      "End fence-post: 16\n",
      "span: (15, 16)\n",
      "\n",
      "subtree label: CD\n",
      "subtree leaves: ['29']\n",
      "Start fence-post: 16\n",
      "End fence-post: 17\n",
      "span: (16, 17)\n",
      "\n",
      "subtree label: .\n",
      "subtree leaves: ['.']\n",
      "Start fence-post: 17\n",
      "End fence-post: 18\n",
      "span: (17, 18)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(0, 18): 'S',\n",
       " (0, 3): 'NP-SBJ',\n",
       " (0, 2): 'NP',\n",
       " (0, 1): 'NNP',\n",
       " (1, 2): 'NNP',\n",
       " (2, 3): ',',\n",
       " (3, 6): 'ADJP',\n",
       " (3, 5): 'NP',\n",
       " (3, 4): 'CD',\n",
       " (4, 5): 'NNS',\n",
       " (5, 6): 'JJ',\n",
       " (7, 17): 'VP',\n",
       " (7, 8): 'MD',\n",
       " (8, 17): 'VP',\n",
       " (8, 9): 'VB',\n",
       " (9, 11): 'NP',\n",
       " (9, 10): 'DT',\n",
       " (10, 11): 'NN',\n",
       " (11, 15): 'PP-CLR',\n",
       " (11, 12): 'IN',\n",
       " (12, 15): 'NP',\n",
       " (12, 13): 'DT',\n",
       " (13, 14): 'JJ',\n",
       " (14, 15): 'NN',\n",
       " (15, 17): 'NP-TMP',\n",
       " (15, 16): 'NNP',\n",
       " (16, 17): 'CD',\n",
       " (17, 18): '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_span_labels(example_tree, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a pytorch dataset for creating the (input, target) instances for our BERT span score prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      "[101, 5578, 19354, 7520, 1010, 6079, 2086, 2214, 1010, 2097, 3693, 1996, 2604, 2004, 1037, 3904, 2595, 8586, 28546, 2472, 13292, 1012, 2756, 1012, 102]\n",
      "['[CLS]', 'pierre', 'vin', '##ken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'none', '##x', '##ec', '##utive', 'director', 'nov', '.', '29', '.', '[SEP]']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 13, 13, 14, 15, 15, 16, 17, None]\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (0, 15), (0, 16), (0, 17), (0, 18), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (1, 15), (1, 16), (1, 17), (1, 18), (2, 3), (2, 4), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (2, 15), (2, 16), (2, 17), (2, 18), (3, 4), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (3, 15), (3, 16), (3, 17), (3, 18), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (4, 15), (4, 16), (4, 17), (4, 18), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (5, 15), (5, 16), (5, 17), (5, 18), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (6, 15), (6, 16), (6, 17), (6, 18), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (7, 15), (7, 16), (7, 17), (7, 18), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (8, 15), (8, 16), (8, 17), (8, 18), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (9, 15), (9, 16), (9, 17), (9, 18), (10, 11), (10, 12), (10, 13), (10, 14), (10, 15), (10, 16), (10, 17), (10, 18), (11, 12), (11, 13), (11, 14), (11, 15), (11, 16), (11, 17), (11, 18), (12, 13), (12, 14), (12, 15), (12, 16), (12, 17), (12, 18), (13, 14), (13, 15), (13, 16), (13, 17), (13, 18), (14, 15), (14, 16), (14, 17), (14, 18), (15, 16), (15, 17), (15, 18), (16, 17), (16, 18), (17, 18)]\n"
     ]
    }
   ],
   "source": [
    "sentence = sentences[0]\n",
    "input_encoding = tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "input_idx = input_encoding['input_ids']\n",
    "word_ids = input_encoding.word_ids()\n",
    "print(sentences[0])\n",
    "print(input_idx)\n",
    "print(tokenizer.convert_ids_to_tokens(input_idx))\n",
    "print(word_ids)\n",
    "\n",
    "# get all possible word spans for the sentence\n",
    "word_spans = [(i, j+1) for i in range(len(sentence)) for j in range(i, len(sentence))]\n",
    "print(word_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get span labels for all sentences\n",
    "span_labels = [get_span_labels(p) for p in parse_trees]\n",
    "\n",
    "# create mapping of span labels to unique ids\n",
    "unique_span_labels = list(set([label for span in span_labels for label in span.values()]))\n",
    "label2idx = {label: i for i, label in enumerate(unique_span_labels)}    \n",
    "\n",
    "# create train-val splits\n",
    "n_train = int(0.9 * len(sentences))\n",
    "sentences_train = sentences[:n_train]\n",
    "parse_trees_train = parse_trees[:n_train]\n",
    "span_labels_train = span_labels[:n_train]\n",
    "sentences_val = sentences[n_train:]\n",
    "parse_trees_val = parse_trees[n_train:]\n",
    "span_labels_val = span_labels[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseTreeDataset(Dataset):\n",
    "    def __init__(self, sentences, span_labels, label2idx, block_size=256, max_spans=64):\n",
    "        self.sentences = sentences\n",
    "        self.parse_trees = parse_trees\n",
    "        self.span_labels = span_labels\n",
    "        self.label2idx = label2idx\n",
    "        self.block_size = block_size\n",
    "        self.max_spans = max_spans\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    def get_span_labels(self, parse_tree):\n",
    "        span_labels = {}\n",
    "        # iterate over all subtrees in level-order traversal\n",
    "        for subtree in parse_tree.subtrees():       \n",
    "            span = (parse_tree.leaves().index(subtree.leaves()[0]), parse_tree.leaves().index(subtree.leaves()[-1])+1)\n",
    "            span_labels[span] = subtree.label()\n",
    "        return span_labels \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get sentence and span labels\n",
    "        sentence = self.sentences[idx]\n",
    "        span_labels = self.span_labels[idx]\n",
    "        # convert span labels to indices\n",
    "        span_labels_idx = {span:self.label2idx[label] for span, label in span_labels.items()}\n",
    "\n",
    "        # tokenize the sentence\n",
    "        input_encoding = self.tokenizer.encode_plus(sentence, is_split_into_words=True, return_offsets_mapping=False, padding=False, truncation=False, add_special_tokens=True)\n",
    "        input_idx = input_encoding['input_ids']\n",
    "        word_ids = input_encoding.word_ids()\n",
    "\n",
    "        if len(input_idx) > self.block_size:\n",
    "            raise ValueError(f\"Tokenized sentence {idx} is too long: {len(input_idx)}. Truncation unsupported.\")\n",
    "\n",
    "        # add padding \n",
    "        input_idx = input_idx + [self.tokenizer.pad_token_id] * (self.block_size - len(input_idx))    \n",
    "        # create attention mask \n",
    "        input_attn_mask = [1 if idx != self.tokenizer.pad_token_id else 0 for idx in input_idx]\n",
    "\n",
    "        # convert to tensors\n",
    "        input_idx = torch.tensor(input_idx)\n",
    "        input_attn_mask = torch.tensor(input_attn_mask) \n",
    "        \n",
    "        return input_idx, input_attn_mask, word_ids, span_labels_idx   \n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the tensors and the dictionaries\n",
    "    input_idxs, input_attn_masks, word_ids, span_labels_idx = zip(*batch)\n",
    "\n",
    "    # Default collate the tensors\n",
    "    input_idxs = torch.stack(input_idxs)\n",
    "    input_attn_masks = torch.stack(input_attn_masks)\n",
    "\n",
    "    # Pad word_ids to the longest sequence in the batch\n",
    "    #max_length = max(len(ids) for ids in word_ids)\n",
    "    #padded_word_ids = [ids + [None] * (max_length - len(ids)) for ids in word_ids]\n",
    "\n",
    "    # Handle the dictionaries\n",
    "    spans_labels = []\n",
    "    for d in span_labels_idx:\n",
    "        # Convert the dictionary to two lists: one for spans and one for labels\n",
    "        spans, labels = zip(*d.items())\n",
    "        spans_labels.append((spans, labels))\n",
    "\n",
    "    return input_idxs, input_attn_masks, word_ids, spans_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets implement the BERT model for span score prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_CKY_scorer(torch.nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.1, mlp_hidden_size=128):\n",
    "        super().__init__()\n",
    "        # load pretrained BERT model\n",
    "        self.bert_encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        # define classifier head (2 layer MLP)\n",
    "        self.classifier_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.bert_encoder.config.hidden_size, mlp_hidden_size),\n",
    "            torch.nn.LayerNorm(mlp_hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(mlp_hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "        # make sure BERT parameters are trainable\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_idx, input_attn_mask, batch_word_ids, targets=None):\n",
    "        # compute BERT embeddings for input tokens\n",
    "        bert_output = self.bert_encoder(input_idx, attention_mask=input_attn_mask)\n",
    "        bert_output = self.dropout(bert_output.last_hidden_state) # shape: (batch_size, block_size, hidden_size)\n",
    "\n",
    "        if targets is not None:\n",
    "            # for each sequence, get word embeddings for each word by using the BERT embeddings for first subword token for that word\n",
    "            logits = []\n",
    "            loss = 0.0\n",
    "            for i, word_ids in enumerate(batch_word_ids):\n",
    "                word_embeddings = [bert_output[i,0,:]] # add CLS token embedding\n",
    "                # construct embeddings for each word in the original (untokenized) sequence\n",
    "                for j in range(word_ids[-2]+1):\n",
    "                    y_j = bert_output[i,word_ids.index(j),:] # gets first subword token embedding for word j\n",
    "                    word_embeddings.append(y_j)        \n",
    "                word_embeddings.append(bert_output[i,len(word_ids)-1,:]) # add SEP token embedding\n",
    "\n",
    "                # now that we have the word embeddings, we need to construct the span embeddings\n",
    "                spans, labels = targets[i]\n",
    "                span_logits = []\n",
    "                for i,j in spans:\n",
    "                    # get right and left fencepost embeddings for the bounding fenceposts of this span\n",
    "                    y_j_right = word_embeddings[j][768//2:]\n",
    "                    y_i_right = word_embeddings[i][768//2:]\n",
    "                    y_jplus1_left = word_embeddings[j+1][:768//2]\n",
    "                    y_iplus1_left = word_embeddings[i+1][:768//2]\n",
    "                    # concatenate difference vectors to get span embedding\n",
    "                    span_embedding = torch.cat([y_j_right-y_i_right, y_jplus1_left-y_iplus1_left], dim=0) # shape: (hidden_size,)\n",
    "                    # compute logits for this span\n",
    "                    span_logits.append(self.classifier_head(span_embedding)) # shape: (num_classes,)\n",
    "\n",
    "                span_logits = torch.stack(span_logits, dim=0) # shape: (num_spans, num_classes)\n",
    "                logits.append(span_logits)\n",
    "                # accumulate loss for the spans in this sequence (note that we only compute losses for labeled spans)\n",
    "                loss += F.cross_entropy(span_logits, torch.tensor(labels, device=input_idx.device))\n",
    "\n",
    "            loss = loss / len(input_idx) # average loss over batch\n",
    "            return logits, loss\n",
    "\n",
    "        else:\n",
    "            loss = None\n",
    "            logits = []\n",
    "            batch_spans = []\n",
    "            for i, word_ids in enumerate(batch_word_ids):\n",
    "                word_embeddings = [bert_output[i,0,:]] # add CLS token embedding\n",
    "                # construct embeddings for each word in the original (untokenized) sequence\n",
    "                for j in range(word_ids[-2]+1):\n",
    "                    y_j = bert_output[i,word_ids.index(j),:] # gets first subword token embedding for word j\n",
    "                    word_embeddings.append(y_j)        \n",
    "                word_embeddings.append(bert_output[i,len(word_ids)-1,:]) # add SEP token embedding\n",
    "\n",
    "                # in inference mode, we will compute scores for all possible spans\n",
    "                spans = [(i, j+1) for i in range(len(word_embeddings)) for j in range(i, len(word_embeddings))]\n",
    "                # compute logits\n",
    "                span_logits = []\n",
    "                for i,j in spans:\n",
    "                    # get right and left fencepost embeddings for the bounding fenceposts of this span\n",
    "                    y_j_right = word_embeddings[j][768//2:]\n",
    "                    y_i_right = word_embeddings[i][768//2:]\n",
    "                    y_jplus1_left = word_embeddings[j+1][:768//2]\n",
    "                    y_iplus1_left = word_embeddings[i+1][:768//2]\n",
    "                    span_embedding = torch.cat([y_j_right-y_i_right, y_jplus1_left-y_iplus1_left], dim=0) # shape: (hidden_size,)                    \n",
    "                    # compute logits for each span\n",
    "                    span_logits.append(self.classifier_head(span_embedding)) # shape: (num_classes,)  \n",
    "                span_logits = torch.stack(span_logits, dim=0) # shape: (num_spans, num_classes)\n",
    "                logits.append(span_logits)\n",
    "                batch_spans.append(spans)\n",
    "            return logits, batch_spans\n",
    "        \n",
    "\n",
    "\n",
    "# training loop\n",
    "def train(model, optimizer, train_dataloader, val_dataloader, scheduler=None, device=\"cpu\", num_epochs=10, val_every=100, save_every=None, log_metrics=None):\n",
    "    avg_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.train()\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    for epoch in range(num_epochs):\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        pbar = tqdm(train_dataloader, desc=\"Epochs\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            input_idx, input_attn_mask, word_ids, targets = batch\n",
    "            # move batch to device\n",
    "            input_idx, input_attn_mask = input_idx.to(device), input_attn_mask.to(device)\n",
    "            # forward pass\n",
    "            logits, loss = model(input_idx, input_attn_mask, word_ids, targets)\n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            # optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "            avg_loss = 0.9* avg_loss + 0.1*loss.item()\n",
    "            B, _ = input_idx.shape\n",
    "            # concatenate the labels and logits over the batch\n",
    "            logits = torch.cat(logits, dim=0)\n",
    "            labels = torch.tensor([label for spans,labels in targets for label in labels], device=input_idx.device)\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) \n",
    "            # compute accuracy\n",
    "            num_correct += y_pred.eq(labels).sum().item()\n",
    "            num_total += len(labels)\n",
    "            train_acc = num_correct / num_total        \n",
    "\n",
    "            if val_every is not None:\n",
    "                if i%val_every == 0:\n",
    "                    # compute validation loss\n",
    "                    val_loss, val_acc = validation(model, val_dataloader, device=device)\n",
    "                    pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch + 1}, EMA Train Loss: {avg_loss:.3f}, Train Accuracy: {train_acc: .3f}, Val Loss: {val_loss: .3f}, Val Accuracy: {val_acc: .3f}\")  \n",
    "\n",
    "            if log_metrics:\n",
    "                metrics = {\"Batch loss\":loss.item(), \"Moving Avg Loss\":avg_loss, \"Train Accuracy\":train_acc, \"Val Loss\": val_loss, \"Val Accuracy\":val_acc}\n",
    "                log_metrics(metrics)\n",
    "\n",
    "        if save_every is not None:\n",
    "            if (epoch+1) % save_every == 0:\n",
    "                save_model_checkpoint(model, optimizer, epoch, avg_loss)\n",
    "\n",
    "\n",
    "def validation(model, val_dataloader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    val_losses = torch.zeros(len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "        for i,batch in enumerate(val_dataloader):\n",
    "            input_idx, input_attn_mask, word_ids, targets = batch\n",
    "            input_idx, input_attn_mask = input_idx.to(device), input_attn_mask.to(device)\n",
    "            logits, loss = model(input_idx, input_attn_mask, word_ids, targets)\n",
    "            B, _ = input_idx.shape\n",
    "            # concatenate the labels and logits over the batch\n",
    "            logits = torch.cat(logits, dim=0)\n",
    "            labels = torch.tensor([label for spans,labels in targets for label in labels], device=input_idx.device)\n",
    "            y_pred = logits.argmax(dim=-1).view(-1) \n",
    "            # compute accuracy\n",
    "            num_correct += y_pred.eq(labels).sum().item()\n",
    "            num_total += len(labels)\n",
    "            val_losses[i] = loss.item()\n",
    "    model.train()\n",
    "    val_loss = val_losses.mean().item()\n",
    "    val_accuracy = num_correct / num_total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "def save_model_checkpoint(model, optimizer, epoch=None, loss=None, filename='BERT_CKY_checkpoint.pth'):\n",
    "    # Save the model and optimizer state_dict\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }\n",
    "    # Save the checkpoint to a file\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Saved model checkpoint!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(model, optimizer=None,  filename='BERT_CKY_checkpoint.pth'):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Loaded model from checkpoint!\")\n",
    "    if optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model.train()\n",
    "        return model, optimizer          \n",
    "    else:\n",
    "        return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 16\n",
    "DEVICE = \"cuda\"\n",
    "learning_rate = 1e-5\n",
    "epochs = 3\n",
    "\n",
    "train_dataset = ParseTreeDataset(sentences_train, span_labels_train, label2idx)\n",
    "val_dataset = ParseTreeDataset(sentences_val, span_labels_val, label2idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=B, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=B, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in transformer network: 66.513942 M\n",
      "RAM used: 2610.34 MB\n"
     ]
    }
   ],
   "source": [
    "# model with finetuning disabled\n",
    "model = BERT_CKY_scorer(num_classes=len(label2idx)).to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "warmup_steps = int(len(train_dataloader) * 0.1 *  epochs) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "#model, optimizer = load_model_checkpoint(model, optimizer)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in transformer network: {num_params/1e6} M\")\n",
    "print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, EMA Train Loss: 5.850, Train Accuracy:  0.005, Val Loss:  6.002, Val Accuracy:  0.002:  27%|██▋       | 60/220 [00:39<01:26,  1.84it/s]"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataloader, val_dataloader, device=DEVICE, num_epochs=epochs, scheduler=scheduler, save_every=None, val_every=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
